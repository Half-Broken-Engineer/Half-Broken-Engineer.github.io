<!DOCTYPE html>
<html lang="ja">
  <head>
    
    <script type="application/ld+json">

{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "DeepLearningDay4",
  
  "datePublished": "2023-06-27T00:00:00+09:00",
  "dateModified": "2023-06-27T00:00:00+09:00",
  "author": {
    "@type": "Person",
    "name": "Half-Broken Engineer",
    
    "image": "https://half-broken-engineer.github.io/img/profile.png"
    
  },
  "mainEntityOfPage": { 
    "@type": "WebPage",
    "@id": "https:\/\/half-broken-engineer.github.io\/deeplearningday4\/" 
  },
  "publisher": {
    "@type": "Organization",
    "name": "壊れかけのエンジニアのログ",
    
    "logo": {
      "@type": "ImageObject",
      "url": "https://half-broken-engineer.github.io/img/profile.png"
    }
    
  },
  "description": "実施内容と得点表 章タイトル 要点 実装演習 確認テストまたは考察 参考図書など関連記事レポート 強化学習 ◯ ◯ AlphaGo ◯ ◯ 軽量化・高速化技術 ◯ ◯ 応用技術 ◯ ◯ ◯ ResNet ◯ ◯ ◯ EfficientNet ◯ - - ◯ 物体検知と SS 解説 ◯ - - ◯ Mask R-CNN ◯ - - ◯ FCOS ◯ - - ◯ Transformer ◯ - - ◯ BERT ◯ ◯ ◯ ◯ GPT ◯ - - ◯ 音声認識 ◯ - - ◯ CTC ◯ - - DCGAN ◯ - - ◯ Conditinal GAN ◯ - - ◯ Pix2Pix ◯ - - ◯ A3C ◯ - - Metric-Learning ◯ - - MAML ◯ - - GCN ◯ - - ◯ CAM,Grad-CAM,LIME,SHAP ◯ ◯ - Docker ◯ - - 合計：４５ 基準：３６",
  "keywords": []
}

</script>
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="generator" content="Hugo 0.112.5 with theme Tranquilpeak 0.5.3-BETA">
<meta name="author" content="Half-Broken Engineer">
<meta name="keywords" content="">
<meta name="description" content="実施内容と得点表 章タイトル 要点 実装演習 確認テストまたは考察 参考図書など関連記事レポート 強化学習 ◯ ◯ AlphaGo ◯ ◯ 軽量化・高速化技術 ◯ ◯ 応用技術 ◯ ◯ ◯ ResNet ◯ ◯ ◯ EfficientNet ◯ - - ◯ 物体検知と SS 解説 ◯ - - ◯ Mask R-CNN ◯ - - ◯ FCOS ◯ - - ◯ Transformer ◯ - - ◯ BERT ◯ ◯ ◯ ◯ GPT ◯ - - ◯ 音声認識 ◯ - - ◯ CTC ◯ - - DCGAN ◯ - - ◯ Conditinal GAN ◯ - - ◯ Pix2Pix ◯ - - ◯ A3C ◯ - - Metric-Learning ◯ - - MAML ◯ - - GCN ◯ - - ◯ CAM,Grad-CAM,LIME,SHAP ◯ ◯ - Docker ◯ - - 合計：４５ 基準：３６">


<meta property="og:description" content="実施内容と得点表 章タイトル 要点 実装演習 確認テストまたは考察 参考図書など関連記事レポート 強化学習 ◯ ◯ AlphaGo ◯ ◯ 軽量化・高速化技術 ◯ ◯ 応用技術 ◯ ◯ ◯ ResNet ◯ ◯ ◯ EfficientNet ◯ - - ◯ 物体検知と SS 解説 ◯ - - ◯ Mask R-CNN ◯ - - ◯ FCOS ◯ - - ◯ Transformer ◯ - - ◯ BERT ◯ ◯ ◯ ◯ GPT ◯ - - ◯ 音声認識 ◯ - - ◯ CTC ◯ - - DCGAN ◯ - - ◯ Conditinal GAN ◯ - - ◯ Pix2Pix ◯ - - ◯ A3C ◯ - - Metric-Learning ◯ - - MAML ◯ - - GCN ◯ - - ◯ CAM,Grad-CAM,LIME,SHAP ◯ ◯ - Docker ◯ - - 合計：４５ 基準：３６">
<meta property="og:type" content="article">
<meta property="og:title" content="DeepLearningDay4">
<meta name="twitter:title" content="DeepLearningDay4">
<meta property="og:url" content="https://half-broken-engineer.github.io/deeplearningday4/">
<meta property="twitter:url" content="https://half-broken-engineer.github.io/deeplearningday4/">
<meta property="og:site_name" content="壊れかけのエンジニアのログ">
<meta property="og:description" content="実施内容と得点表 章タイトル 要点 実装演習 確認テストまたは考察 参考図書など関連記事レポート 強化学習 ◯ ◯ AlphaGo ◯ ◯ 軽量化・高速化技術 ◯ ◯ 応用技術 ◯ ◯ ◯ ResNet ◯ ◯ ◯ EfficientNet ◯ - - ◯ 物体検知と SS 解説 ◯ - - ◯ Mask R-CNN ◯ - - ◯ FCOS ◯ - - ◯ Transformer ◯ - - ◯ BERT ◯ ◯ ◯ ◯ GPT ◯ - - ◯ 音声認識 ◯ - - ◯ CTC ◯ - - DCGAN ◯ - - ◯ Conditinal GAN ◯ - - ◯ Pix2Pix ◯ - - ◯ A3C ◯ - - Metric-Learning ◯ - - MAML ◯ - - GCN ◯ - - ◯ CAM,Grad-CAM,LIME,SHAP ◯ ◯ - Docker ◯ - - 合計：４５ 基準：３６">
<meta name="twitter:description" content="実施内容と得点表 章タイトル 要点 実装演習 確認テストまたは考察 参考図書など関連記事レポート 強化学習 ◯ ◯ AlphaGo ◯ ◯ 軽量化・高速化技術 ◯ ◯ 応用技術 ◯ ◯ ◯ ResNet ◯ ◯ ◯ EfficientNet ◯ - - ◯ 物体検知と SS 解説 ◯ - - ◯ Mask R-CNN ◯ - - ◯ FCOS ◯ - - ◯ Transformer ◯ - - ◯ BERT ◯ ◯ ◯ ◯ GPT ◯ - - ◯ 音声認識 ◯ - - ◯ CTC ◯ - - DCGAN ◯ - - ◯ Conditinal GAN ◯ - - ◯ Pix2Pix ◯ - - ◯ A3C ◯ - - Metric-Learning ◯ - - MAML ◯ - - GCN ◯ - - ◯ CAM,Grad-CAM,LIME,SHAP ◯ ◯ - Docker ◯ - - 合計：４５ 基準：３６">
<meta property="og:locale" content="ja">

  
    <meta property="article:published_time" content="2023-06-27T00:00:00">
  
  
    <meta property="article:modified_time" content="2023-06-27T00:00:00">
  
  
  
    
      <meta property="article:section" content="RabbitChallenge">
    
  
  
    
      <meta property="article:tag" content="obsidian_note">
    
  


<meta name="twitter:card" content="summary">

  <meta name="twitter:site" content="@overcome_kidney">


  <meta name="twitter:creator" content="@overcome_kidney">






  <meta property="og:image" content="https://half-broken-engineer.github.io/img/profile.png">
  <meta property="twitter:image" content="https://half-broken-engineer.github.io/img/profile.png">






    <title>DeepLearningDay4</title>

    <link rel="icon" href="https://half-broken-engineer.github.io/favicon.png">
    

    

    <link rel="canonical" href="https://half-broken-engineer.github.io/deeplearningday4/">

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" integrity="sha512-iBBXm8fW90+nuLcSKlbmrPcLa0OT92xO1BIsZ+ywDWZCvqsWgccV3gFoRBv0z+8dLJgyAHIhR35VZc2oM/gI1w==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha512-H9jrZiiopUdsLpg94A333EfumgUBpO9MdbxStdeITo+KEIMaNfHNvwyjjDJb+ERPaRS6DpyRlKbvPUasNItRyw==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    
    
    
    <link rel="stylesheet" href="https://half-broken-engineer.github.io/css/style-h6ccsoet3mzkbb0wngshlfbaweimexgqcxj0h5hu4h82olsdzz6wmqdkajm.min.css" />
    
    
      
        <link rel="stylesheet"  href="https://half-broken-engineer.github.io/css/mystyle.css">
      
    

    
      
    
    
  </head>

  <body>
    <div id="blog">
      <header id="header" data-behavior="5">
  <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
  <div class="header-title">
    <a class="header-title-link" href="https://half-broken-engineer.github.io/" aria-label="ホームページへ">壊れかけのエンジニアのログ</a>
  </div>
  
    
      <a class="header-right-picture "
         href="https://half-broken-engineer.github.io/#about" aria-label="リンクを開く: /#about">
    
    
    
      
        <img class="header-picture" src="https://half-broken-engineer.github.io/img/profile.png" alt="プロフィール画像" />
      
    
    </a>
  
</header>

      <nav id="sidebar" data-behavior="5">
  <div class="sidebar-container">
    
      <div class="sidebar-profile">
        <a href="https://half-broken-engineer.github.io/#about" aria-label="著者についてもっと読む">
          <img class="sidebar-profile-picture" src="https://half-broken-engineer.github.io/img/profile.png" alt="プロフィール画像" />
        </a>
        <h4 class="sidebar-profile-name">Half-Broken Engineer</h4>
        
          <h5 class="sidebar-profile-bio">🤖　　　　壊れかけのエンジニア　　　　💻不安を解消したいから💰のお勉強もする</h5>
        
      </div>
    
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://half-broken-engineer.github.io/" title="Home">
    
      <i class="sidebar-button-icon fas fa-lg fa-home" aria-hidden="true"></i>
      
      <span class="sidebar-button-desc">ホーム</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://half-broken-engineer.github.io/categories" title="Categories">
    
      <i class="sidebar-button-icon fas fa-lg fa-bookmark" aria-hidden="true"></i>
      
      <span class="sidebar-button-desc">カテゴリー</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://half-broken-engineer.github.io/tags" title="Tags">
    
      <i class="sidebar-button-icon fas fa-lg fa-tags" aria-hidden="true"></i>
      
      <span class="sidebar-button-desc">タグ</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://half-broken-engineer.github.io/archives" title="Archives">
    
      <i class="sidebar-button-icon fas fa-lg fa-archive" aria-hidden="true"></i>
      
      <span class="sidebar-button-desc">アーカイブ</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://half-broken-engineer.github.io/#about" title="About">
    
      <i class="sidebar-button-icon fas fa-lg fa-question" aria-hidden="true"></i>
      
      <span class="sidebar-button-desc">プロフィール</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://github.com/Half-Broken-Engineer" target="_blank" rel="noopener" title="GitHub">
    
      <i class="sidebar-button-icon fab fa-lg fa-github" aria-hidden="true"></i>
      
      <span class="sidebar-button-desc">GitHub</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://half-broken-engineer.github.io/index.xml" title="RSS">
    
      <i class="sidebar-button-icon fas fa-lg fa-rss" aria-hidden="true"></i>
      
      <span class="sidebar-button-desc">RSS</span>
    </a>
  </li>


    </ul>
  </div>
</nav>

      

      <div id="main" data-behavior="5"
        class="
               hasCoverMetaIn
               ">
        <article class="post" id="top">
          
          
            <div class="post-header main-content-wrap text-left">
  
    <h1 class="post-title">
      DeepLearningDay4
    </h1>
  
  
  <div class="postShorten-meta post-meta">
    
      <time datetime="2023-06-27T00:00:00&#43;09:00">
        
  
  
  
  
    2023-06-27
  

      </time>
    
    
  
  
    <span>カテゴリー</span>
    
      <a class="category-link" href="https://half-broken-engineer.github.io/categories/rabbitchallenge">RabbitChallenge</a>
    
  

  </div>

</div>
          
          <div class="post-content markdown">
            <div class="main-content-wrap">
              <ul>
<li></li>
</ul>
<h1 id="実施内容と得点表">実施内容と得点表</h1>
<table>
<thead>
<tr>
<th>章タイトル</th>
<th>要点</th>
<th>実装演習</th>
<th>確認テストまたは考察</th>
<th>参考図書など関連記事レポート</th>
</tr>
</thead>
<tbody>
<tr>
<td>強化学習</td>
<td>◯</td>
<td></td>
<td></td>
<td>◯</td>
</tr>
<tr>
<td>AlphaGo</td>
<td>◯</td>
<td></td>
<td></td>
<td>◯</td>
</tr>
<tr>
<td>軽量化・高速化技術</td>
<td>◯</td>
<td></td>
<td></td>
<td>◯</td>
</tr>
<tr>
<td>応用技術</td>
<td>◯</td>
<td></td>
<td>◯</td>
<td>◯</td>
</tr>
<tr>
<td>ResNet</td>
<td>◯</td>
<td>◯</td>
<td></td>
<td>◯</td>
</tr>
<tr>
<td>EfficientNet</td>
<td>◯</td>
<td>-</td>
<td>-</td>
<td>◯</td>
</tr>
<tr>
<td>物体検知と SS 解説</td>
<td>◯</td>
<td>-</td>
<td>-</td>
<td>◯</td>
</tr>
<tr>
<td>Mask R-CNN</td>
<td>◯</td>
<td>-</td>
<td>-</td>
<td>◯</td>
</tr>
<tr>
<td>FCOS</td>
<td>◯</td>
<td>-</td>
<td>-</td>
<td>◯</td>
</tr>
<tr>
<td>Transformer</td>
<td>◯</td>
<td>-</td>
<td>-</td>
<td>◯</td>
</tr>
<tr>
<td>BERT</td>
<td>◯</td>
<td>◯</td>
<td>◯</td>
<td>◯</td>
</tr>
<tr>
<td>GPT</td>
<td>◯</td>
<td>-</td>
<td>-</td>
<td>◯</td>
</tr>
<tr>
<td>音声認識</td>
<td>◯</td>
<td>-</td>
<td>-</td>
<td>◯</td>
</tr>
<tr>
<td>CTC</td>
<td>◯</td>
<td>-</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>DCGAN</td>
<td>◯</td>
<td>-</td>
<td>-</td>
<td>◯</td>
</tr>
<tr>
<td>Conditinal GAN</td>
<td>◯</td>
<td>-</td>
<td>-</td>
<td>◯</td>
</tr>
<tr>
<td>Pix2Pix</td>
<td>◯</td>
<td>-</td>
<td>-</td>
<td>◯</td>
</tr>
<tr>
<td>A3C</td>
<td>◯</td>
<td>-</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>Metric-Learning</td>
<td>◯</td>
<td>-</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>MAML</td>
<td>◯</td>
<td>-</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>GCN</td>
<td>◯</td>
<td>-</td>
<td>-</td>
<td>◯</td>
</tr>
<tr>
<td>CAM,Grad-CAM,LIME,SHAP</td>
<td>◯</td>
<td>◯</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>Docker</td>
<td>◯</td>
<td>-</td>
<td>-</td>
<td></td>
</tr>
</tbody>
</table>
<p>合計：４５
基準：３６</p>
<!-- raw HTML omitted -->
<h1 id="要点">要点</h1>
<h2 id="強化学習">強化学習</h2>
<h3 id="強化学習とは">強化学習とは</h3>
<p>環境の中で長期的に報酬を最大化できるように、行動を選択できるエージェントを作ることを目標とする機械学習の一分野</p>
<p>→　行動の結果から与えられる報酬をもとに行動を決定する原理を改善していく仕組み</p>
<h4 id="マーケティングでの応用例">マーケティングでの応用例</h4>
<ul>
<li>エージェント
<ul>
<li>入力：プロフィールと購入履歴</li>
<li>行動：顧客ごとにキャンペーンメールを送る、または送らない</li>
</ul>
</li>
<li>報酬：キャンペーンのコスト、キャンペーンで生み出されると推測される売上</li>
<li>環境：会社の販売促進部（＋顧客）</li>
</ul>
<h3 id="探索と利用のトレードオフ">探索と利用のトレードオフ</h3>
<p>環境について事前に完璧な知識がある状態（行動と報酬の関係が既知）なら最適な行動を決定することは可能</p>
<p>ただし、強化学習は上記仮定が成り立たないケースで適用する
→不完全な知識をもとに行動しながらデータを収集して最適な行動を見つけていく</p>
<p>持っている知識の範囲でベストな行動を取り続ける（利用）とより良い結果は見つけ出せないが、
未知の行動（探索）のみを取り続けると過去の経験が活かせない</p>
<h3 id="強化学習のイメージ">強化学習のイメージ</h3>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230706102154.png" alt="/img/Pasted_image_20230706102154.png">
(強化学習のイメージ：講義スライドより引用)</p>
<h3 id="強化学習とその他の機械学習手法の違い">強化学習とその他の機械学習手法の違い</h3>
<p>目標が異なる</p>
<ul>
<li>教師あり・教師なし学習：データに含まれるパターンの抽出</li>
<li>強化学習：優れた方策の発見</li>
</ul>
<h3 id="主な手法">主な手法</h3>
<ul>
<li>Q 学習：行動価値関数を行動するごとに更新することにより学習を進める手法</li>
<li>関数近似法：価値関数や方策関数を関数近似する手法のこと</li>
</ul>
<h3 id="価値関数">価値関数</h3>
<ul>
<li>状態価値関数</li>
<li>行動価値関数
<ul>
<li>平均報酬</li>
<li>割引報酬</li>
</ul>
</li>
</ul>
<h3 id="方策関数">方策関数</h3>
<p>状態にもとづいてどのような行動を取るのかの確率を与える関数</p>
<h3 id="方策勾配法">方策勾配法</h3>
<p>方策をモデル化して最適化する手法</p>
<p>方策の良さ (価値関数) を J として、方策関数のパラメータを更新していく</p>
<p>$$\pi(s, a | \theta) \<br>
\space\<br>
\underbrace{\theta^{(t+1)}}<em>{パラメータ} = \theta^{(t)} + \epsilon \nabla \underbrace{J(\theta)}</em>{報酬の期待値} \
$$</p>
<p>$$\nabla_\theta J(\theta) = \nabla_{\theta} \underbrace{\sum_{a \in A}\pi_{\theta}(a|s) Q^{\pi}(s, a)}_{報酬の期待値}$$</p>
<p>$$\nabla_{\theta}J(\theta) = \mathbb{E}<em>{\pi</em>{\theta}}[(\nabla_{\theta}log\pi_{\theta}(a|s)Q^{\pi}(s,a))]$$</p>
<h3 id="参考文献レポート">参考文献レポート</h3>
<p>黒本において、１７章　深層学習を用いた強化学習が該当
強化学習において問題とされる経験の自己相関に関わる内容として、DQN における体験再生の話題が入っていた。
DQN の工夫は</p>
<ul>
<li>体験再生
<ul>
<li>データ効率向上</li>
<li>入力系列の相関を断ち切れる→更新の分散低減</li>
<li>直前んに取得したデータから受ける影響の低下→パラメータの振動低減</li>
</ul>
</li>
<li>目標 Q ネットワークの固定：学習の目標値算出に用いるネットワークと行動価値推定に用いるネットワークが同一のときに学習が安定しない問題があるため、目標値算出のねとワークの固定を行い一定周期で更新するように変更した</li>
<li>報酬のクリッピング</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="alphago">AlphaGo</h2>
<h3 id="論文">論文</h3>
<ul>
<li>
<p>AlphaGo (Lee) :<a href="https://www.nature.com/articles/nature16961">David Silver Mastering the game of Go with deep neural networks and tree search nature 27 January 2016 </a>&ldquo;<a href="https://www.nature.com/articles/nature16961">https://www.nature.com/articles/nature16961</a> &quot;</p>
</li>
<li>
<p>AlphaGo Zero :<a href="https://www.nature.com/articles/nature24270">David Silver Mastering the game of Go without human knowledge nature 18 October 2017 </a> &ldquo;<a href="https://www.nature.com/articles/nature16961">https://www.nature.com/articles/nature16961</a> &quot;</p>
</li>
</ul>
<h3 id="概要">概要</h3>
<p>Alpha Go では方策関数と価値関数を Deep Neural Network で学習する</p>
<p>囲碁の難しさはゲーム終了までのターンの多さ（探索の深さ）と手数の多さ（探索の幅）によるもの
探索の幅は良い方策で手数を絞り込むことで改善でき、
探索の深さは価値関数の精度を上げることで実際にゲーム終了までしなくても状態を評価できることで改善する</p>
<p>AlphaGo では 方策関数は Policy Net、価値関数は Value Net によって代替することでこれを実現している</p>
<ul>
<li>Policy Net
<ul>
<li>入力：盤面特徴入力 19✕19 の 48 チャンネル</li>
<li>出力：19✕19 マスの着手予想確率
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230711201510.png" alt="/img/Pasted_image_20230711201510.png"></li>
</ul>
</li>
<li>Value Net
<ul>
<li>入力：盤面特徴入力 19✕19 の 48 チャンネル</li>
<li>出力：1~1 の範囲での現在の局面の勝率
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230711201517.png" alt="/img/Pasted_image_20230711201517.png"></li>
</ul>
</li>
</ul>
<h3 id="入力特徴量">入力特徴量</h3>
<h4 id="policynet-と-value-ネットの共通特徴量">PolicyNet と Value ネットの共通特徴量</h4>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230706153406.png" alt="/img/Pasted_image_20230706153406.png">
<a href="https://codezine.jp/article/detail/10952">アルファ碁ゼロに使われているディープラーニングを解き明かす 論文から詳細を紹介|CodeZine（コードジン）&ldquo;https://www.nature.com/articles/nature16961 &ldquo;より引用</a></p>
<h4 id="valuenet-だけの特徴量">ValueNet だけの特徴量</h4>
<p>現在の手番が黒番であるかどうか</p>
<!-- raw HTML omitted -->
<h4 id="rolloutpolicy-の特徴量">RollOutPolicy の特徴量</h4>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230707150301.png" alt="/img/Pasted_image_20230707150301.png">
（講義スライドより引用）</p>
<h3 id="学習の流れ">学習の流れ</h3>
<ol>
<li>教師あり学習で譜面を用いて RollOutPolicy と Policy Net の学習
<ol>
<li>盤面のデータを入力したらその盤面のときに実際に打たれた手の確率が最大になるような確率分布を出力するように学習させる</li>
</ol>
</li>
<li>強化学習で Policy Net の学習
<ol>
<li>更新させる Policy Net と PolicyPool（教師あり学習で出力させた PolicyNet～これまでの強化学習で更新された PolicyNet）からランダムに選んだネットワークを戦わせる</li>
</ol>
</li>
<li>強化学習で Value Net の学習
<ol>
<li>強化学習部分
<ol>
<li>まず SL PolicyNet(教師あり学習で作成した PolicyNet) で N 手まで打つ。</li>
<li>N+1 手目の手をランダムに選択し、その手で進めた局面を S（N+1）とする。</li>
<li>S（N+1）から RL PolicyNet（強化学習で作成した PolicyNet）で終局まで打ち、その勝敗報酬を R とする。</li>
</ol>
</li>
<li>強化学習させた Policy Net 同士を戦わせた盤面と勝敗を教師データとして学習させる</li>
</ol>
</li>
</ol>
<!-- raw HTML omitted -->
<h3 id="モデルを用いた探索">モデルを用いた探索</h3>
<p>以下の４ステップからなるモンテカルロ木探索を行う</p>
<ol>
<li>選択</li>
<li>評価</li>
<li>バックアップ</li>
<li>成長</li>
</ol>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230707144705.png" alt="/img/Pasted_image_20230707144705.png"></p>
<h3 id="alphagozero">AlphaGoZero</h3>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230711201537.png" alt="/img/Pasted_image_20230711201537.png"></p>
<h4 id="alphagozero-と-alphago-の違い">AlphaGoZero と AlphaGo の違い</h4>
<ol>
<li>教師あり学習を一切行わず、強化学習のみで作成</li>
<li>特徴入力からヒューリスティックな要素を排除し、石の配置のみにした</li>
<li>PolicyNet と ValueNet を１つのネットワークに統合した</li>
<li>Residual Net（後述）を導入した</li>
<li>モンテカルロ木探索から RollOut シミュレーションをなくした</li>
</ol>
<h3 id="参考文献レポート-1">参考文献レポート</h3>
<p>黒本　p３６６、３６９に該当の問題があった。
各学習ステージにおけるパラメータの勾配の式があったため、以下にまとめる。</p>
<h4 id="教師有り学習による方策ネットワーク学習勾配上昇法">教師有り学習による方策ネットワーク学習　（勾配上昇法）</h4>
<p>$$
\Delta \sigma=\frac{\alpha}{m} \sum_{k=1}^m \frac{\partial \log p_\sigma\left(a^k \mid s^k\right)}{\partial \sigma}\
$$</p>
<h4 id="強化学習による方策ネットワーク学習勾配上昇法">強化学習による方策ネットワーク学習　（勾配上昇法）</h4>
<p>$$\Delta \rho=\frac{\alpha}{n} \sum_{i=1}^n \sum_{t=1}^{T^i} \frac{\partial \log p_\rho\left(a_t^i \mid s_t^i\right)}{\partial \rho}\left(z_t^i-v\left(s_t^i\right)\right)\
$$</p>
<h4 id="強化学習による価値ネットワーク学習勾配降下法">強化学習による価値ネットワーク学習（勾配降下法）</h4>
<p>$$
\Delta \boldsymbol{\theta}=\frac{\alpha}{m} \sum_{k=1}^m\left(z^k-v_\theta\left(s^k\right)\right) \frac{\partial v_\theta\left(s^k\right)}{\partial \theta}
$$</p>
<p>モンテカルロ法についても記載があったので、簡単にまとめると
AlphaGo で使われるモンテカルロ木探索アルゴリズムは Asynchronous policy and Value MCTS と呼ばれる。
非同期処理だが、記録ステップの工夫で重複したシミュレーションを避けている。</p>
<p>行動の選択には行動価値関数と PUCT アルゴリズムに基づく、広い探索に関するボーナス項で構成される目的関数を最大化する行動が選ばれる。</p>
<p>行動価値関数は価値ネットワークの行動価値の総和を更新回数で割った値とロールアウトに基づく行動価値の総和を探索回数で割った値更新量の荷重平均</p>
<!-- raw HTML omitted -->
<h2 id="軽量化高速化技術">軽量化・高速化技術</h2>
<h3 id="分散深層学習">分散深層学習</h3>
<p>深層学習はデータ量、パラメータ更新のための計算量から高速な計算が求められる
→複数の計算資源（ワーカー）を使用して並列で NN を構成することで効率のよい学習を行いたい</p>
<p>モデルが大きいとき→<a href="https://half-broken-engineer.github.io//%e3%83%a2%e3%83%87%e3%83%ab%e4%b8%a6%e5%88%97">モデル並列</a>
データが多いとき→<a href="https://half-broken-engineer.github.io//%e3%83%87%e3%83%bc%e3%82%bf%e4%b8%a6%e5%88%97">データ並列</a></p>
<h4 id="モデル並列">モデル並列</h4>
<p>モデルが大きいときに利用
パラメータ数が多いほどスピードアップの効率も向上</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230708143852.png" alt="/img/Pasted_image_20230708143852.png"></p>
<ol>
<li>親モデルを複数の部分で分割し、各ワーカに割り当てる</li>
<li>それぞれのワーカで分割したモデルを学習させる</li>
<li>学習が終わったら一つのモデルに復元
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230708143726.png" alt="/img/Pasted_image_20230708143726.png"></li>
</ol>
<!-- raw HTML omitted -->
<h4 id="データ並列">データ並列</h4>
<ul>
<li>親モデルを子モデルとしてコピー</li>
<li>データを分割してそれぞれの子モデルに計算させる</li>
<li>同期型と非同期型の２種類
処理速度は非同期型が速いが、学習が不安定（Stale Gradient Problem）で同期型のほうが精度が良いので、同期型が主流
<ul>
<li>同期型：すべての子モデルの計算が終わるのを待って全ワーカーの勾配の平均をつかって親モデルの更新を行う</li>
<li>非同期型：各ワーカーごとに学習させて、学習が終わった子モデルをパラメータサーバーにキューとして Push して、新しいデータがでたらパラメータサーバから pop して学習させる
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230708133547.png" alt="/img/Pasted_image_20230708133547.png"></li>
</ul>
</li>
</ul>
<h3 id="gpu-による高速化">GPU による高速化</h3>
<p>GPGPU: General Purpose GPU で本来のグラフィック以外の用途で使用される GPU の総称</p>
<h4 id="cpu-と-gpu-の比較">CPU と GPU の比較</h4>
<table>
<thead>
<tr>
<th>種類</th>
<th>コア数</th>
<th>コア性能</th>
<th>特徴</th>
</tr>
</thead>
<tbody>
<tr>
<td>CPU</td>
<td>少ない</td>
<td>高性能</td>
<td>複雑で連続する処理が得意</td>
</tr>
<tr>
<td>GPU</td>
<td>多い</td>
<td>低性能</td>
<td>簡単な並列処理が得意</td>
</tr>
</tbody>
</table>
<h4 id="gpgpu-の開発環境">GPGPU の開発環境</h4>
<ul>
<li>CUDA
<ul>
<li>GPU 上で並列コンピューティングを行うためのプラットフォーム</li>
<li>NVIDIA 社が開発している GPU のみで使用可能</li>
<li>Deep Leaning 用に提供されているため使いやすい</li>
</ul>
</li>
<li>OpenCL
<ul>
<li>オープンな並列コンピューティングのプラットフォーム</li>
<li>NVIDIA 社以外（Intel,AMD,ARM など）の GPU からでも利用可能</li>
<li>DeepLearning 用の計算に特化しているわけではない</li>
</ul>
</li>
</ul>
<p><strong>Deep Leaning 用フレームワーク内で実装されているため、使用する際にはインストールした後パスを通せば良い</strong></p>
<h3 id="モデルの軽量化">モデルの軽量化</h3>
<p>モデルの精度を保ちつつ、パラメータや演算回数を低減する手法の総称</p>
<p>PC に比べて性能が落ちる (主にメモリや計算速度)　IoT やモバイル端末において有用な手法</p>
<h4 id="量子化">量子化</h4>
<p>変数の精度を 64bit → 32bit に落としてメモリと演算処理の削減を行う</p>
<ul>
<li>メリット
<ul>
<li>計算の高速化</li>
<li>省メモリ化</li>
</ul>
</li>
<li>デメリット
<ul>
<li>精度の低下</li>
</ul>
</li>
</ul>
<h5 id="計算の高速化">計算の高速化</h5>
<p>処理能力を示す TeraFLOPS で評価すると、
単純に精度を半分にするとおよそ倍の処理が可能になる
DeepLearning 向けの Tensor 演算もあり、そちらを利用したほうが処理能力は上がる</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230708150608.png" alt="/img/Pasted_image_20230708150608.png">
（講義スライドより引用）</p>
<p>→　TeraFLOPS の評価でも想定できたが、処理時間は精度におよそ反比例していることがわかる</p>
<h5 id="省メモリ化">省メモリ化</h5>
<p>Float32 が使うメモリ領域は Float64 が使用するメモリ領域の半分という当たり前の話</p>
<h5 id="精度の低下">精度の低下</h5>
<p>Float32 にすることで有効桁数が下がるため、精度の下限以下の重みを表現できなくなる
（$単精度の下限は1.175494\times10^{-38}、倍精度の下限は2.225074\times10^{-308}$）</p>
<h6 id="極端な量子化の例">極端な量子化の例</h6>
<p>y = 0.5 x を 1bit の精度で y ＝ w・x で近似することを考えてみると
w = {0,1} のため、</p>
<p>y = 0, y= x の２つしか表現できなくなり、どちらも誤差の大きい状態になってしまう。
→量子化をする際には極端に精度が落ちない程度に量子化しなくてはならない
→モデルを適用する目的（目標性能）の明確化が必要
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230708150834.png" alt="/img/Pasted_image_20230708150834.png">
（講義スライドより引用）</p>
<h4 id="蒸留">蒸留</h4>
<p>精度が高いモデル (教師モデル) の知識を使って軽量なモデル (生徒モデル) の作成を行う</p>
<p>一般に</p>
<ul>
<li>教師モデル：複雑なモデルや、アンサンブルを用いた高精度なモデル</li>
<li>生徒モデル：簡略化した構造を持つ軽量なモデル
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230708152913.png" alt="/img/Pasted_image_20230708152913.png"></li>
</ul>
<h4 id="プルーニング">プルーニング</h4>
<p>精度に寄与していないニューロンを切り落とす
→モデルの軽量化、計算の高速化</p>
<p>重みの値によってニューロンを削減して、再学習を行う</p>
<p>閾値は各層の重みの標準偏差に係数をかけた値を用いて設定する</p>
<h5 id="参考文献レポート-2">参考文献レポート</h5>
<p>以下の図は（教師モデルはパラメータ数~９M で Multiplication~725M）演算回数を固定して、層を深くしたケースだが、通常の学習方法ではそもそも学習が進まなかったのを蒸留手法 (KD と Hint Training) では学習できていることを示している。
また、幅が広いモデルよりも深いモデルのほうが精度が高く出ていることがわかる。
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230708154125.png" alt="/img/Pasted_image_20230708154125.png">
(Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta, C., &amp; Bengio, Y. (2015). Fitnets: Hints for thin deep nets. In International Conference on Learning Representations.:&ldquo;<a href="https://arxiv.org/pdf/1412.6550.pdf%22">https://arxiv.org/pdf/1412.6550.pdf"</a>)</p>
<p>Hint Training と Knowledge Distillation の違いは以下の表</p>
<table>
<thead>
<tr>
<th>項目</th>
<th>Hint Training</th>
<th>Knowledge Distillation</th>
</tr>
</thead>
<tbody>
<tr>
<td>目的変数</td>
<td>教師モデルの隠れ層からの中間レベルのヒントを使用して、生徒モデルをトレーニングする</td>
<td>教師モデルの出力を使用して、生徒モデルをトレーニングする</td>
</tr>
<tr>
<td>最適化方法</td>
<td>生徒モデルは、教師モデルの隠れ層からのヒント（隠れ層の出力のこと）に近づくように損失関数を最小化する</td>
<td>生徒モデルは、教師モデルの出力と真のラベルの両方に近づくように損失関数を最小化する</td>
</tr>
<tr>
<td>学習の比較</td>
<td>生徒モデルは、教師モデルの隠れ層からの情報を直接学習する</td>
<td>生徒モデルは、教師モデルが持つ細かい情報や構造を間接的に学習する</td>
</tr>
</tbody>
</table>
<p>モデル軽量化技術についてまとめた論文があったので、そちらの表から効果のまとめ。
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230708144651.png" alt="/img/Pasted_image_20230708144651.png">
(&rdquo; 山本康平 橘素子 前野 蔵人 ディープラーニングのモデル軽量化 技術 &quot; より引用 &ldquo;<a href="https://www.oki.com/jp/otr/2019/n233/pdf/otr233_r11.pdf%22">https://www.oki.com/jp/otr/2019/n233/pdf/otr233_r11.pdf"</a>)</p>
<!-- raw HTML omitted -->
<h2 id="応用技術">応用技術</h2>
<h3 id="mobilenet">MobileNet</h3>
<h4 id="目的">目的</h4>
<p>組み込みアプリケーションの実行環境で実行可能なモデルをめざして
ディープラーニングモデルの軽量化・高速化・高精度化を行う</p>
<h4 id="手法">手法</h4>
<p><strong>Depthwise Convolution</strong>(K✕K✕1 のカーネルをそれぞれのチャンネルに畳み込む) と <strong>Pointwise Covolution</strong>（1✕1✕C のカーネルを M 個）による軽量化
通常の CNN の計算では空間方向とチャンネル方法の畳み込みを同時に行っていたのを個別に行っている</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230707170356.png" alt="/img/Pasted_image_20230707170356.png"></p>
<h5 id="depthwise-convolution">Depthwise Convolution</h5>
<ul>
<li>入力の <strong>チャンネルごと</strong> に畳み込み</li>
<li>出力マップはチャンネルごとの出力を結合</li>
<li>K✕K✕1 のカーネル</li>
</ul>
<p>※チャンネルごとで畳みこみをしているため、チャンネル方向の関係性が考慮されない問題がある。
　→　Pointwise Convolution をセット使うことで対応</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230707171340.png" alt="/img/Pasted_image_20230707171340.png"></p>
<h5 id="pointwise-convolution">Pointwise Convolution</h5>
<ul>
<li>入力マップの <strong>ポイントごと</strong> に畳み込みを実施</li>
<li>出力のチャンネル数はフィルタ数で決定される</li>
</ul>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230707171452.png" alt="/img/Pasted_image_20230707171452.png"></p>
<h3 id="densenet">DenseNet</h3>
<h4 id="概要-1">概要</h4>
<ul>
<li>Dense ブロック内の各層の入力はこれまでの入力すべて＋直前の出力をチャンネル方向に結合したものになる</li>
<li>各層の構成は Batch 正規化→Relu→３×３× k の畳み込み</li>
<li>チャンネル方向のサイズの入力は Dense ブロックに最初に入ってきたチャンネル数 $k_0$ と growth_rate:k を使って $k_0 + i \times k$ となる（i は０始まりの層のインデックス）</li>
<li>Dense ブロック内で特徴マップのサイズは一定で、その間の Transition Layer(=Convolution+Pooling）でチャネルサイズと空間方向のダウンサンプリングを行う
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230707174801.png" alt="/img/Pasted_image_20230707174801.png">
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230707175044.png" alt="/img/Pasted_image_20230707175044.png">
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230707175513.png" alt="/img/Pasted_image_20230707175513.png">
(講義スライドより引用)</li>
</ul>
<h4 id="residual-ブロックとの違い">Residual ブロックとの違い</h4>
<ul>
<li>入力履歴の扱い
<ul>
<li>Residual: 直前の一層の入力のみ後方の層へ入力</li>
<li>Dense：ブロック内の前方の各層への入力すべてが後方の層へ入力</li>
</ul>
</li>
<li>ハイパーパラメータの追加：Growth_rate</li>
</ul>
<h3 id="layer-正規化instance-正規化">Layer 正規化/Instance 正規化</h3>
<p>vault backup: 2023-07-26 17:01:06 Ken_obsidian</p>
<p>Affected files:</p>
<p>バッチ正規化はミニバッチ単位で Normalization するのでバッチサイズが小さいときに学習が収束しない問題が起こる。
その際には Layer Normalization などを使う</p>
<h4 id="正規化手法のイメージ比較">正規化手法のイメージ比較</h4>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230707184623.png" alt="/img/Pasted_image_20230707184623.png">
（講義スライドより引用）
着色されてる範囲のデータを使って平均、分散を求めて正規化する。</p>
<h4 id="特徴">特徴</h4>
<ul>
<li>Batch 正規化
<ul>
<li>正規化の方法：空間方向とサンプル方向でデータを集計して、各チャンネルに対して正規化を適用</li>
<li></li>
</ul>
</li>
<li>Layer 正規化
<ul>
<li>正規化の方法：空間方向とチャンネル方向でデータを集計して、各サンプルごとに正規化を適用</li>
<li>バッチ数が少なくても適用できる</li>
<li>入力データのスケールに対してロバスト</li>
<li>重み行列のスケールやシフトに対してロバスト</li>
</ul>
</li>
<li>Instance 正規化
<ul>
<li>正規化の方法：空間方向だけでデータを集計して、チャンネルとサンプルごとに正規化を適用</li>
<li>コントラストの正規化に寄与する</li>
</ul>
</li>
</ul>
<h3 id="wavenet">Wavenet</h3>
<h4 id="概要-2">概要</h4>
<ul>
<li>生の音声波形を生成する深層学習モデル</li>
<li>Pixel CNN という高解像度の画像を精密に生成する手法を音声に応用</li>
<li>Dilated Convolution をリンクを離しながら時系列データに適用→需要野の拡大
<a href="https://gigazine.net/news/20171005-wavenet-launch-in-google-assistant/">https://gigazine.net/news/20171005-wavenet-launch-in-google-assistant/</a>
<a href="https://qiita.com/MasaEguchi/items/cd5f7e9735a120f27e2a">https://qiita.com/MasaEguchi/items/cd5f7e9735a120f27e2a</a>
<a href="https://www.slideshare.net/NU_I_TODALAB/wavenet-86493372">https://www.slideshare.net/NU_I_TODALAB/wavenet-86493372</a></li>
</ul>
<!-- raw HTML omitted -->
<h3 id="確認問題">確認問題</h3>
<h4 id="問題-1">問題 1</h4>
<p>MobileNet のアーキテクチャ</p>
<ul>
<li>Depthwise Separable Convolution という手法を用いて計算量を削減し ている。通常の畳込みが空間方向とチャネル方向の計算を同時に行うの に対して、Depthwise Separable Convolution ではそれらを Depthwise Convolution と Pointwise Convolution と呼ばれる演算によって個別に行 う。</li>
<li>Depthwise Convolition はチャネル毎に空間方向へ畳み込む。すなわち、 チャネル毎に $D_K×D_K$×１のサイズのフィルターをそれぞれ用いて計算を 行うため、その計算量は（い）となる。</li>
<li>次に Depthwise Convolution の出力を Pointwise Convolution によってチャネル方 向に畳み込む。すなわち、出力チャネル毎に１×１×M サイズのフィルターをそ れぞれ用いて計算を行うため、その計算量は（う）となる</li>
</ul>
<h5 id="回答">回答</h5>
<p>入出力画像の縦サイズ： H<br>
入出力画像の横サイズ： W<br>
フィルターのサイズ： $D_K​\times D_K​$
フィルタ数： M</p>
<p>としたとき</p>
<p>（い）：　$W\times H \times D_K \times D_K \times C$　で通常の畳み込み計算の $\frac{1}{M}$ となる</p>
<p>（う）：$W\times H \times C\times M$　で通常の畳み込み計算の $\frac{1}{D_K​\times D_K​}$ となる</p>
<h4 id="問題-2">問題 2</h4>
<p>深層学習を用いて結合確率を学習する際に、効率的に学習が行えるアーキテクチャを提案したことが WaveNet の大きな貢献の１つである。
提案された新しい Convolution 型アーキテクチャは（あ）と呼ばれ、結合確率を効率的に学習できるようになっている。</p>
<ol>
<li>Dilated Causal Convolution</li>
<li>Depthwise Separable Convolution</li>
<li>Pointwise Convolution</li>
<li>Deconvolution</li>
</ol>
<h5 id="回答-1">回答</h5>
<p>１．　WaveNet で提案されたのは時系列データに対して Dilated Convolution を適用すること。
2.3 は MobileNet において計算量の効率化につかわれ、4 はセグメンテーション関連で DeconvNet などで活用されるアーキテクチャ</p>
<h4 id="問題-3">問題 3</h4>
<p>（あ）を用いた西尾大きな利点は単純な Convolution Layer と比べて（い）ことである。</p>
<ol>
<li>パラメータ数に対する受容野が広い</li>
<li>受容野あたりのパラメータ数が多い</li>
<li>学習時に並列計算が行える</li>
<li>推論時に並列計算が行える</li>
</ol>
<h5 id="回答-2">回答</h5>
<p>正解は１、Dilated Convolution はカーネルに隙間を空ける構造をしているので、パラメータあたりの受容野が大きくなる
２は１と逆なので✗
学習、推論時に並列計算を行えるのは GPU を用いた CNN 共通の話で Dilated Convolution に限った話ではない。</p>
<h3 id="参考文献レポート-3">参考文献レポート</h3>
<p>黒本において MobileNet の問題が P232 に、DenseNet にかんする言及が P234 の問題にあった、
MobileNet に関しては計算効率化の Depthwise と Pointwise の内容が中心。
DenseNet の問題に関しては ResNet との違いで、特徴量マップを結合していくことがあげられている。</p>
<p>P335 に WaveNet について問題が掲載されていた。
Causal 畳み込みはモデルがデータの時間順序をやぶらないことの保証ができる。
再帰計算がない分高速
Causal 畳み込みと同等の処理にマスクテンソルと畳み込みフィルタの要素積をとってから畳み込みをおこなうマスク畳み込みがある。</p>
<!-- raw HTML omitted -->
<h2 id="resnet">ResNet</h2>
<h3 id="resnet-の構造">ResNet の構造</h3>
<p>中間層の繰り返し部分では BN-&gt;ReLU-&gt;Conv が速度と精度の面で有利とのこと
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230710095051.png" alt="/img/Pasted_image_20230710095051.png">
（講義スライドより引用）</p>
<h4 id="skip-connection">Skip Connection</h4>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230710095249.png" alt="/img/Pasted_image_20230710095249.png"></p>
<p>$$ H(x) = F(x) + x$$</p>
<p>※H(x) が中間層の出力、学習部分は F(x) となる</p>
<p>深い NN の学習を可能に
- 勾配消失の回避
- 勾配爆発の回避</p>
<h4 id="bottleneck-構造">Bottleneck 構造</h4>
<p>1×1 の畳み込みによる Bottleneck 構造により同一の計算コストで層数を増やした</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230710095845.png" alt="/img/Pasted_image_20230710095845.png">
256 チャンネルを一度 64 チャンネルに落とし込むことで計算量を軽くしている
（講義スライドより引用）</p>
<h3 id="wide-resnet">Wide ResNet</h3>
<ul>
<li>層数を浅くして、パラメータ数を増やした
<ul>
<li>深くする（シーケンスのステップを増やす）より広くする（並列に展開する量を増やす）方が GPU の特性に合っている⇒高速化</li>
<li>Skip Connection だけでは、ブロックを強制的に経由する仕組みが無いため、無意味なブロックができてしまう</li>
</ul>
</li>
<li>Residual ブロックの中に DropOut を導入
パラメータを増やすことによる過学習を避けるための正則化、
原著論文内において、すでに Batch Normalization は入っているが、活用するには DataAugumentation が必要になるためそれを避けるために DropOut を導入したとある
（Batch Normalization を効かせるには各チャンネル（対応特徴量）ごとのサンプル数が必要になるため、不均衡データなどを考えるとそれだけに頼らないほうがいいのでは無いかと考えられる。）</li>
</ul>
<p>WRN-n-k : 全部で n 層の畳み込みをもち、Resnet の特徴マップのチャネル数の k 倍のチャネル数 をもつ Wide Residual Networks
(パラメータの数と計算量は k の二乗になる。k=1 のとき、もとの ResNet と同じで、k&gt;1 のとき WideResNet となる。)</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713151123.png" alt="/img/Pasted_image_20230713151123.png">
(設定別の処理時間と精度、S. Zagoruyko and N. Komodakis, “Wide Residual Networks,&ldquo;<a href="https://arxiv.org/pdf/1605.07146.pdf%22">https://arxiv.org/pdf/1605.07146.pdf"</a>)</p>
<h3 id="参考文献レポート-4">参考文献レポート</h3>
<p>認定テストにおいて、パラメータ数を固定したときに、残差ブロック内の畳み込みの層数を変更したときの問題があった。
ブロック内の層数を増やすとエラー率が上がる理由は、原著論文内において、おそらく残差結合の数が減る事によって最適化が困難になったためとされ得ている。
S. Zagoruyko and N. Komodakis, “Wide Residual Networks,&ldquo;<a href="https://arxiv.org/pdf/1605.07146.pdf%22">https://arxiv.org/pdf/1605.07146.pdf"</a></p>
<!-- raw HTML omitted -->
<h2 id="efficientnet">EfficientNet</h2>
<h3 id="導入">導入</h3>
<p>2019 年の EfficientNet までは CNN のスケールアップで精度を改善してきたが
モデルが複雑で高コストという問題があった</p>
<p>⇒　モデルスケーリングの法則を見つけて効率化したい
⇒　複合係数（Conpound Coefficient）</p>
<h3 id="性能">性能</h3>
<ul>
<li>パラメータ数、計算量は数分の１から 1/10 くらいに削減</li>
<li>同程度の計算量の ResNet-50 と比べて 6.3% 精度改善</li>
<li>転移学習で性能を発揮（過剰なパラメータがあることによる過学習が抑えられた結果、汎用性が上がったと思われる）</li>
</ul>
<h3 id="複合スケーリング手法">複合スケーリング手法</h3>
<h4 id="スケーラー">スケーラー</h4>
<ul>
<li>Depth (d):　モデルの表現力を上げる、特に次数を上げる形になるので、複雑な特徴を捉えられるようになる</li>
<li>Width (w、チャンネル):　細かい特徴表現を学習、深さ対比で幅が広すぎると高レベルな特徴量を学習しにくくなる</li>
<li>Resolution (r、幅・高さ): 高解像度の入力を与えることで画像中の詳細なパターンを獲得できる</li>
</ul>
<h5 id="スケーラの扱い">スケーラの扱い</h5>
<p>$$
\begin{aligned}
&amp; \text { depth: } d=\alpha^\phi \
&amp; \text { width: } w=\beta^\phi \
&amp; \text { resolution: } r=\gamma^\phi \
&amp; \text { s.t. } \alpha \cdot \beta^2 \cdot \gamma^2 \approx 2 \
&amp; \alpha \geq 1, \beta \geq 1, \gamma \geq 1
\end{aligned}$$</p>
<ul>
<li>単一の係数Φによりネットワークのスケーリング
畳み込みの演算量FLOPSは$d,w^2,r^2$に比例
（CNNにおいては畳み込み演算が計算コストを占領するため、d,w,rを制御するのが良い）</li>
<li>制約条件の式より、ネットワークのFLOPSは約$2^\phi$でスケーリング</li>
<li>アルファ、ベータ、ガンマはグリッドサーチにより適切な値を定数として扱う</li>
</ul>
<h4 id="最適化問題">最適化問題</h4>
<p>目的関数をAccuracyとして、
使用するメモリ容量と、処理能力（FLOPS）の制約を満たす条件下で最大化を目指す</p>
<p>$$</p>
<p>\begin{array}{ll}\
\max <em>{d, w, r}&amp;\operatorname{Accuracy}(\mathcal{N}(d, w, r))\
\text { s.t. }&amp;\mathcal{N}(d, w, r)=\bigodot</em>{i=1 \ldots s} \hat{\mathcal{F}}_i^{d \cdot \hat{L}<em>i}\left(X</em>{\left\langle r \cdot \hat{H}_i, r \cdot \hat{W}_i, w \cdot \hat{C}_i\right\rangle}\right)\
&amp;\operatorname{Memory}(\mathcal{N})\leq\text { target_memory } \
&amp;\operatorname{FLOPS}(\mathcal{N})\leq\text { target_flops } \
\end{array}</p>
<p>$$</p>
<ul>
<li>制約条件の第一式はネットワークの定義式</li>
<li>Residualブロックなどの処理ブロック一つをステージとしてiを用いて表現</li>
<li>ハットマークはスケーラの1単位あたりの量をしめす</li>
<li>Lについては１ステージあたりの畳み込み演算回数</li>
</ul>
<h4 id="参考文献レポートスケーリングと精度の対応">参考文献レポート（スケーリングと精度の対応）</h4>
<p>原著論文において、個別のスケーラーごとに計算量を上げたときの正解率と提案された複合スケーラでの計算量を上げたときの正解率があったので、掲載する。</p>
<p>計算量を上げたときのパフォーマンス向上は複合スケーラが勝っていることは明らかで、
単一のスケーラの向上は頭打ちが見られたものの、複合スケーラの方ではまだ精度が上がる余地があるように思われる。</p>
<p>＜イメージネットとスケーラの対応＞
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230711163912.png" alt="/img/Pasted_image_20230711163912.png"></p>
<p>&lt;EfficientNetとスケーラ、複合スケーラの対応&gt;
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230711172121.png" alt="/img/Pasted_image_20230711172121.png">
(M. Tan and Q. V. Le, “EfficientNet: Rethinking model scaling for convolutional neural networks,” in Proceedings of the 36th International Conference on Machine Learning, Long Beach, CA, USA, 2019, pp. 6105-6114.)</p>
<!-- raw HTML omitted -->
<h2 id="物体検知とss解説">物体検知とSS解説</h2>
<h3 id="物体認識周りのタスクの概観">物体認識周りのタスクの概観</h3>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230711172347.png" alt="/img/Pasted_image_20230711172347.png">
※SemanticとInstanceの違いは同属性・別個体を分けるかどうか
（講義スライドより引用）</p>
<h3 id="データセット">データセット</h3>
<p>物体検出コンペで使われたデータセットの比較</p>
<table>
<thead>
<tr>
<th>データセット名$^{※1}$</th>
<th>クラス数$^{※2}$</th>
<th>Train+Val</th>
<th>Box/画像$^{※3}$</th>
<th>画像サイズ</th>
<th>BB形式</th>
<th>Segmentation data</th>
</tr>
</thead>
<tbody>
<tr>
<td>VOC12</td>
<td>20</td>
<td>11540</td>
<td>2.4</td>
<td>470 × 380</td>
<td> (x_min, y_min, x_max, y_max)</td>
<td>無</td>
</tr>
<tr>
<td>ILSVRC17</td>
<td>200</td>
<td>476668</td>
<td>1.1</td>
<td>500 × 400</td>
<td> (x_min, y_min, x_max, y_max)</td>
<td>無</td>
</tr>
<tr>
<td>MS COCO18</td>
<td>80</td>
<td>123287</td>
<td>7.3</td>
<td>640 × 480</td>
<td>(x_min、y_min、width、height)</td>
<td>有</td>
</tr>
<tr>
<td>OICID18</td>
<td>500</td>
<td>1743042</td>
<td>7</td>
<td>一様ではない</td>
<td>(x_min, y_min, x_max, y_max)</td>
<td>無</td>
</tr>
<tr>
<td>（講義スライドより引用）</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>※1:CIFAR-10/CIFAR-100, Food-101, 楽天データ（文字領域アノテーション画像）など他にもデータセットは存在する
※2:クラス数が大きいことは細かすぎる違いがあるケースもあって、必ずしも良いこととは言えない
→ラベル変換すれば良いので、どうとでもできる
※3:Box/画像の値は、大きいほど日常生活に近い重なりを含んだ画像になり、小さいほどアイコニックで検出し易い画像になる</p>
<p>&lt;代表的データセットのポジショニングマップ&gt;
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230711182437.png" alt="/img/Pasted_image_20230711182437.png">
（講義スライドより引用）</p>
<h3 id="評価指標">評価指標</h3>
<p>IoUを固定してmAPを算出する
IoUを変化させたときの平均を取った$mAP_{COCO}$という指標もある</p>
<h4 id="クラス分類">クラス分類</h4>
<p>vault backup: 2023-07-26 17:01:06 Ken_obsidian</p>
<p>Affected files:</p>
<p>vault backup: 2023-07-26 17:01:06 Ken_obsidian</p>
<p>Affected files:</p>
<p>モデルの利用目的ごとに適切な閾値の設定を行う</p>
<h4 id="bounding-boxの予測性能">Bounding Boxの予測性能</h4>
<p>指標：IoU （Intercection over Union）　※別名Jaccard係数</p>
<p>※Ground Truthの中のTPでもなく、予測したBounding BoxのなかのGTでもないことに注意</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230711193224.png" alt="/img/Pasted_image_20230711193224.png">
（講義スライドより引用）
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230711193512.png" alt="/img/Pasted_image_20230711193512.png">
（講義スライドより引用）</p>
<h4 id="実利用上の指標">実利用上の指標</h4>
<p>検出速度:FPS</p>
<h3 id="物体検知の流れ">物体検知の流れ</h3>
<p>AlexNet以前はSIFT（Scale Invariant Feature Transform:スケール不変特徴量変換）が主流だったが
現在はDCNNが主流</p>
<h4 id="ネットワークの発展20122018">ネットワークの発展　2012~2018</h4>
<ol>
<li>AlexNet（DCNN)</li>
<li>Inceptionモジュール</li>
<li>Residualブロック</li>
<li>Denseブロック</li>
<li>MobileNetなどの軽量化</li>
</ol>
<h4 id="物体検知フレームワークの発展-20132018">物体検知フレームワークの発展 2013~2018</h4>
<ol>
<li>DetectorNet</li>
<li>RCNN</li>
<li>Fast,FasterRCNN</li>
<li>YOLO</li>
<li>SSD</li>
</ol>
<h5 id="検出ステップ数での分類">検出ステップ数での分類</h5>
<ul>
<li>2段階検出器：候補領域の検出とクラス推定を別々に行う
<ul>
<li>精度は高い傾向</li>
<li>計算量が大きく推論も遅い傾向</li>
</ul>
</li>
<li>1段階検出器：候補領域の検出とクラス推定を同時に行う
<ul>
<li>精度が低い傾向</li>
<li>計算量が小さく推論も速い傾向</li>
</ul>
</li>
</ul>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230712121829.png" alt="/img/Pasted_image_20230712121829.png">
(P. Poirson, P. Ammirato, C.-Y. Fu, W. Liu, J. Kosecka, and A. C. Berg, “Fast single shot detection and pose estimation,” arXiv preprint arXiv:1609.05590, 2016.)</p>
<h3 id="single-shot-detector">Single Shot Detector</h3>
<h4 id="ssdの概観">SSDの概観</h4>
<ol>
<li>Default Boxを用意</li>
<li>検出物体に合わせてDefault Boxを変形して、Confidenceを出力</li>
</ol>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230712122722.png" alt="/img/Pasted_image_20230712122722.png">
（講義スライドより引用）</p>
<h4 id="ネットワークアーキテクチャ">ネットワークアーキテクチャ</h4>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230712121934.png" alt="/img/Pasted_image_20230712121934.png"></p>
<h4 id="特徴マップからの出力">特徴マップからの出力</h4>
<p>各クラスのConfidence＋デフォルトボックスの調整項がマップ別に設定される特徴量あたりのDefault Box個数分あって、それを特徴マップサイズ分持つ</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230712235240.png" alt="/img/Pasted_image_20230712235240.png"></p>
<p>※SSDでは層ごとに特徴量あたりのDefault Box数を変更しているが、これは計算量との兼ね合いで変更しているだけで特に深い意味があるわけではない</p>
<h4 id="defaultboxが多いときに発生する問題とその対応">DefaultBoxが多いときに発生する問題とその対応</h4>
<ul>
<li>Non-Maximum Suppression:　一つの物体しかなくても、複数のDefault Boxが生成されて冗長になってしまう
→IoUで一定以上のモノはConfidenceが高いものを残して削除する</li>
<li>Hard Negative Mining:背景と非背景の不均衡が発生する
→割合に制限をかけて背景の生成を抑える
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713004203.png" alt="/img/Pasted_image_20230713004203.png">
（講義スライドより引用）</li>
</ul>
<h4 id="損失関数">損失関数</h4>
<p>クラス分類に関する損失と、物体検出した位置に関する損失をあわせて評価する</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713084628.png" alt="/img/Pasted_image_20230713084628.png"></p>
<h3 id="semantic-segmentation">Semantic Segmentation</h3>
<p>ピクセルごとにクラス情報を持つためには、畳み込みでダウンサンプリングしてきたものを入力と同サイズまでアップサンプリングする必要がある</p>
<p>ダウンサンプリングしないという選択肢は受容野と計算量、メモリの兼ね合いから難しいため、
うまくアップサンプリングしていく必要がある</p>
<ul>
<li>
<p>転置畳み込み（逆畳み込み）</p>
<ol>
<li>入力特徴マップのpixel間隔をStride分あける</li>
<li>特徴マップの外周に(kernelsize-1) - padding分だけ余白を足す</li>
<li>畳み込み演算を行う
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713003255.png" alt="/img/Pasted_image_20230713003255.png">
（講義スライドより引用：k=3,p=1,s=1で青が入力、灰色がカーネル、緑が出力）</li>
</ol>
</li>
<li>
<p>輪郭情報の補完</p>
<ul>
<li>Pooling層の出力をelement-wise addition　（FCN）</li>
<li>スキップ接続　Encoderの特徴量マップを対応するデコーダーの入力に連結する　（U-Net）
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713001657.png" alt="/img/Pasted_image_20230713001657.png"></li>
<li>Unpooling:Pooling時に採用した位置のインデックス情報を保持しておいてアップサンプリング時に活用
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713001902.png" alt="/img/Pasted_image_20230713001902.png">
DeconvNet（上）とSegNet（下）
※SegNetではUpSamplingにおいて、逆畳み込みは行っておらず、Unpoolingのみ、これによってメモリ効率がよく高速に動作する
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713001844.png" alt="/img/Pasted_image_20230713001844.png"></li>
</ul>
</li>
</ul>
<h4 id="cnnと受容野">CNNと受容野</h4>
<p>猫はどちらか？
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230712235904.png" alt="/img/Pasted_image_20230712235904.png">
（講義スライドより引用）</p>
<h5 id="受容野を広げる方法">受容野を広げる方法</h5>
<ol>
<li>Pooling (ストライド)</li>
<li>畳み込み
<ol>
<li>層を深くする　※計算量、メモリの問題がでる</li>
<li>Dilated Convolution　（畳み込みの際に入力を隙間を空けて要素を取るようにする）
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713001055.png" alt="/img/Pasted_image_20230713001055.png">
（講義スライドより引用）</li>
</ol>
</li>
</ol>
<h3 id="参考記事レポート">参考記事レポート</h3>
<p>認定テスト内で、SSDについて正誤を問う内容がでた。
SSDが小さな物体検出が苦手かどうか、VOC07とVOC12のクラス数について迷ったが、
原著論文&quot;https://arxiv.org/pdf/1512.02325.pdf&rdquo;　において、オブジェクトのサイズと精度の比較が出ていた。
（新しいデータ拡張で改善できたという文脈）
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230718175028.png" alt="/img/Pasted_image_20230718175028.png">
（論文より引用）</p>
<p>認定テストでデータセットに関する問題が出たため、以下の参考サイトで確認
OICID 2018:&ldquo;<a href="https://github.com/openimages/dataset/blob/main/READMEV3.md%22">https://github.com/openimages/dataset/blob/main/READMEV3.md"</a>
&ldquo;<a href="https://storage.googleapis.com/openimages/web/factsfigures_v7.html%22">https://storage.googleapis.com/openimages/web/factsfigures_v7.html"</a>
ILSVRC17:&ldquo;<a href="https://www.kaggle.com/competitions/imagenet-object-localization-challenge/data%22">https://www.kaggle.com/competitions/imagenet-object-localization-challenge/data"</a> Pascal VOC形式でバウンディングボックスを表現しているとされているが、kaggle特有なのか、もとからそうなのか不明
MS COCO: <a href="https://cocodataset.org/#format-results">https://cocodataset.org/#format-results</a></p>
<p>フリマアプリの利用に向くかどうかは、アイコニックな画像なのかどうかの基準で、講義で触れられたBB/画像の値で判定し、Segmentationデータの有無は参考サイトをもとにして要約中の表に記載した。</p>
<!-- raw HTML omitted -->
<h2 id="mask-r-cnn">Mask R-CNN</h2>
<h3 id="r-cnn-reginal-cnn">R-CNN (Reginal CNN)</h3>
<ol>
<li>関心領域　Region of Interestを切り出す</li>
<li>切り出した関心領域の画像のサイズを調整</li>
<li>CNNによって特徴量を求める
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713085050.png" alt="/img/Pasted_image_20230713085050.png">
（講義スライドより引用）</li>
</ol>
<p>問題点：関心領域の切り出しで１秒、CNNで0.22秒という処理時間、リアルタイム処理には遠い</p>
<h3 id="fast-r-cnn">Fast R-CNN</h3>
<p>RCNN：関心領域ごとにCNNに入力
Fast R-CNN ：画像全体をCNNに入力して、特徴量を抽出したFeature Mapに対してROIに対応する部分を抽出、サイズ調整するROI Poolingを行う</p>
<p>→　計算量を大幅に削減</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713085612.png" alt="/img/Pasted_image_20230713085612.png"></p>
<p>※ROI poolingでは入力画像と特徴量マップの解像度が異なるために、実際の画像のROIとROI Poolingで対応する領域が異なる問題がある
（参考：&ldquo;<a href="https://qiita.com/yu4u/items/5cbe9db166a5d72f9eb8%22">https://qiita.com/yu4u/items/5cbe9db166a5d72f9eb8"</a>）</p>
<h4 id="roi-pooling">ROI Pooling</h4>
<ol>
<li>Region Proposal の座標を整数に丸めて対応する領域定める</li>
<li>領域を固定サイズになるように等分する</li>
<li>等分された各領域に含まれるピクセルの値の平均か最大を用いて各領域の値を決定する
※このとき各ピクセルは等分された領域のいずれか一つに割り当てられる</li>
</ol>
<p>１の丸めと、３のピクセルの割り当て方によって位置ずれが発生する</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713091854.png" alt="/img/Pasted_image_20230713091854.png">
（講義スライドより引用）</p>
<h3 id="faster-r-cnn">Faster R-CNN</h3>
<p>関心領域の切り出しもCNNで行う　（Region Proposal Network）
→リアルタイムで動作、動画認識への応用が可能に　and End-to-Endでの学習</p>
<p>※R-CNN系の処理の処理時間はCNN以外の部分が大半を占めていたため、CNNで完結することでかなりの高速化がなされた</p>
<h4 id="region-proposal-network">Region Proposal Network</h4>
<ol>
<li>特徴マップ上のすべての点にAnchor Pointを設定し、そのPointごとにAnchor BoxをS個作成
※特徴マップがH×WならAnchor PointはH×W
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713104405.png" alt="/img/Pasted_image_20230713104405.png"></li>
<li>Ground TruthとBoxを比較して、背景と物体の2値と座標のズレ（中心、幅、高さ）の4変数を出力する</li>
</ol>
<h3 id="mask-r-cnn-1">Mask R-CNN</h3>
<p>&ldquo;<a href="https://arxiv.org/abs/1703.06870%22">https://arxiv.org/abs/1703.06870"</a></p>
<ul>
<li>Faster R-CNNの拡張</li>
<li>物体検出結果で得られた領域に限定してセグメンテーションを行うことで効率アップ</li>
</ul>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713090927.png" alt="/img/Pasted_image_20230713090927.png">
（講義スライドより引用）</p>
<h4 id="roi-align">ROI Align</h4>
<p>[ROI Pooling](<a href="https://half-broken-engineer.github.io//ROI">https://half-broken-engineer.github.io//ROI</a> Pooling)の位置ずれ問題に対応</p>
<ol>
<li>(丸め込みは行わない)</li>
<li>N×Nの固定領域の際は候補領域を縦横でN等分ずつにする</li>
<li>区切られた各領域内に４つ点を設定する　</li>
<li>各点の値をBilinier Interpolationで最近傍の４ピクセルの値を使って算出する</li>
<li>４点の値を平均または最大で代表値にまとめる
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713101440.png" alt="/img/Pasted_image_20230713101440.png">
(Mask R-CNN, &ldquo;K. He, G. Gkioxari, P. Dollar, and R. Girshick, in Proc. of ICCV, 2017.より引用)</li>
</ol>
<h3 id="参考文献レポート-5">参考文献レポート</h3>
<p>ROI Alignについて、点の決定方法が曖昧だったため、原著論文等を確認した、
原著論文によると、点の数や位置の影響は小さいとのことで、適当に決めて良いようである。
４点取らずとも単純に区切った領域の中心でInterpolationしても精度が出るとのこと、
&ldquo;<a href="https://arxiv.org/pdf/1703.06870.pdf%22">https://arxiv.org/pdf/1703.06870.pdf"</a></p>
<p>黒本において、P241にR-CNNの問題があった。
Selective Searchの方法、検知速度が遅い問題から、Fast,Faster R-CNNの内容が問われた。</p>
<!-- raw HTML omitted -->
<h2 id="fcos-fully-convolutional-one-stage-detection">FCOS Fully Convolutional One-Stage Detection</h2>
<p>問題意識：Bounding Boxを出力する前に大量のAnchor Boxを作成している</p>
<ol>
<li>ハイパーパラメータとなっていて設定次第で精度が大きく変わってしまう</li>
<li>角度やサイズなどの影響が強くでてしまう</li>
<li>前景と背景の不均衡</li>
</ol>
<h3 id="手法-1">手法</h3>
<p>FPN(Feature Pyramid Networks)：複数サイズの特徴マップを利用
→全体特徴を捉えると同時に細かいローカルな特徴も捉える
→画像上の位置的に重なっているクラスも別のレベルのマップで捉えられる</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713105308.png" alt="/img/Pasted_image_20230713105308.png">
（講義スライドより引用）</p>
<h4 id="出力">出力</h4>
<ul>
<li>ピクセルごとのクラスラベル</li>
<li>各ピクセルから物体領域のボックスの4辺までの距離</li>
</ul>
<p>ポジティブサンプルとネガティブサンプルの分け方
→ラベルの領域にピクセルが含まれる∧クラスが一致</p>
<p>centerness:縦横の4辺までの距離を大きい方で小さい方を割った値の幾何平均</p>
<h4 id="損失関数-1">損失関数</h4>
<p>クラス間のサンプルの不均衡を考慮したFocal Lossと位置ずれに関する損失関数IoU Lossを計算している (GT:Ground Truth,PA:Predicted Area)</p>
<p>　$$Focal_Loss(p_t) = -(1-p_t)^\gamma log(p_t)$$
　$$IoU_Loss = 1 - \frac{GT\cap PA}{GT\cup PA}$$</p>
<h3 id="参考文献レポート-6">参考文献レポート</h3>
<p>Focal Lossについて提案論文で確認した
論文中では$\gamma=2$となっている,$\gamma=0$はクロスエントロピーロスになる
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713112401.png" alt="/img/Pasted_image_20230713112401.png">
(T.-Y. Lin, P. Goyal, R. Girshick, K. He and P. Dollar, “Focal Loss for Dense Object Detection,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 42, no. 2, pp. 318-327, 2020.&ldquo;<a href="https://arxiv.org/pdf/1708.02002.pdf%22">https://arxiv.org/pdf/1708.02002.pdf"</a>)</p>
<p>またセグメンテーションに使われる損失関数にどういったものがあるのか確認した。</p>
<h5 id="セグメンテーション周りの損失関数まとめ">セグメンテーション周りの損失関数まとめ</h5>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713112549.png" alt="/img/Pasted_image_20230713112549.png"></p>
<p><a href="https://github.com/JunMa11/SegLoss">&ldquo;https://github.com/JunMa11/SegLoss&rdquo;</a></p>
<!-- raw HTML omitted -->
<h2 id="transformer">Transformer</h2>
<h3 id="rnnと言語モデル">RNNと言語モデル</h3>
<p>言語モデル：単語の並びに確率を与える（単語の並びが文章として自然なのかを確率で評価する）
同時確率を事後確率の総乗で表現する</p>
<p>$$P(w_1,w_2,\dots,w_m) = \overset{m}{\underset{i=1}{\prod}}P(w_i|w_1,w_2,\dots,w_{i-1})$$
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713113903.png" alt="/img/Pasted_image_20230713113903.png">
（講義スライドより引用）</p>
<h3 id="seq2seq">Seq2Seq</h3>
<p>Encoder RNNで文脈ベクトル（最終内部状態）を生成して、それをDecoder RNNの初期内部状態にして単語生成していく</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713114143.png" alt="/img/Pasted_image_20230713114143.png">
（講義スライドより引用）</p>
<h4 id="beam-search">Beam Search</h4>
<p>各時刻において選ぶ単語をk個にして、各時刻において確率が高いものから順にk個の出力系列を保持してデコーダー出力を出していく</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713183338.png" alt="/img/Pasted_image_20230713183338.png">
(&ldquo;<a href="https://data-analytics.fun/2020/12/16/understanding-beamsearch/#toc3%22">https://data-analytics.fun/2020/12/16/understanding-beamsearch/#toc3"</a> より引用,ｋ＝３、ボキャブラリーが5の例)</p>
<h4 id="encoder-decoderモデルの弱点">Encoder-Decoderモデルの弱点</h4>
<p>翻訳本の文章を一つのベクトルで表現する
→入力系列が長くなるとベクトルの表現力が足りなくなる
（系列が長くなったときに統計的機械翻訳モデルに比べて顕著に精度低下が見られる）</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713184234.png" alt="/img/Pasted_image_20230713184234.png">
（講義スライドより引用）</p>
<p>どうするのか？</p>
<p>→　Attention機構</p>
<h4 id="attention機構">Attention機構</h4>
<p>簡単にいうと辞書オブジェクト</p>
<p>入力の検索クエリに一致するKeyをボキャブラリから索引して、対応するValueを取り出す操作</p>
<p>固定次元の単語ベクトルにEmbedingされた各トークンを線形写像によってValue,Query,Keyに変換する</p>
<ul>
<li>Value: そのトークンから得られる「値」</li>
<li>Query: 関連の強い別のトークン（自分自身も含む）を知り、そのValueを得るための「問い合わせ」</li>
<li>Key: 自分自身や別のトークンのQueryから問い合わせを受ける「索引」
問い合わせてきたQueryからの関連性を計算し、それに応じて自身に結びついたValueを返すために存在</li>
</ul>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713184703.png" alt="/img/Pasted_image_20230713184703.png">
（講義スライドより引用）</p>
<p>Attentionの導入によって、精度が低下しなくなった。
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713184917.png" alt="/img/Pasted_image_20230713184917.png">
（講義スライドより引用）</p>
<h3 id="transformer-1">Transformer</h3>
<ul>
<li>2017 June</li>
<li>RNNを使わない　Attentionだけ</li>
<li>3600万文を８GPUで3.5日など圧倒的に低いコストでSOTAを実現</li>
</ul>
<h4 id="ネットワークアーキテクチャ-1">ネットワークアーキテクチャ</h4>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713185201.png" alt="/img/Pasted_image_20230713185201.png">
（講義スライドより引用）</p>
<h4 id="2種類のattention">2種類のAttention</h4>
<p>2種類のAttention があるが、Transformerで使われるのはSelf-Attention</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713185304.png" alt="/img/Pasted_image_20230713185304.png">
（講義スライドより引用）</p>
<p>Self Attentionは、どの時刻のエンコードを行う場合も入力は入力系列全体で同じ
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713185923.png" alt="/img/Pasted_image_20230713185923.png"></p>
<h4 id="point-wise-feed-forward-networks">Point-Wise Feed-Forward Networks</h4>
<p>位置情報を保持したまま順伝播させる（出力の形状が入力と同じになる）ために、入力に対して掛ける重みの行列のサイズを定める</p>
<p>※512は系列長、2048は隠れ層のノード数</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713190117.png" alt="/img/Pasted_image_20230713190117.png"></p>
<h4 id="scaled-dot-product-attention">Scaled dot product attention</h4>
<ol>
<li>入力系列をQuery,Key,Value行列に線形変換</li>
<li>AttentionのためのqueryとKeyの行列積　（内積によって関連の強さを返す）</li>
<li>Softmaxで勾配消失することを防ぐためのスケーリング　（ベクトル次元数の平方根で割る）</li>
<li>Unknownなどの特別なトークンを処理するためのマスク機構</li>
<li>ソフトマックスで全確率が１になるようにする</li>
<li>valueを対応する確率で荷重平均する　（行列積）
※下図のq1,k1,v1などはベクトルでスカラーではないことに注意
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713190101.png" alt="/img/Pasted_image_20230713190101.png">
（講義スライドより引用）</li>
</ol>
<h4 id="multi-head-attention">Multi-Head Attention</h4>
<p>畳み込みのチャネル違いのようなモノ、異なる特徴を捉える</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713190345.png" alt="/img/Pasted_image_20230713190345.png"></p>
<h4 id="positional-encoding">Positional Encoding</h4>
<p>RNNと違い、Attention機構のみだと、単語の位置情報を保持できない
→Positional Encodingを行う</p>
<p>Positional Encodingとは、</p>
<p>単語の埋め込みベクトル　＋　位置情報ベクトル　を　Attentionへの入力とする</p>
<p>このとき、位置情報ベクトルは</p>
<ul>
<li>文章の長さに関係なく、それぞれの位置が同じ位置ベクトルに対応する必要がある</li>
<li>埋め込みベクトルに対して位置情報ベクトルが大きくなりすぎては行けない
→
$${PE\left( pos,2i\right) =\sin \left( \dfrac{pos}{10000^{2i/d_{model}}}\right)
}$$</li>
</ul>
<p>$${PE\left( pos,2i+1\right) =\cos \left( \dfrac{pos}{10000^{2i/d_{model}}}\right)
}$$</p>
<p>※10000はハイパーパラメータでどれだけ長い文章が来ても同じ値にならないために十分大きな値を取っている
※位置情報ベクトルの系列後半だと（iが大きくなると）周波数が小さな値となり、位置の変化の影響が小さくなる。</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713222737.png" alt="/img/Pasted_image_20230713222737.png">
参考：”Positional Encodingを理解したい - Qiita:https://qiita.com/snsk871/items/93aba7ad74cace4abc62&rdquo;</p>
<p>こちらは絶対位置を利用しているとのことだが、相対位置を利用する方法だと、
データセットで学習したより長い系列情報を入れてデータ生成させても破綻しないとのこと。</p>
<h3 id="考察">考察</h3>
<p>位置符号化でややこしい三角関数を使っているのは
”(普通の)三角関数による出力を位置符号の値として用いようとしてしまうと，周期ごとに同じ値が出力されてしまい，お互い独立した位置符号値を，各pos番目のトークンに与えることができない．”　
(参考：&ldquo;<a href="https://cvml-expertguide.net/terms/dl/seq2seq-translation/transformer/positional-encoding/%22">https://cvml-expertguide.net/terms/dl/seq2seq-translation/transformer/positional-encoding/"</a>)
からとされているが、
ハイパーパラメータの１００００を十分大きくすれば、通常の三角関数でも大丈夫な様に思われる。こちらの方がわかりやすく時系列データに対する１次元畳み込みによる周波数成分抽出で位置情報をNN演算で復元できるように感じるが、
単語ベクトルと合成したときのS/N比や、長い系列に対して周波数を大きくすることで位置が少しズレたときの差分が分からなくなる問題が考えられるが、実際のところはもう少し調べないと分からない。</p>
<h4 id="その他の学習上の工夫">その他の学習上の工夫</h4>
<ul>
<li>Residual Connection　（Add)
<ul>
<li>実装上は入力を出力に加算して次のブロックに渡すだけ
→勾配の安定化によって学習・テスト誤差の低減</li>
</ul>
</li>
<li>Layer Normalization
<ul>
<li>各層においてバイアスを除く活性化関数への入力を平均0,分散１に正規化
→学習の高速化と安定化</li>
</ul>
</li>
</ul>
<h4 id="attentionの可視化">Attentionの可視化</h4>
<p>言語構造が捉えられている様に思われる</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713231704.png" alt="/img/Pasted_image_20230713231704.png">
（講義スライドより引用）</p>
<h3 id="確認問題-1">確認問題</h3>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230715224616.png" alt="/img/Pasted_image_20230715224616.png">
時刻tのデコーダ出力をt+1のデコーダー入力に使っているため、誤差が蓄積していく、連鎖的に大きくなっていく問題があり、学習が安定せず、収束がおそい
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230715224755.png" alt="/img/Pasted_image_20230715224755.png">
誤差が連鎖的に大きくなる問題に対応するために、
一つ前の時刻のターゲットをデコーダー入力とする、Teacher Forcingという手法
こちらは、推論時の分布と訓練時の分布が異なるという問題がある。</p>
<h3 id="参考文献レポート-7">参考文献レポート</h3>
<p>認定テストにおいて、BLEUの演算に関する問題が出たのでまとめる
$$
\mathrm{BLEU}=\mathrm{BP} \cdot \exp \left(\sum_{n=1}^N w_n \log p_n\right)
$$</p>
<p>※対数荷重平均なのは、nが大きくなるほどスコアが指数的に高くなることを防ぐため</p>
<p>$$Brevity Penalty =  exp(1-\frac{r}{c})$$
r：参照文の長さ
c：候補文の長さ</p>
<p>このペナルティは、候補文が単に短いためにmodified precisionが高くなり、BLEUが高くなることを防ぐために導入された
参考：&ldquo;<a href="https://blog.pangeanic.com/ensuring-good-machine-translation-using-bleu-scoring%22">https://blog.pangeanic.com/ensuring-good-machine-translation-using-bleu-scoring"</a></p>
<p>modified n-gram precision
あるngramトークンの候補文における出現回数をCount,
参照訳ごとの出現回数のうち最大値をMax_Ref_Countとして
$$
Count_{clip}=\min (\text { Count }, max_Ref_Count)
$$
$$
p_n = \frac{\sum_{\text {n-gram } \in C} \operatorname{Count}<em>{\text {clip }}(\mathrm{n} \text {-gram })}{\sum</em>{\mathrm{n} \text {-gram } \in C} \operatorname{Count}(\mathrm{n} \text {-gram })}
$$
参考：&ldquo;<a href="https://nryotaro.dev/posts/bleu/%22">https://nryotaro.dev/posts/bleu/"</a></p>
<!-- raw HTML omitted -->
<h2 id="bert">BERT</h2>
<p>Bidirectional Transformers</p>
<h3 id="背景">背景</h3>
<p>自然言語処理タスクは<strong>事前学習が有効</strong></p>
<p>文レベルのタスク：文同士の関係が重要
文章類似度
言い換え表現
トークンレベル：モデルはトークンレベルで良い出力が要求される</p>
<h4 id="事前学習のアプローチ">事前学習のアプローチ</h4>
<ul>
<li>Feature Based
<ul>
<li>N-gram モデル　（１９９２）</li>
<li>Word2Vec（２０１３）</li>
<li>ELMo(2017,2018)
<ul>
<li>Context sensitiveな素性を抽出</li>
<li>既存の素性にConcatして使用することで複数のNLPタスクでSOTAを達成</li>
</ul>
</li>
</ul>
</li>
<li>Fine Tuning：モデルはそこまで巨大ではないのでFine Tuningも現実的な速度で実行可能</li>
</ul>
<h3 id="双方向transformer">双方向Transformer</h3>
<p>転移学習で８つのタスクでSOTAを達成</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230714203842.png" alt="/img/Pasted_image_20230714203842.png">
（講義スライドより引用）</p>
<h4 id="特徴-1">特徴</h4>
<h5 id="入力表現">入力表現</h5>
<ul>
<li>文章のペア　または　文章単体</li>
<li>単語ベクトルの系列
<ul>
<li>WordPieceでTokenizeしたToken Embeding</li>
<li>TransformerでもあったPosition Embeding　（Positional Encoding）</li>
<li>文章のペアを考慮したSegment Embeding</li>
</ul>
</li>
</ul>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230714182503.png" alt="/img/Pasted_image_20230714182503.png">
（講義スライドより引用）</p>
<h4 id="事前学習タスク">事前学習タスク</h4>
<ul>
<li>マスク単語予測タスク:入力系列のうち１５%がMASK対象に選ばれてフラグが付与される。そのうち８０％をMASKトークン、１０％がランダムな他の単語、10%はそのままにして、フラグがついている位置の単語がなにか予測する
<ul>
<li>通常の言語モデルの同時確率＝条件付き確率の時系列での総乗が未来の情報のリークによって使えない
→CBOW(continuous bag-of-words)の考え方を使う$$L = - \frac{1}{M}\underset{i\in{Mask}}{\sum} log P(w_i|W\lnot _{j\in{Mask}})$$</li>
</ul>
</li>
<li>隣接文判定タスク:２つの連なる文章ペアに対して、隣接文を50%の確率でシャッフル→入力された２つの文章が隣接文かの判定をおこなう</li>
</ul>
<h4 id="汎用モデルとしてのbert">汎用モデルとしてのBERT</h4>
<p>見る出力の箇所を変えることで、複数のタスクに対応することが可能</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230714203748.png" alt="/img/Pasted_image_20230714203748.png">
（講義スライドより引用）</p>
<h3 id="参考文献レポート-8">参考文献レポート</h3>
<p>黒本P326 にBERTに関する問題があった。
BERTの構成に関する問題、系列長文並列に並んだTransformer Encoderに前の層のノードすべてが結合している構造がBERTのモノ、系列的に未来の方向のみにつなげているのがOpen AI GPT、LSTMを使っているものがELMO、</p>
<p>事前学習タスクはマスク単語予測、隣接文予測の教師無し学習、再学習については教師あり学習
Positional Encodingに関しても問題として取り上げられていた。</p>
<!-- raw HTML omitted -->
<h2 id="gptgenerative-pre-training">GPT(Generative Pre-Training)</h2>
<p>2019 年にOpenAIが開発した事前学習モデル
→汎用特徴量を学習済みなため、転移学習に使用可能
　　特定タスクのためのデータセットが小規模でも、高精度な予測モデルを実現できる
　　転用の際には、主に下層のファインチューニングを行う（GPT-3以降はファインチューニングしない）
　　モデル構造は変えないことが多い
　　オープンソースが存在する</p>
<p>GPT-2→GPT-3→GPT-4　と後継モデルがある</p>
<p>基本構造はTransformerで
ある単語の次に来る単語を予測して文章を完成できるように教師なし学習を行う
入力：単語系列
出力：次に来る単語の確率</p>
<p>学習前のパラメータはランダム値</p>
<h3 id="スペック">スペック</h3>
<p>GPT-1: 117 million parameters
GPT-2: 1.5 billion parameters
GPT-3: 175 billion parameters
GPT-4: 170 trillion parameters</p>
<p>GPT-4においては入力系列長が25000単語(GPT-3の８倍)まで拡張され、
画像入力も可能になったMulti Modalモデルになっているのが大きなアップデート
NNの記憶機能としては、8,000→64000単語</p>
<p>各種試験において、GPT３系よりも大幅に改善された結果を残している
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230715113653.png" alt="/img/Pasted_image_20230715113653.png">
（※２より引用）</p>
<h3 id="問題点">問題点</h3>
<ul>
<li>フェイクニュースなどの悪用問題
<ul>
<li>GPT-4においては、兵器作成、犯罪などに関するプロンプトへの制限が入っている</li>
</ul>
</li>
<li>まだ精度がでていない分野もある、物理現象に対する推論や慣習・常識に依存する質問回答など</li>
<li>計算リソースの問題：膨大なパラメータが存在するため、事前学習を現実的に実行するために非常に高性能なGPUを要求</li>
</ul>
<h3 id="学習">学習</h3>
<p>GPT-4においては、ライセンスを持つコーパスに加えて、インターネット上のデータを用いている。
インターネット上のデータを用いることで、返答が多岐に渡ってユーザー意図から離れた回答をするのを防ぐためにReinforcement Learning with Human Feedback(RLHF)を導入している</p>
<h4 id="目的関数">目的関数</h4>
<p>言語モデルの最尤推定を行う
→同時確率＝時系列の条件付き確率の総乗　の　対数を取って、尤度を最大化するパラメータΘを求める。
コンテキストウィンドウ（サイズをｋとする）で次の単語を予測する際に見る過去の系列の範囲を定める
$$L(U;\Theta) = \sum log P(u_i|u_{i-k},\dots,u_{i-1};\Theta)$$</p>
<h4 id="ネットワークの演算">ネットワークの演算</h4>
<ol>
<li>次の単語を予測するためのコンテキストウィンドウ分の単語系列UをWeで単語ベクトル空間に埋め込み、位置エンコーディングを付加する$$h_0 = UW_e + W_p$$</li>
<li>層数分のTransformerのデコードを実行する$$h_l = transformer_block(h_{l-1}) \forall l \in [1,n]$$</li>
<li>単語ベクトル空間から系列の次元に戻した後にSoftmax関数で確率の値に変換$$P(u) = softmax(h_nW_e^T)$$</li>
</ol>
<h4 id="学習タスク別の入力系列整形と出力表現の変更">学習タスク別の入力系列整形と出力表現の変更</h4>
<p>下図に示すように、タスクによって、入力系列に予約済みのTokenを付加したり、Transformerの後の線形結合の取り方を変更する</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230715124104.png" alt="/img/Pasted_image_20230715124104.png">
（タスク別のBERT入出力：講義スライドより引用）</p>
<h3 id="推論時のprompt方法">推論時のPrompt方法</h3>
<ul>
<li>zero-shot:タスクを指定した後、すぐ推論させる</li>
<li>one-shot:タスクを指定した後、一つ例を与えて推論させる</li>
<li>few-shot:タスクを指定した後、複数例を与えて推論させる</li>
</ul>
<h3 id="bertとの比較">BERTとの比較</h3>
<table>
<thead>
<tr>
<th>モデル</th>
<th>使用するTransformer</th>
<th>方向</th>
<th>Fine Tuning</th>
</tr>
</thead>
<tbody>
<tr>
<td>BERT</td>
<td>エンコーダー</td>
<td>双方向</td>
<td>必要</td>
</tr>
<tr>
<td>GPT</td>
<td>デコーダー</td>
<td>単方向</td>
<td>GPT-3以降は不要</td>
</tr>
</tbody>
</table>
<h3 id="参考文献レポート-9">参考文献レポート</h3>
<p>GPT-4が現在最新バージョンだったため、講義内容まとめにタスクに対する成績やパラメータ数などGPT-4の内容を調べて記載した</p>
<p>※１：<a href="https://openai.com/gpt-4">GPT-4</a> &ldquo;<a href="https://openai.com/gpt-4%22">https://openai.com/gpt-4"</a>
※２：<a href="https://arxiv.org/abs/2303.08774">[2303.08774] GPT-4 Technical Report</a>&ldquo;<a href="https://arxiv.org/abs/2303.08774%22">https://arxiv.org/abs/2303.08774"</a>
※３：<a href="https://medium.com/@mlubbad/the-ultimate-guide-to-gpt-4-parameters-everything-you-need-to-know-about-nlps-game-changer-109b8767855a">The Ultimate Guide to GPT-4 Parameters: Everything You Need to Know about NLP’s Game-Changer | by Mohammed Lubbad | Medium</a>&ldquo;<a href="https://medium.com/@mlubbad/the-ultimate-guide-to-gpt-4-parameters-everything-you-need-to-know-about-nlps-game-changer-109b8767855a%22">https://medium.com/@mlubbad/the-ultimate-guide-to-gpt-4-parameters-everything-you-need-to-know-about-nlps-game-changer-109b8767855a"</a></p>
<!-- raw HTML omitted -->
<h2 id="音声認識">音声認識</h2>
<h3 id="音のデータとしての表現">音のデータとしての表現</h3>
<ul>
<li>振幅</li>
<li>周波数</li>
</ul>
<h3 id="音データの処理">音データの処理</h3>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230715174847.png" alt="/img/Pasted_image_20230715174847.png">
(波形データ処理の概念図：講義スライドより引用)</p>
<ul>
<li>標本化：アナログ信号をサンプリング周波数にしたがってサンプリングしてデジタル信号に変換する
<ul>
<li>サンプリングの法則：周波数hのアナログ信号をサンプリングするには最低でも2倍のサンプリング周波数として2hが必要</li>
</ul>
</li>
<li>量子化：連続値を近い離散値に変換する</li>
<li>フーリエ変換：時系列データを周波数領域のデータに変換する</li>
</ul>
<h4 id="フーリエ変換">フーリエ変換</h4>
<p>フーリエ変換</p>
<p>関数の級数展開とオイラーの公式と三角関数の直交性から、各周波数成分の大きさを抽出する操作であることがわかる
$$\hat{f}(\omega) = \int_{-\infty}^{\infty} f(t) e^{-i\omega t} , dt$$</p>
<ul>
<li>周波数領域の関数の微分は-iω倍になる</li>
<li>畳み込んだ関数のフーリエ変換はフーリエ変換した関数の積に等しい、関数の積のフーリエ変換はフーリエ変換された関数の畳み込みに等しい</li>
</ul>
<p>フーリエ逆変換</p>
<p>$$f(t) = \dfrac{1}{2\pi} \int_{-\infty}^{\infty} \hat{f}(\omega) e^{i\omega t} , d\omega$$</p>
<h5 id="dftとfft">DFTとFFT</h5>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230715184739.png" alt="/img/Pasted_image_20230715184739.png">
（フーリエ変換の応用例：講義スライドより引用）</p>
<h4 id="スペクトル">スペクトル</h4>
<p>周期的関数→離散スペクトル
非周期的関数→連続スペクトル</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230715181634.png" alt="/img/Pasted_image_20230715181634.png">
（周期的関数と非周期的関数の周波数領域変換結果の比較：講義スライドより引用）</p>
<h4 id="スペクトログラム">スペクトログラム</h4>
<p>現実的な非周期音声データを<strong>窓関数</strong>（特定の時間区間で波形を区切る関数）</p>
<p>窓を少しずつずらしたときのスペクトルをまとめたものを<strong>スペクトログラム</strong>という</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230715182313.png" alt="/img/Pasted_image_20230715182313.png">
（スペクトログラムの例：講義スライドより引用）</p>
<h5 id="窓関数">窓関数</h5>
<p>元の関数f(t)と窓関数w(t)をつかって新しく周期的な関数g(t)を作成する</p>
<p>$$g(t) =  w_i(t)\times f(t)$$</p>
<p>このとき一番カンタンな窓関数は矩形窓で着目する領域が1でそれ以外が0のステップ関数だが下に示す様に
窓1つを取ったときに周期の整数倍で終わっていないと、不連続な点が生まれて、スペクトルに不要な周波数が現れる
（参考：<a href="https://daigakudenki.com/windowing/">窓関数は周波数特性を調べて使い分ける！［サンプルコードあり］ - 大学の知識で学ぶ電気電子工学</a>&ldquo;<a href="https://daigakudenki.com/windowing/%22">https://daigakudenki.com/windowing/"</a>　）
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230715182434.png" alt="/img/Pasted_image_20230715182434.png">
（窓関数に関する問題提起：講義スライドより引用）</p>
<p>窓関数を使用する対象の非周期関数は周期が明確ではないので、
窓関数の方で周期的な関数になるように調整する必要がある
そのためによく使われるのがハミング窓
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230715184025.png" alt="/img/Pasted_image_20230715184025.png">
（窓関数の例：講義スライドより引用）</p>
<p>関数の積のフーリエ変換→フーリエ変換した関数の畳み込みにより、
元の関数のスペクトルより均されたスペクトルが得られることになる</p>
<p>そこで窓関数が満たすべき特徴として以下の2つが上げられる</p>
<ul>
<li>周波数分解能が高い</li>
<li>ダイナミックレンジが低い</li>
</ul>
<h3 id="音声認識に関する特徴量">音声認識に関する特徴量</h3>
<ul>
<li>メル尺度：人間の認知特性に応じた分解能に周波数の表現を変える、低周波の感度を上げ（分解能を上げる）て高周波の感度を下げる</li>
<li>ケプストラム：フーリエ変換した後に対数を取ってフーリエ逆変換を行う</li>
</ul>
<h3 id="参考文献レポート-10">参考文献レポート</h3>
<p>ケプストラムというのが何なのか分からなかったため、調査した。
人間の発生特性を考慮して、声帯振動と声道調音の成分を分離して解析できる特徴量とのこと。
参考：水田真理子, 羽地敏明, 阿部千鶴子, 「ケプストラム音響分析の有用性」, 日本言語聴覚医学会雑誌, 第62巻, 第3号, pp. 186-194, 2021. &ldquo;<a href="https://www.jstage.jst.go.jp/article/jjlp/62/3/62_186/_pdf">https://www.jstage.jst.go.jp/article/jjlp/62/3/62_186/_pdf</a>”</p>
<p>窓関数の項目で、不連続になるのがなぜ問題なのかの調査を行った。
”””（再掲）
窓1つを取ったときに周期の整数倍で終わっていないと、不連続な点が生まれて、スペクトルに不要な周波数が現れる
（参考：<a href="https://daigakudenki.com/windowing/">窓関数は周波数特性を調べて使い分ける！［サンプルコードあり］ - 大学の知識で学ぶ電気電子工学</a>&ldquo;<a href="https://daigakudenki.com/windowing/%22">https://daigakudenki.com/windowing/"</a>　）
”””</p>
<!-- raw HTML omitted -->
<h2 id="ctcconnectionist-temporal-classification">CTC（Connectionist Temporal Classification）</h2>
<h3 id="従来の音声認識モデルの概略">従来の音声認識モデルの概略</h3>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230715190458.png" alt="/img/Pasted_image_20230715190458.png">
（従来の音声認識モデルの概略：講義資料より引用）</p>
<ul>
<li>音響モデル
音声特徴量と音素列の間の確率を計算するモデル
音素とは、/a/や/i/といった母音、/k/や/s/といった子音から構成される音の最小単位
隠れマルコフモデル＋混合正規分布→隠れマルコフモデル＋DNNとなることで飛躍的にせいどが向上した</li>
<li>発音辞書
　音素列と単語との対応を扱うモデル</li>
<li>言語モデル
　自然言語処理と同様にこれまでの系列入力からある単語が得られる確率を求める</li>
</ul>
<h3 id="ctcの概略">CTCの概略</h3>
<p>End-to-Endでの学習 を行うRNNを用いた音声認識モデル
ステップバイステップで学習するモデルに比べて構成がシンプルで実装がし易いメリットがある</p>
<p>重要な発明</p>
<ul>
<li>ブランクラベルの導入</li>
<li>forward-backward algorythmを用いたDNNの学習</li>
</ul>
<h3 id="ブランクラベルの導入">ブランクラベルの導入</h3>
<h4 id="目的-1">目的</h4>
<ul>
<li>連続する同一ラベルの入力を表現する</li>
<li>ポーズのときに不自然なラベルを与えることを避ける</li>
</ul>
<h4 id="フレーム単位の入力系列から最終的なテキスト系列までの変換">フレーム単位の入力系列から最終的なテキスト系列までの変換</h4>
<ol>
<li>RNNで各音素の確率から最大のか確率を持つものを取得する</li>
<li>縮約　（以降 関数Bとする）
<ol>
<li>連続して出現している同一ラベルを 1 つのラベルにまとめる</li>
<li>ブランク「−」を削除する
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230715191729.png" alt="/img/Pasted_image_20230715191729.png">
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230715191756.png" alt="/img/Pasted_image_20230715191756.png">
（入力からテキスト系列出力まで：講義資料より引用）</li>
</ol>
</li>
</ol>
<p>B(a, −, −, b, b, −, c, c)　=　 a, b, c</p>
<h3 id="forward-backward-algorythmを用いたrnnの学習">forward-backward algorythmを用いたRNNの学習</h3>
<h4 id="目的関数-1">目的関数</h4>
<p>正解テキスト系列$l^\star$、入力系列$x$としたときの$P(l^\star|x)$が最大となるパラメータを求める</p>
<p>$$
\begin{aligned}
P(\boldsymbol{l^\star} \mid \boldsymbol{x}) &amp; =P([\mathrm{a},-, \mathrm{b}, \mathrm{b}, \mathrm{b}, \mathrm{c},-,-] \mid \boldsymbol{x})+P([-,-, \mathrm{a},-, \mathrm{b}, \mathrm{b},-, \mathrm{c}] \mid \boldsymbol{x})+\cdots \
&amp; =\sum_{\boldsymbol{\pi} \in \mathcal{B}^{-1}(\boldsymbol{l^\star})} P(\boldsymbol{\pi} \mid \boldsymbol{x})
\end{aligned}
$$</p>
<p>$P(l^\star|x)$は上に示す様に、縮約して$l^\star$になる入力系列の確率をすべて足し合わせたものになる。
各パターンの確率は各ラベルの得られる確率$y_k^t$(ｋはラベル、ｔはフレーム)の総乗になるので</p>
<p>$$
P\left(\boldsymbol{l}^* \mid \boldsymbol{x}\right)=\sum_{\boldsymbol{\pi} \in \mathcal{B}^{-1}\left(\boldsymbol{l}^<em>\right)} P(\boldsymbol{\pi} \mid \boldsymbol{x})=\sum_{\pi \in \mathcal{B}^{-1}\left(\boldsymbol{l}^</em>\right)} \prod_{t=1}^T y_{\pi_t}^t
$$</p>
<p>損失関数としては
$$
L_{CTC} = - log P(l^\star|x)
$$</p>
<h4 id="効率的な計算のためのforward-backward-algorythm">効率的な計算のためのforward-backward algorythm</h4>
<p>問題意識：$P(l^\star|x)$を算出する際に愚直に行うのは効率が悪い</p>
<ol>
<li>
<p>排反な事象への分類：特定のフレームにおいて特定のラベルを通って正解テキスト$l^\star$に縮約したときになる確率を考える
($l^\prime$は$l^\star$の各ラベルの間にブランクを挿入したラベル系列)
$$\left.P\left(\boldsymbol{l}^* \mid \boldsymbol{x}\right)=\sum_{s=1}^{\left|\boldsymbol{l}^{\prime}\right|} \underbrace{\sum_{\substack{\boldsymbol{\pi} \in \mathcal{B}^{-1}\left(\boldsymbol{l}^*\right), \pi_t=l_s^{\prime}}} P(\boldsymbol{\pi} \mid \boldsymbol{x})}_{\text{※１：特定フレームに着目した排反事象の確率の和}} \quad \right.\text{ (for any t )}$$</p>
</li>
<li>
<p>前向き確率、後ろ向き確率を使って※１の値を表現する
(以下の数式中の[]はガウス記号で、その値を超えない最大の整数を示す。ベクトルの下付き文字a:bは成分a~bまでの要素を持つベクトルを示す)</p>
<ul>
<li>前向き確率 $\alpha_t(s)$
始点からフレーム $t$ 、拡張ラベル $s$ の頂点に到達するまでの全パスの確率の総和
$$
\alpha_t(s) \equiv \sum_{\mathcal{B}\left(\pi_{1: t}\right)=l_{1:[s / 2]}^*} \prod_{t^{\prime}=1}^t y_{\pi_{t^{\prime}}}^{t^{\prime}}
$$</li>
<li>後ろ向き確率 $\beta_t(s)$
フレーム $t$ 、拡張ラベル $s$ の頂点から終点まで到達する全パスの確率の総和
$$
\beta_t(s) \equiv \sum_{\mathcal{B}\left(\pi_{t: T}\right)=l_{[s / 2]: l^* \mid}} \prod_{t^{\prime}=t}^T y_{t_{t^{\prime}}}^{t^{\prime}}
$$
$$
\begin{aligned}
\alpha_t(s) \beta_t(s) &amp;=\left(\sum_{\mathcal{B}\left(\pi_{1: t}\right)=l_{1:[s / 2]}^<em>} \prod_{t^{\prime}=1}^t y_{\pi_{t^{\prime}}}^{t^{\prime}}\right)\left(\sum_{\mathcal{B}\left(\pi_{t: T}\right)=l_{[s / 2]: ! l \mid}^</em>} \prod_{t^{\prime}=t}^T y_{\pi_{t^{\prime}}}^{t^{\prime}}\right) \
&amp;=\sum_{\mathcal{B}\left(\rho_{1: t}\right)=l_{1:[s / 2]}^<em>} \sum_{\mathcal{B}\left(\boldsymbol{\sigma}<em>{t: T}\right)=l</em>{[s / 2]:|l|}^</em>} \prod_{\tau=1}^t y_{\rho_\tau}^\tau \prod_{\nu=t}^T y_{\sigma_\nu}^\nu \
&amp; =y_{l_s^{\prime}}^t \sum_{\pi \in \mathcal{B}^{-1}\left(l^<em>\right), \Pi_t=l_s^\prime} \prod_{\pi_{t^{\prime}}}^{t^{\prime}}y_{\pi_{t^\prime}}^{t^\prime} \
&amp; =y_{l_s^{\prime}}^t \sum_{\substack{\pi \in \mathcal{B}^{-1}\left(l^</em>\right), \pi_t=l_s^{\prime}}} P(\boldsymbol{\pi} \mid \boldsymbol{x}) \
&amp;
\end{aligned}
$$
以上より、前向き確率と後ろ向き確率を用いて、フレームｔにおいてラベルｓの頂点を通るパスの確率の総和は以下の様に示せる
$$
\sum_{\substack{\pi \in \mathcal{B}^{-1}\left(l^*\right), \pi_t=l_s^{\prime}}} P(\boldsymbol{\pi} \mid \boldsymbol{x}) = \frac{\alpha_t(s) \beta_t(s)}{y_{l_s^\star}^{t}}
$$</li>
</ul>
</li>
<li>
<p>前向き確率、後ろ向き確率を再帰計算によって求める
初項についてはブランクか、正解ラベルの１文字目の２通りしか無いので以下の様になる。
$$
\begin{equation}\
a_0(s) =
\begin{cases}
y_{_}^0 &amp; \text{s=1:ブランク} \
y_{l_{1}^\star}^0 &amp; \text{s=2:正解ラベルの１文字目} \
0       &amp; \text{else}
\end{cases}
\end{equation}</p>
<p>$$
それ以降の遷移について考えると下図の様に３通りで
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230716131712.png" alt="/img/Pasted_image_20230716131712.png">
（状態遷移：講義スライドより引用）
$$
\alpha_{t+1}(s)= \begin{cases}{\left[\alpha_t(s)+\alpha_t(s-1)\right] y_{l_s^<em>}^{t+1}} &amp; \text { (if } l_s^</em>=\text { blank or } l_{s-2}^<em>=l_s^</em> \text { ) } \ {\left[\alpha_t(s)+\alpha_t(s-1)+\alpha_t(s-2)\right] y_{l_s^*}^{t+1}} &amp; \text { (otherwise) }\end{cases}
$$</p>
</li>
</ol>
<p>後ろ向き確率についても同様</p>
<h4 id="損失関数の誤差逆伝播">損失関数の誤差逆伝播</h4>
<p>出力層の各ノード$y_k^t$に対する損失関数の勾配の計算は、前向き確率及び、後ろ向き確率を用いて以下の様になる。</p>
<p>$$
\begin{aligned}
\frac{\partial \mathcal{L}<em>{\mathrm{CTC}}}{\partial y_k^t} &amp; =-\frac{1}{P\left(\boldsymbol{l}^* \mid \boldsymbol{x}\right)} \frac{\partial P\left(\boldsymbol{l}^* \mid \boldsymbol{x}\right)}{\partial y_k^t} \
&amp; =-\frac{1}{P\left(\boldsymbol{l}^* \mid \boldsymbol{x}\right)} \frac{\partial}{\partial y_k^t}\left(\sum</em>{s=1}^{\left|\boldsymbol{l}^<em>\right|} \frac{\alpha_t(s) \beta_t(s)}{y_{l_s^</em>}^t}\right) \
&amp; =-\frac{1}{P\left(\boldsymbol{l}^* \mid \boldsymbol{x}\right)}\left(-\sum_{s \in \operatorname{lab}\left(\boldsymbol{l}^<em>, k\right)} \frac{\alpha_t(s) \beta_t(s)}{\left(y_{l_s^</em>}^t\right)^2}\right) \
&amp; =\frac{1}{P\left(\boldsymbol{l}^* \mid \boldsymbol{x}\right)} \frac{1}{\left(y_k^t\right)^2} \sum_{s \in \operatorname{lab}\left(l^*, k\right)} \alpha_t(s) \beta_t(s)
\end{aligned}
$$</p>
<h3 id="推論の実行">推論の実行</h3>
<p>best path decoding：各フレームで最も確率の高いラベルのみを通るパスに着目して、そのパスを縮約して得られるテキスト系列を認識結果として出力する</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230716195113.png" alt="/img/Pasted_image_20230716195113.png">
（推論時の動作と問題点：講義スライドより引用）</p>
<p>上の例では左の赤い矢印で示した(s, t) = (1, 1) → (1, 2) というパスが best path
このときB(-,-) ＝ラベルなし、確率は 0.8 × 0.6 = 0.48</p>
<p>次に右の青い矢印で示した$B(\pi) = [a]$となる経路を考えると、
確率の総和は0.2 × 0.4 + 0.2 × 0.6 + 0.8 × 0.4 = 0.52となり、best pathより大きい</p>
<p>この様に必ずしもbest pathが一番高い確率になるとは限らない問題がある
→確率が高い順にいくつかのパスを保持してデコードするbeam search decodingが実践上はよく使われる</p>
<!-- raw HTML omitted -->
<h2 id="dcgan">DCGAN</h2>
<h3 id="gan">GAN</h3>
<h4 id="概要-3">概要</h4>
<p>Generator: 乱数からデータを生成
Discriminator:入力データが真のデータか、乱数から生成されたデータかを識別
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230716201724.png" alt="/img/Pasted_image_20230716201724.png">
（GANの概念図：講義資料より引用）</p>
<h4 id="2player-min-max-game">2player min-max game</h4>
<p>一人が自分の勝利する確率を最大化、もうひとりが相手の勝利する確率を最小化する作戦をとる
Discriminatorが最大化、Generatorが最小化をする
$$
\begin{gathered}
\min <em>G \max <em>D V(D, G) \
V(D, G)=\mathbb{E}</em>{\boldsymbol{x} \sim p</em>{\text {data }}(\boldsymbol{x})}[\log D(\boldsymbol{x})]+\mathbb{E}<em>{\mathbf{z} \sim p</em>{\boldsymbol{z}}(\mathbf{z})}[\log (1-D(G(\mathbf{z})))]
\end{gathered}
$$</p>
<p>最適化の手順
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230716210600.png" alt="/img/Pasted_image_20230716210600.png">
（GeneratorとDiscriminatorのパラメータ更新方法：講義スライドより引用）
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230716224134.png" alt="/img/Pasted_image_20230716224134.png">
（GANのパラメータ最適化と確率分布の変化：講義スライドより引用）</p>
<h4 id="generatorが本物のようなデータを生成できるのはなぜか">Generatorが本物のようなデータを生成できるのはなぜか？</h4>
<p>$p_g = p_{data}$のとき、価値関数V(D,G)が最適化されていることを示せば良い</p>
<ol>
<li>Gを固定し、価値関数が最大値を取るときのD(x)を算出</li>
<li>上記のD(x)を価値関数に代入して、Gが価値関数を最小化する条件を算出</li>
</ol>
<h5 id="価値関数の最大化">価値関数の最大化</h5>
<p>$$
\begin{aligned}
&amp; V(D, G)= \mathbb{E}<em>{\boldsymbol{x} \sim p</em>{\text {data }}(\boldsymbol{x})}[\log D(\boldsymbol{x})]+\mathbb{E}<em>{\mathbf{z} \sim p_z(\mathbf{z})}[\log (1-D(G(\mathbf{z})))] \
&amp;= \int</em>{\boldsymbol{x}} p_{\text {data }}(\boldsymbol{x}) \log (D(\boldsymbol{x})) d x+\int_{\boldsymbol{z}} p_z(\mathbf{z}) \log (1-D(G(\mathbf{z}))) d z \
&amp;= \int_{\boldsymbol{x}} p_{\text {data }}(\boldsymbol{x}) \log (D(\boldsymbol{x}))+p_g(\boldsymbol{x}) \log (1-D(\boldsymbol{x})) d x\
\&amp;\text{ここで、}{\mathrm{y}=D(\boldsymbol{x}), a=p_{\text {data }}(\boldsymbol{x}), b=p_g(\boldsymbol{x}) \text { と置けば }} \
\
&amp; a \log (y)+b \log (1-y)
\end{aligned}
$$</p>
<p>変分問題の公式を考えると、</p>
<p>$$
\mathcal{L}[f] = \int L(x, f(x)) dx
$$
以上のような目的関数があるとき、
この目的関数の停留関数（最大化または最小化を実現する関数）f(x)は以下の方程式を満たす。
$$
\frac{dL(x, f(x))}{df(x)} = 0
$$</p>
<p>以上のことから、y=f(x)とみると
$V(D,G) = \int_{\boldsymbol{x}} a \log (y)+b \log (1-y)dx$
なので、$a \log (y)+b \log (1-y)$をyで微分して
$$
\begin{align}
&amp;\frac{a}{y}+\frac{b}{1-y}(-1) = 0\
&amp;\frac{a}{y}=\frac{b}{1-y}\
&amp;a-ay = by\
&amp;y = \frac{a}{a+b}
\end{align}
$$
より,</p>
<p>$$
D^\star(x) = \frac{p_{data}(x)}{p_{data}(x)+p_g(x)}
$$</p>
<h5 id="価値関数の最小化">価値関数の最小化</h5>
<p>ここまで求めてきた$D^\star(x)$を用いて、価値関数は以下のようになる
$$
\begin{aligned}
V &amp; =\mathbb{E}<em>{\boldsymbol{x} \sim p</em>{\text {data }}} \log \left[\frac{p_{\text {data }}(\boldsymbol{x})}{p_{\text {data }}(\boldsymbol{x})+p_g(\boldsymbol{x})}\right]+\mathbb{E}<em>{\boldsymbol{x} \sim p_g} \log \left[1-\frac{p</em>{\text {data }}(\boldsymbol{x})}{p_{\text {data }}(\boldsymbol{x})+p_g(\boldsymbol{x})}\right] \
&amp; =\mathbb{E}<em>{\boldsymbol{x} \sim p</em>{\text {data }}} \log \left[\frac{p_{\text {data }}(\boldsymbol{x})}{p_{\text {data }}(\boldsymbol{x})+p_g(\boldsymbol{x})}\right]+\mathbb{E}<em>{\boldsymbol{x} \sim p_g} \log \left[\frac{p_g}{p</em>{\text {data }}(\boldsymbol{x})+p_g(\boldsymbol{x})}\right]
\end{aligned}
$$</p>
<p>ここでJSダイバージェンスを思い出すと
$$
JS(p||q) = \frac{1}{2} (\mathbb{E}<em>{x\sim p}log(\frac{2p}{p+q})+\mathbb{E}</em>{x\sim q}log(\frac{2q}{p+q}))
$$
なので、
$$
V = 2 \times JS(p_{data}||p_g) - 2log2
$$</p>
<p>JSダイバージェンスは2つの確率分布が一致するときに最小値０を取る非負の値なので
$$
\underset{p_g}{argmin}\quad V = p_{data}
$$
$p_g = p_{data}$のとき、最小値が得られる。</p>
<h3 id="dcganとは">DCGANとは</h3>
<p>Deep Convolutional GANの略で、いくつかの構造制約を持つ</p>
<ul>
<li>Generator ：転置畳み込みによって乱数を画像にアップサンプリングする
<ul>
<li>Pooling層の代わりに転置畳み込み層を使⽤</li>
<li>最終層はtanh、その他はReLU関数で活性化</li>
</ul>
</li>
<li>Discriminator：畳み込み層により画像から特徴量を抽出し、最終層をsigmoid関数で活性化
<ul>
<li>Pooling層の代わりに畳み込み層を使⽤</li>
<li>Leaky ReLU関数で活性化</li>
</ul>
</li>
<li>共通事項
<ul>
<li>中間層に全結合層を使わない</li>
<li>バッチノーマライゼーションを適⽤</li>
</ul>
</li>
</ul>
<h3 id="参考文献レポート-11">参考文献レポート</h3>
<p>黒本P３４８に該当箇所あり。
２プレイヤーの最適化に関する項目、Discriminatorが目的関数を最大化し、Generatorが最小化する。
どんな工夫がなされたかという項目は、バッチ正規化が生成、識別器ともに使われているというところが問われた</p>
<!-- raw HTML omitted -->
<h2 id="conditinal-gan">Conditinal GAN</h2>
<h3 id="特徴-2">特徴</h3>
<ul>
<li>GANの入力について、Generator、Discriminatorともにラベル入力が加わる</li>
<li>基本的なネットワーク構成はGANと同様</li>
<li>Discriminatorのラベル識別のパターンが異なる</li>
</ul>
<table>
<thead>
<tr>
<th>画像入力</th>
<th>ラベル入力</th>
<th>Discriminatorの判定</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gが生成した犬の画像G(z|y)</td>
<td>yラベル</td>
<td>☓</td>
</tr>
<tr>
<td>Gが生成した犬の画像G(z|y)</td>
<td>y以外のラベル</td>
<td>☓</td>
</tr>
<tr>
<td>真のラベルyの画像x</td>
<td>yラベル</td>
<td>◯</td>
</tr>
<tr>
<td>真のラベルyの画像x</td>
<td>y以外のラベル</td>
<td>☓</td>
</tr>
</tbody>
</table>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230716230926.png" alt="/img/Pasted_image_20230716230926.png">
（GANとConditinal GANの入力の比較：講義スライドをもとに作成）</p>
<h3 id="参考文献レポート-12">参考文献レポート</h3>
<p>黒本P３４８に該当箇所あり。
生成器、識別器への入力に関する内容が問われた。</p>
<!-- raw HTML omitted -->
<h2 id="pix2pix">Pix2Pix</h2>
<p>簡単にいうと、画像をラベルに変換にした後にConditional GANを実行するようなモノ
条件画像を入力→何らかの変換処理を行った画像を出力
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230716233047.png" alt="/img/Pasted_image_20230716233047.png">
（Pix2Pixの動作イメージ：講義資料より引用）</p>
<h3 id="工夫">工夫</h3>
<ul>
<li>UNetを利用
→物体の位置情報を伝達できる
→入出力のサイズが一致
→ピクセル単位での変換が可能</li>
<li>L1正則化（Generatorに追加）
→視覚的一致性を高める
→画像の高周波成分を学習してぼやけることが防げる
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230716233526.png" alt="/img/Pasted_image_20230716233526.png">（Pix2PixでのL1正則化の効果：講義資料より引用）</li>
<li>PatchGAN：条件画像をパッチに分けて各パッチにPix2Pixを適用
→正確な高周波成分の強調による視覚的一致性の向上
→L1正則化の効果を向上</li>
</ul>
<h3 id="参考文献レポート-13">参考文献レポート</h3>
<p>黒本P３４８,351に該当箇所あり。
Pix2Pixの目的関数に入力と生成画像のL1ノルムを追加することで、画像の大域的な特徴を捉えられる様になったとのこと。</p>
<p>Pix2Pixには画像ペアが必要、Cycle GANとの区別が問われた。</p>
<p>L1,L2損失では捉えられない高周波成分（細かい構造）を捉えるために局所的なパッチ構造に注意をむける
PatchGANについての内容。</p>
<p>L1損失のほうがL2損失よりぼかす効果が小さい。</p>
<p>Pix2Pixの生成器でUNetを使う効果はエッジ位置の共有。</p>
<!-- raw HTML omitted -->
<h2 id="a3c-asynchronous-advantage-actor-critic">A3C (Asynchronous Advantage Actor-Critic)</h2>
<h3 id="概要-4">概要</h3>
<p>強化学習の学習手法の一つで
<strong>複数</strong>のエージェントが<strong>同一の環境</strong>で<strong>非同期的に学習</strong>する</p>
<ul>
<li>Asynchoronous:非同期な並列学習
→学習の高速化：非同期処理で時間効率が上がる
→学習を安定化：同じ状態から始まって、同じ方策に基づいて行動することで生じていた経験の自己相関という強化学習の課題を、複数エージェントを並列で学習させることで低減した</li>
<li>Advantage:複数ステップ先を考慮して更新する</li>
<li>Actor:方策によって行動を選択</li>
<li>Critic:状態価値関数に応じて方策を修正</li>
</ul>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230717142543.png" alt="/img/Pasted_image_20230717142543.png">
（Actor-Criticの動作イメージ：講義資料より引用）</p>
<p>DQNではバッファに蓄積した経験をランダムに取り出すことで経験の自己相関を低減したが、
この方法はオフポリシー手法でしか使えない</p>
<table>
<thead>
<tr>
<th>オンポリシー</th>
<th>オフポリシー</th>
</tr>
</thead>
<tbody>
<tr>
<td>学習時と実行時に同じ方策を用いる</td>
<td>学習時と実行時に異なる方策を用いる</td>
</tr>
<tr>
<td>方策評価と方策改善を交互に行う</td>
<td>方策評価と方策改善を同時に行う</td>
</tr>
<tr>
<td>例：SARSA, Actor-Critic</td>
<td>例：Q-Learning, DQN</td>
</tr>
</tbody>
</table>
<h3 id="非同期学習">非同期学習</h3>
<ul>
<li>複数のエージェントが並列で独立にゲーム実行して勾配計算を行う</li>
<li>勾配情報でGlobal Networkの重みを更新する</li>
<li>定期的にLocal Networkの重みをGlobal Networkの重みと同期する</li>
</ul>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230717143331.png" alt="/img/Pasted_image_20230717143331.png">
（非同期学習のイメージ：講義スライドより引用）</p>
<h3 id="損失関数-2">損失関数</h3>
<h4 id="特徴-3">特徴</h4>
<ul>
<li>通常のActor-Criticと異なり、一つの分岐型ネットワークが方策と価値の両方を出力、損失関数も共通</li>
<li>数ステップ先まで見たアドバンテージ方策勾配を利用</li>
<li></li>
</ul>
<p>Total Loss = Advantage方策勾配　＋　α×価値関数ロス　- β×方策エントロピー</p>
<h5 id="advantage方策勾配">Advantage方策勾配</h5>
<p>方策勾配法：パラメータΘに基づく方策πに従ったときの期待収益が最大化されるようにΘを最適化する
方策勾配定理により、パラメータ更新に用いられる勾配は以下のように示される
※b(s)はベースライン、行動価値関数から引くことで推定量の分散を小さくして学習の安定化効果がある
$$
\nabla_\theta \rho_\theta=\mathbb{E}\left[\nabla_\theta \log \pi(a \mid s, \theta)\left(Q^{\pi_\theta}(s, a)-b(s)\right)\right]
$$
ここで、b(s)の推定には$V_\theta^{\pi_\omega}(s_t)$,価値関数Q(s,a)をアドバンテージ関数としてkステップ先読みした収益をもちいて
$$
\begin{align}
&amp;Q(s,a) = (\sum_{i=0}^{k-1}\gamma^i R_{t+i+1})+V_\theta^{\pi_\omega}(s_{t+k})\
&amp;b(s) = V_\theta^{\pi_\omega}(s_t)
\end{align}
$$
$$
\nabla_\theta \log \pi(a \mid s ; \theta)\left((\sum_{i=0}^{k-1} \gamma^i R_{t+i+1})+V_\theta^{\pi_\omega}\left(s_{t+k}\right)-V_\theta^{\pi_\omega}\left(s_t\right)\right)
$$</p>
<p>上記の期待値が方策勾配となる。</p>
<h5 id="方策エントロピー">方策エントロピー</h5>
<p>ランダム性の高い方策にボーナスを与えることで、方策の収束が早すぎて局所解に収束することを防ぐ
方策関数の正則化効果をもつ
$$
-\sum_a \pi\left(a_t \mid s_t\right) \log \pi\left(a_t \mid s_t\right)
$$</p>
<h3 id="性能-1">性能</h3>
<p>より短い訓練時間でGPUを使用しなくても高いスコアを実現</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230717163309.png" alt="/img/Pasted_image_20230717163309.png">
（A3Cの性能比較：講義資料より引用）</p>
<h3 id="実装上の制約">実装上の制約</h3>
<ul>
<li>pythonが非同期処理に向いていない</li>
<li>並列でネットワークを複数持つ関係上、大規模リソースを持つ環境が必要</li>
</ul>
<p>→　A2Cという同期処理を行う手法が発表</p>
<h4 id="a2c">A2C</h4>
<ol>
<li>各エージェントが司令部から行動の指示を受けて1ステップ行動する</li>
<li>各エージェントから状態遷移の結果を司令部が受け取って次の指示を行う</li>
</ol>
<!-- raw HTML omitted -->
<h2 id="metric-learning距離学習">Metric-Learning（距離学習）</h2>
<p>データ間の距離を学習して、近いデータをクラスタリングしたり、遠いデータを異常値として検知したりする。
深層学習を利用した距離学習を特に深層距離学習と呼ぶ。</p>
<h3 id="深層距離学習">深層距離学習</h3>
<p>ネットワークの構造自体を変えなくても、類似度を反映した埋め込み空間を構成できるように学習するだけで精度が向上できる</p>
<h4 id="学習の仕方">学習の仕方</h4>
<ul>
<li>同じクラスに属するサンプルから得られる特徴量ベクトルの距離は近く</li>
<li>異なるクラスに属するサンプルから得られる特徴量ベクトルの距離は遠く</li>
</ul>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230718111625.png" alt="/img/Pasted_image_20230718111625.png">
(学習の動作イメージ：講義資料より引用)</p>
<h5 id="siamese-network">Siamese network</h5>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230718111913.png" alt="/img/Pasted_image_20230718111913.png">
（Siamese Networkの概念図：講義スライドより引用）</p>
<p>Contrastive Loss（損失関数）は以下の様に表現される
$$
L=\frac{1}{2}\left[y D^2+(1-y) \max (m-D, 0)^2\right]
$$
各変数は
D:埋め込み空間でのデータの距離、ユークリッド距離
y:識別ラベル、２入力が同じクラスのとき１、違うクラスのとき０
m:マージン、どの程度の距離まで罰則項を追加するか</p>
<h6 id="課題">課題</h6>
<p>同一クラスと異なるクラスの学習の最適化の不均衡
（異なるクラスの学習は距離がマージンｍ離れるタイミングで最適化が終了するが、同一クラスは１点に収束するまで終わらない）</p>
<h5 id="triplet-network">Triplet network</h5>
<p>Siamese networkの最適化の不均衡を解決するための手法
Siamese networkでは類似クラスと非類似クラスの判定がタスクごとのコンテキストに依存していたが、Triplet networkではアンカークラスを基準に類似度を判定するために、コンテキストを考慮する必要がなくなる
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230718112955.png" alt="/img/Pasted_image_20230718112955.png">
（Triplet Networkの概念図：講義スライドより引用）</p>
<h6 id="siamese-networkからの主な変更点">Siamese networkからの主な変更点</h6>
<p>入力データの準備方法が異なる。</p>
<ol>
<li>アンカーサンプルを選択する</li>
<li>アンカーサンプルの類似サンプル(positive sample)と非類似サンプル(negative sample)を一つずつ選択</li>
<li>３つの入力データのセットをそれぞれ同一のCNNに入力する</li>
<li>Triplet Loss（損失関数）を計算
$$
L = max(D_p-D_n+m,0)
$$</li>
</ol>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230718113444.png" alt="/img/Pasted_image_20230718113444.png">
(埋め込み空間における、データサンプルと距離の関係：講義資料より引用)</p>
<h6 id="課題-1">課題</h6>
<ul>
<li>学習が停滞しやすい
<ul>
<li>学習データセットのサイズが増えると組み合わせが膨大になる</li>
<li>ほとんどの組み合わせが学習が進むに連れてパラメータ更新に寄与しなくなる
→学習に有効な入力セットの厳選（Triplet Selection)が必要になる</li>
</ul>
</li>
<li>クラス内距離がクラス間距離より近くなることを保証しない
→Quadrupt Lossという４つのデータ点で構成する損失関数が提案、任意のクラス間距離＞任意のクラス内距離になる
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230718114917.png" alt="/img/Pasted_image_20230718114917.png">
（Triplet Network学習後の埋め込み空間でのクラス別のサンプルの位置の例：講義資料より作成）</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="mamlメタ学習">MAML（メタ学習）</h2>
<p>大規模データセット作成の問題点</p>
<ul>
<li>アノテーションコスト</li>
<li>データ収集のハードル
→深層学習に用いるデータセットの量を削減したい</li>
</ul>
<h3 id="既存の対策">既存の対策</h3>
<ul>
<li>転移学習</li>
<li>Fine Tuning
→いずれも元のモデルが今回行いたいタスクに適合している重みを持っているとは限らない点が問題</li>
</ul>
<h3 id="mamlのコンセプト">MAMLのコンセプト</h3>
<ol>
<li>全タスク集合で共通の重みを学習</li>
<li>各タスクでファインチューニングして各モデルの重みを更新</li>
<li>各モデルの重みを集めて共通の重みも更新</li>
</ol>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230717172413.png" alt="/img/Pasted_image_20230717172413.png">
（MAMLの動作フロー：講義資料より引用）</p>
<h3 id="mamlの課題">MAMLの課題</h3>
<p>タスクごと、共通パラメータの2回の勾配計算が必要
→計算量が多いのが課題 　（現実的にInner Loopのステップ数を大きくできない）</p>
<h3 id="mamlの課題への対策">MAMLの課題への対策</h3>
<ul>
<li>First-order MAML: 2次以上の勾配を無視し計算コストを大幅低減</li>
<li>Reptile: Inner loopの逆伝搬を行わず、学習前後のパラメータの差を利用</li>
</ul>
<!-- raw HTML omitted -->
<h2 id="gcnグラフ畳み込み">GCN（グラフ畳み込み）</h2>
<p>CNNは幾何学的なデータにしか使えない</p>
<p>GCNは畳み込み操作をグラフ構造に拡張したもの</p>
<p>活用例：コロナウイルスの感染者数の予測</p>
<h3 id="グラフ">グラフ</h3>
<p>ノードとエッジで構成されるデータ構造
→ネットワーク上のデータのスパース表現、データからネットワーク構造の推定、他分野での複雑データ解析</p>
<h4 id="グラフの例1">グラフの例(※1)</h4>
<ul>
<li>交通網</li>
<li>ハイパーリンク</li>
<li>電気回路</li>
<li>対人関係</li>
<li>神経網</li>
<li>電力網</li>
<li>タンパク質・ゲノム構造</li>
<li>３Dメッシュ</li>
</ul>
<h3 id="畳み込みとは">畳み込みとは</h3>
<p>ここでは、<strong>重要な部分の抽出と不要な部分の切り捨てを行う操作</strong>とする</p>
<h4 id="空間的畳み込み">空間的畳み込み</h4>
<p>$$
\begin{align}
連続関数　(f<em>g)(t) &amp;= \int f(\tau)g(t,\tau)d\tau \
離散関数　(f</em>g)(m)&amp; = \sum_n f(n)g(m,n)
\end{align}
$$</p>
<h4 id="スペクトル畳み込み">スペクトル畳み込み</h4>
<p>フーリエ変換の性質より、
下の式の様に畳み込みのフーリエ変換はフーリエ変換の積に等しいので、
$$
\mathbb{F}(f*g) = \mathbb{F}(f)\mathbb{F}(g)
$$
フーリエ変換した後、特徴的な周波数だけ残すようなフィルタ関数をかけて、再度フーリエ逆変換を行う</p>
<h3 id="spatial-gcn">Spatial GCN</h3>
<p>ノードｊの近くにあるノードを以下のように、ノード間の重みが一定以上であるノードとして定義
$$N_\delta(j) = {i \in \Omega: W_{ij}&gt;\delta} $$
これの重心を次の層のノードして扱い、層に分けてクラスタリングを繰り返す</p>
<p>隣接関係（ｋ層における、分割したクラスタ内の要素同士の関係、iは分割したクラスタのi番目の要素）
$$N_k = {N_{k,i};i=1,\dots,d_{k-1}}$$
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230717193344.png" alt="/img/Pasted_image_20230717193344.png">
（グラフデータのクラスタリング：講義資料より引用）</p>
<p>k層目の出力（ｋ＋１層目の入力）は以下の様に書ける
$$
x_{k+1, j}=L_k h\left(\sum_{i=1}^{f_{k-1}} F_{k, i, j} x_{k, i}\right) \quad\left(j=1 \ldots f_k\right)
$$</p>
<ul>
<li>$L_k$はプーリングの機能</li>
<li>hは活性化関数</li>
<li>$F_{k,i}$は隣接関係$N_k$のあるところだけ値をもつスパースな行列</li>
<li>$f_k$はｋ層目のフィルタ数</li>
</ul>
<h3 id="spectal-gcn">Spectal GCN</h3>
<p>フーリエ変換→フィルタ関数を掛ける→逆フーリエ変換</p>
<h4 id="グラフをフーリエ変換するには">グラフをフーリエ変換するには</h4>
<p>フーリエ変換の特徴の、
２回微分したときにもとの関数の定数倍になることに着目する</p>
<p>このとき、２回微分は自分自身と周辺の差を取る操作で行列として表現できる
→グラフラプラシアンという値を導入</p>
<p>行列をベクトルにかけた結果がベクトルの定数倍
→固有値・固有ベクトルの関係
→グラフラプラシアンを固有値分解</p>
<p>ここで離散フーリエ変換について、考えると
元の関数に大きさ１の独立した成分の和をとる操作であるため
$$F(t) = \sum f(x) exp(-i\frac{2\pi x}{N})$$
固有ベクトルと入力ベクトルの内積を取って和をとることに等しい
このことから固有ベクトルからなる行列を$\mathbb{Q}$とすると
GFT:Graph Fourier Transformは
$$\mathbb{F} = F_G(\vec{f}) = \mathbb{Q}^T\vec{f}$$</p>
<p>グラフラプラシアンが実対称行列であることから、$\mathbb{Q}$は直交行列であり$\mathbb{Q}^{-1} = \mathbb{Q}^T$なので
IGFT: Inverse Graph Fourier Transformは
$$F_G^{-1}(\mathbb{F}) = \mathbb{Q}\mathbb{F}$$</p>
<h5 id="グラフラプラシアン">グラフラプラシアン</h5>
<p>$$
L = D -A
$$
L:グラフラプラシアン (実対象行列→固有値ベクトルが直行)
D:次数行列　（ノードから何本エッジが伸びているか、対角成分のみを持つ）
A:隣接行列　（どのノードに繋がっているか、01の値を持つ対称行列）</p>
<h3 id="gcnの弱点">GCNの弱点</h3>
<ul>
<li>Spatial GCN：次元が低く、近傍となる点が限られるため広い範囲で重みを出しにくい</li>
<li>Spectral GCN：計算量が多く、フィルタが固有基底に依存するためパラメータの使いまわしができない
→ChebNetで対応されることで、空間領域にも拡張された</li>
</ul>
<h3 id="参考">参考</h3>
<p>※１：グラフ信号処理〜基礎から応用まで〜&ldquo;<a href="https://speakerdeck.com/ychtanaka/gurahuxin-hao-chu-li-ji-chu-karaying-yong-made?slide=14%22">https://speakerdeck.com/ychtanaka/gurahuxin-hao-chu-li-ji-chu-karaying-yong-made?slide=14"</a>
<a href="https://math-fun.net/20201214/7235/">直交ベクトルの線形独立性、正規直交基底、直交行列について解説 | 趣味の大学数学</a>&ldquo;<a href="https://math-fun.net/20201214/7235%22">https://math-fun.net/20201214/7235"</a>
<a href="https://math-fun.net/20201217/7330/">行列の対角化可能性の定義とメリット、例、同値条件について解説 | 趣味の大学数学</a>&ldquo;<a href="https://math-fun.net/20201217/7330/%22">https://math-fun.net/20201217/7330/"</a>
<a href="https://math-fun.net/20210606/14817/">対称行列の性質：内積による特徴づけ、逆行列、固有値、対角化について | 趣味の大学数学</a>&ldquo;<a href="https://math-fun.net/20210606/14817/%22">https://math-fun.net/20210606/14817/"</a></p>
<!-- raw HTML omitted -->
<h2 id="grad-camlimeshap">Grad-CAM,LIME,SHAP</h2>
<p>社会実装を考える際にはモデルの解釈性は重要
→ディープラーニングモデルのブラックボックス性の解消が必要</p>
<h3 id="モデル解釈の手法">モデル解釈の手法</h3>
<ul>
<li>CAM</li>
<li>Grad-CAM</li>
<li>LIME</li>
<li>SHAP</li>
</ul>
<h3 id="camclass-activation-mapping">CAM　Class Activation Mapping</h3>
<p>”Learning Deep Features for Discriminative Localization”という論文において
Global Average PoolingがCNNが潜在的に着目している部分を可視化できるようにする役割を持っていることが発見された</p>
<h4 id="利用上の制約">利用上の制約</h4>
<ul>
<li>ネットワークの大部分がCNNで構成されていること</li>
<li>出力層の直前でGAPを実行していること</li>
</ul>
<h4 id="動作イメージ">動作イメージ</h4>
<p>出力層の重みを特徴マップ上に投影することで画像領域の重要性を識別
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230718010059.png" alt="/img/Pasted_image_20230718010059.png">
（CAMのコンセプト図：講義資料より引用）</p>
<p>$$
\begin{equation}
M_c(x, y)=\sum_k w_k^c f_k(x, y)
\end{equation}
$$
Mc:クラスCの位置(x,y)の重要度
$w_k^c$:最後の畳み込み層のk番目のフィルタからクラスCにつながる重み
$f_k$:最後の畳み込み層のチャンネルのうちｋ番目の特徴マップ</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230718010656.png" alt="/img/Pasted_image_20230718010656.png">
（CAMの出力結果の例：講義スライドより引用）</p>
<h4 id="camの評価">CAMの評価</h4>
<p>CAMの注目領域にしたがって生成したBounding BoxのLocalizationの指標IoUでの評価が可能</p>
<p>単純にクラス出力から誤差逆伝播で位置を設定した場合よりもGAPを使って、CAMによるBounding Box設定を行ったほうが誤差が小さく出る。
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230718010919.png" alt="/img/Pasted_image_20230718010919.png">
（講義スライドより引用）</p>
<h3 id="grad-cam">Grad-CAM</h3>
<p>Grad-CAM: Why did you say that?
Visual Explanations from Deep Networks via Gradient-based Localization
&ldquo;<a href="https://arxiv.org/pdf/1610.02391v1.pdf%22">https://arxiv.org/pdf/1610.02391v1.pdf"</a></p>
<ul>
<li>CNNに判断根拠をもたせ、モデルの予測根拠を可視化する手法</li>
<li>最後の畳み込み層の予測クラスの出力値に対する勾配を利用
（勾配が大きい場所は予測クラスの出力に大きく影響する重要な場所という考え方）
※CAMにおいて、特徴マップにGAP－出力層間の重みを使用していたところを勾配で代用している</li>
<li>GAPがなくても使える、出力がクラスでなくても良いため、<strong>様々なタスクで使える</strong></li>
</ul>
<p>$$
\alpha_k^c=\overbrace{\frac{1}{Z} \sum_i \sum_j}^{\text {global average pooling }} \underbrace{\frac{\partial y^c}{\partial A_{i j}^k}}_{\text {gradients via backprop }}
$$</p>
<p>$a_k^c$:クラスCのk番目のフィルタに関する重み係数
$y_c$:クラスCのスコア
$A_{ij}^k$:k番目の特徴マップの座標i,jにおける値</p>
<p>ヒートマップの値は以下の様に示される。
$$
L_{c} = ReLU \left( \sum_{k} \alpha_{k}^{c} A^k \right)
$$</p>
<h3 id="limelocal-interpretable-model-agnostic-explanations">LIME　Local Interpretable Model-agnostic Explanations</h3>
<p>&ldquo;Why Should I Trust You?&rdquo;: Explaining the Predictions of Any Classifier
<a href="https://arxiv.org/abs/1602.04938">https://arxiv.org/abs/1602.04938</a></p>
<p>特定の入力に対する予測に対して、予測根拠を解釈・可視化するツール
Ex.)</p>
<ul>
<li>表データ：どの変数が予測に効いたのか</li>
<li>画像データ：どの部分が予測に効いたのか</li>
</ul>
<h4 id="手法-2">手法</h4>
<p>解釈可能なシンプルなモデルで複雑なモデルを近似することで解釈を行う</p>
<p>シンプルなモデルの例</p>
<ul>
<li>決定木</li>
<li>線形モデル</li>
</ul>
<h5 id="近似上の工夫">近似上の工夫</h5>
<ul>
<li>全体ではなく１つの個別の予測結果をLIMEに入力</li>
<li>対象サンプル周辺に限ったデータセットを教師データとして、対象範囲内のみで有効なモデルを作成</li>
</ul>
<p>＜複雑な決定境界を持つモデルに対して、太い赤十字で示したデータを説明しようとしたときのLIMEの働きのイメージ＞
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230718094504.png" alt="/img/Pasted_image_20230718094504.png">
（講義資料より引用）</p>
<h5 id="目的関数-2">目的関数</h5>
<p>入力データに対して、元のモデルと近似するモデルの距離に基づく不正確さ（第一項）及び近似モデルの複雑さを示す正則化項（第二項）の和が最小になるような近似モデルgをGから選択する
$$
\xi(x) = \underset{g\in G}{argmin}(L(f,g,\pi_x) + \Omega (g))
$$</p>
<p>近似モデルの入力には、元の入力に摂動を加えたものを用いる
摂動の例</p>
<ul>
<li>テキスト：単語のランダム除去</li>
<li>画像：スーパーピクセルなどの領域分割に基づくマスク処理</li>
<li>表：ランダムに一部のデータを置き換え</li>
</ul>
<p>$\mathcal{Z}$は説明したいデータ周辺のデータセット、
元の入力ｚに対して摂動を加えた入力をｚ’とすると、
$$
\mathcal{L}\left(f, g, \pi_x\right)=\sum_{z, z^{\prime} \in \mathcal{Z}} \pi_x(z)\left(f(z)-g\left(z^{\prime}\right)\right)^2
$$
このとき、$\pi_x(z) = exp(-D(x,z)/\sigma^2)$でσはハイパーパラメータで設定基準が無い問題がある。</p>
<h4 id="解釈例">解釈例</h4>
<p>講義ではテキストをキリスト教、無神論の２クラスに分類するモデルについて
LIMEを使って分類時の単語の重要度を可視化した</p>
<p>可視化した結果、どちらのクラスにも関係のない単語が重要であるとされたため
データセットを確認したところ、データセットの２割に出現（高頻度）しており、
９９%が無神論のクラスに属していた（偏在）</p>
<p>→　データセットのデータ分布の問題があり、作成されたモデルは行いたいタスクに対する汎用性がない</p>
<h4 id="実装時の参考">実装時の参考</h4>
<p>&ldquo;<a href="https://github.com/marcotcr/lime%22">https://github.com/marcotcr/lime"</a></p>
<p>※Pythonを用いた実装ライブラリはデータ形式によってアルゴリズムが異なる</p>
<h3 id="shap">SHAP</h3>
<p>A Unified Approach to Interpreting Model Predictions&quot;https://arxiv.org/pdf/1705.07874.pdf&rdquo;</p>
<p>協力ゲームプレイ理論の概念Shapley Valueを機械学習に応用</p>
<p>Shapley Valueを算出する上での想定：プレイヤーが協力して、それによって獲得した報酬を分配する</p>
<h4 id="shapley-value">Shapley Value</h4>
<p>限界貢献度：プレイヤーが一人加入することでどれだけ報酬が増えるか
（０人から一人増えるとき、１人から２人になるときなど、元の状態と変化後の状態次第で限界貢献度は異なり、プレイヤーだけの関数にはならない）</p>
<p>参加者の順列分パターンを変更して、各人の限界貢献度を算出して平均をとる。
この平均限界貢献度をShapley Valueという</p>
<p>下図の例では
Aは(4 + 3 + 15 + 15 + 4 + 11) / 6 = 8.7
Bは(6 + 7 + 7 + 15 + 19 + 19) / 6 = 12.2
Cは(25 + 25 + 13 + 5 + 12 + 5) / 6 = 14.2
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230718102102.png" alt="/img/Pasted_image_20230718102102.png">
（講義スライドより作成）</p>
<h4 id="機械学習への応用">機械学習への応用</h4>
<table>
<thead>
<tr>
<th>分野</th>
<th>説明</th>
</tr>
</thead>
<tbody>
<tr>
<td>協力ゲーム理論</td>
<td>協力して得た報酬を、貢献度が異なるプレイヤーにどう分配するか</td>
</tr>
<tr>
<td>機械学習</td>
<td>モデルから出力された予測値を、貢献度が異なる特徴量にどう分配するか</td>
</tr>
</tbody>
</table>
<p>下図に示す様に、協力ゲームでの参加・不参加を特徴量を使用・不使用に置き換えて、予測値の期待値の差を限界貢献度とみなす。</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230718102911.png" alt="/img/Pasted_image_20230718102911.png">
（条件付きの予測値の期待値をつかった機械学習での使用イメージ：講義スライドより引用）</p>
<h5 id="講義での例">講義での例</h5>
<p>ボストンの不動産価格
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230718103415.png" alt="/img/Pasted_image_20230718103415.png">
(SHAPによる各特徴量ごとの寄与度の可視化結果：講義スライドより引用)
※値は特徴量でShapley Valueではない</p>
<ol>
<li>寄与度の大きい特徴量を把握する</li>
<li>その特徴量のデータの分布を確認</li>
<li>特徴量の値の評価の仕方についての仮説をたてる</li>
<li>その特徴量だけを変化させたときのモデル出力の変化をみる</li>
</ol>
<!-- raw HTML omitted -->
<h2 id="docker">Docker</h2>
<h3 id="利用価値">利用価値</h3>
<p>コンテナは仮想マシンに比べて軽量</p>
<ul>
<li>必要最小限のファイル</li>
<li>必要最小限のライブラリ</li>
</ul>
<h3 id="コンテナ型の仮想化">コンテナ型の仮想化</h3>
<h4 id="仮想化">仮想化</h4>
<p>効率化</p>
<ul>
<li>ハードウェアの数を減らす</li>
<li>柔軟なサービス運用</li>
<li>サービス不可の分散</li>
</ul>
<p>仮想化の種類</p>
<ul>
<li>VM
<ul>
<li>完全VM：OSを仮想化、物理的なOS環境を置き換えられる</li>
<li>プロセスVM：JAVA VMなどCPUプロセスなどを仮想化するVM
プロセス：アプリケーションプログラムの実行単位、実行中のインスタンス</li>
</ul>
</li>
<li>コンテナ：アプリケーションの仮想化、完全なゲストOSを必要としない
※コンテナ型の仮想化 はVMではない、</li>
</ul>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230718000304.png" alt="/img/Pasted_image_20230718000304.png"></p>
<h3 id="dockerの仮想化">Dockerの仮想化</h3>
<h4 id="特徴-4">特徴</h4>
<p>Dockerのコンテナは、それぞれ最小限のライブラリと実行ファイルを持ち、
カーネルやハードウェアリソースは共有するので非常に軽量
→起動時間が顕著に高速
→開発サイクルが早くリアルタイム性が求められるWebアプリケーション開発に向く</p>
<h4 id="注意点">注意点</h4>
<ul>
<li>カーネルバージョンのホストOSとコンテナOSの互換性問題
<ul>
<li>コンテナOSがLinux　→　通常問題にならない</li>
<li>コンテナOSがLinuxではない　→　カーネルバージョンの互換性の配慮が必要</li>
</ul>
</li>
<li>セキュリティ対策
ホストOS上で実行されるユーザプロセスとしてカーネル・ネットワーク・ファイルシステムを共有するため、
論理的隔離度合いが低くなる</li>
</ul>
<h3 id="dockerの具体的なオペレーション">Dockerの具体的なオペレーション</h3>
<p>Dockerオブジェクト：Dockerコマンドで操作できる対象</p>
<ul>
<li>コンテナ
<ul>
<li>run/createでイメージからコンテナを生成する</li>
<li>commitでコンテナの実行内容でimageを更新する</li>
</ul>
</li>
<li>イメージ
<ul>
<li>tagで管理用の名前をつける</li>
</ul>
</li>
<li>ネットワーク：ボリュームやネットワークを通してリソースの隔離と共有を行う
同一ネットワーク上のコンテナのみが直接通信でき、特にネットワークの名前を指定しない場合はデフォルトのbridgeネットワークにコンテナが追加される</li>
</ul>
<p>Dockerレジストリによって、
公式イメージを利用したり、自作のイメージの管理ができる</p>
<p>コンテナ自体の保存はできず、
ファイルシステムのみをexportでリソースファイルに抽出して、
イメージに取り込むことができる
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230718001450.png" alt="/img/Pasted_image_20230718001450.png">
（講義資料より引用）</p>
<h3 id="dockerfile">Dockerfile</h3>
<p>Dockerfileにイメージ作成時の一連の操作
（公式イメージをベースとして取ってくる、必要ライブラリのインストール、ファイルのコピー、作成、ポート指定など）を
Dockerインストラクションを用いて記載する
ビルドは差分キャッシュを行うので、効率的にビルドできる
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230718003435.png" alt="/img/Pasted_image_20230718003435.png">
（講義資料より引用）</p>
<h4 id="通信管理">通信管理</h4>
<ul>
<li>ホストと通信する：ポートを指定して、コンテナ作成時にバインドする</li>
<li>ホストの属する物理ネットワークに接続する
<ul>
<li>hostドライバで直接接続</li>
<li>macvlanドライバで、ホスト-VLANインターフェイス上にVLANを構成する必要がある</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th>ホスト内</th>
<th>ホスト外</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230718003006.png" alt="/img/Pasted_image_20230718003006.png"></td>
<td><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230718003031.png" alt="/img/Pasted_image_20230718003031.png"></td>
</tr>
</tbody>
</table>
<h3 id="docker-compose">docker-compose</h3>
<p>サービスの構成や依存関係をyaml形式ファイルにまとめることで複雑なdockerコマンドのオペレーションを整理できる
→swarm機能によって提供されるデプロイメントを大幅に簡略化できる</p>
<!-- raw HTML omitted -->
<h1 id="実装演習結果">実装演習結果</h1>
<h2 id="4_1_transfer-learningipynb">4_1_transfer-learning.ipynb</h2>
<ul>
<li>一般的な処理関連
<ul>
<li>クラス数の取得：tensorflow_datasets.データセット.info.features[&rsquo;label&rsquo;].num_classesで取得</li>
<li>前処理の適用: datset.map(function)メソッド</li>
<li>ミニバッチでまとめる：datset.batch(BATCH_SIZE)メソッド</li>
<li>実行時のデータロードの改善：dataset.prefetch（）メソッドで、ステップｓの訓練とステップｓ＋１のデータのロードをオーバーラップして実行時間の改善をする</li>
</ul>
</li>
<li>転移学習関連
<ul>
<li>定義済みネットワークの利用:tensorflow.keras.applications.モデル.モデルのバージョン()
<ul>
<li>学習済みの重みを使うときは引数weights=&ldquo;使いたいモデルの名前&quot;にする</li>
<li>学習可能にする（FineTuning）にはインスタンスのtrainable属性をTrueにする,事前学習部分を固定したい場合はFalse
※model.layersの各要素ごとにtrainable属性があるので、層ごとに学習するかは選べる</li>
<li>ネットワークモデルの再構成
model.summary()（必要に応じてtensorflow.keras.utils.plot_model(model)）をつかって構造を確認して使用する部分のインデックスを確認して、そこのアウトプットから自分の定義したいモデルのアウトプットを定義
keras.models.Model(inputs,outputs)を利用してモデルを定義
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x <span style="color:#f92672">=</span> DefinedModel<span style="color:#f92672">.</span>layers[index]<span style="color:#f92672">.</span>output
</span></span><span style="display:flex;"><span>output <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>使いたいクラス(適切な引数)(x)
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> keras<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>Model(inputs<span style="color:#f92672">=</span>DefinedModel<span style="color:#f92672">.</span>input,outputs<span style="color:#f92672">=</span>output)
</span></span></code></pre></div><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713135321.png" alt="/img/Pasted_image_20230713135321.png"></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="実行結果と考察">実行結果と考察</h3>
<p>転移学習なしでは、早期に局所最適解に陥ったのか、損失関数の改善が止まって学習が打ち切られてしまった。
転移学習でもとのモデルの重みを固定した場合は、緩やかに改善されたが、正解率は低い
これはImageNetのデータセットが多様な物体を対象としたものに対して、今回利用したFlowerデータセットは花に限って細かく分類するものであるため、事前学習モデルで抽出されている特徴量が合わなかった可能性が考えられる
事前学習モデルの重みの更新も含めたFine Tuningでは正解率は1近くまで改善しており、学習がうまく行っていることがわかる
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713141434.png" alt="/img/Pasted_image_20230713141434.png"></p>
<h2 id="4_2_wide_resnetipynb">4_2_wide_resnet.ipynb</h2>
<p>4_1と異なり、tf.keras.Sequentialを利用してモデルを再定義、
この場合は、学習済みモデルをtensorflow_hub.KerasLayer(url,trainable)で指定。</p>
<p>FineTuningで重みを再学習する場合も、Batch正規化の平均分散のパラメータは更新されない。</p>
<h3 id="実行結果と考察-1">実行結果と考察</h3>
<h4 id="パラメータ数と実行時間の比較">パラメータ数と実行時間の比較</h4>
<p>ResNetとWideResNetの学習コストをみたいので、重み固定ではなくFineTuningの結果を載せる
以下に示すように学習可能なパラメータ数はWideResNetで約9倍　（= 211201805/23518277）
実行時間は9.6倍程度（290/30）で言及されていたGPUの特性に合わせた演算での高速化は見られなかった。
※深層学習のフレームワーク内部で最適化されているのかもしれない</p>
<h4 id="resnet-1">ResNet</h4>
<table>
<thead>
<tr>
<th>モデルサマリ</th>
<th>実行結果</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713153123.png" alt="/img/Pasted_image_20230713153123.png"></td>
<td><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713153946.png" alt="/img/Pasted_image_20230713153946.png"></td>
</tr>
</tbody>
</table>
<h4 id="wide-resnet-1">Wide ResNet</h4>
<table>
<thead>
<tr>
<th>モデルサマリ</th>
<th>実行結果</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713153020.png" alt="/img/Pasted_image_20230713153020.png"></td>
<td><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713161445.png" alt="/img/Pasted_image_20230713161445.png"></td>
</tr>
</tbody>
</table>
<h3 id="訓練結果の比較">訓練結果の比較</h3>
<p>ResNetのFine Tuningの方ではValidationの方の正解率が落ちており、過学習を起こしてしまっていることがわかる
Wide ResNetに比べてDropOutがない分、正則化が足りなかったことが原因かと思われる。</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713153349.png" alt="/img/Pasted_image_20230713153349.png">
ResNetにも正則化効果のあるBatch Normalizationは入っているので、念のためデータセットのクラス分布が偏っていてDataAugumentationが必要なケースだった可能性も考えたが、そこまで極端な分布になっておらず、テスト、訓練で比較した際も別物の分布とまでは言えないため、バッチサイズが16と小さめに設定されていることが原因では無いかと考えられる。
（Google ColaboratoryのGPU割当制限に引っかかったのか、バッチサイズを上げて検証することはできませんでした。）</p>
<table>
<thead>
<tr>
<th>訓練データセットのクラス分布</th>
<th>テストデータセットのクラス分布</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713161030.png" alt="/img/Pasted_image_20230713161030.png"></td>
<td><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713161039.png" alt="/img/Pasted_image_20230713161039.png"></td>
</tr>
</tbody>
</table>
<h2 id="4_3_lecture_chap1_exercise_publicipynb">4_3_lecture_chap1_exercise_public.ipynb</h2>
<p>Pytorchでの実装だったため、実装部分のまとめは省略
一部データ型についてのエラーがあったため、修正をしました。</p>
<p>Seq2Seqの学習テクニックとして、訓練時のDecoderの内部状態の入力にターゲット系列を用いる<strong>Teacher Forcing</strong>というテクニックがある。
推論時と訓練時でデータの分布が異なるという問題が発生することもあるので、
確率でターゲット系列と前時刻の生成された状態を切り替えて用いる<strong>Scheduled Sampling</strong>という手法がある。</p>
<p>Seq2Seqではデータセット中の入力系列の長さの差分をPaddingで吸収しているので、
損失関数はPadding部分は計算しないようにマスクを掛けた<strong>masked_cross_entropy</strong>をもちいる</p>
<p>Loss関数の他に、機械翻訳において一般的なモデル評価指標である<strong>BLEU</strong>を計算する。
※BLEUはn-gram(n個の単語のリスト)のプロが翻訳したリストと予測されたリストのマッチ率に基づく指標</p>
<p>データローダーを実装しないと、メモリに全展開して学習を進めることになり、DeepLearningの規模だとメモリに乗り切らない問題が起こる</p>
<h3 id="実行結果と考察-2">実行結果と考察</h3>
<h4 id="訓練の経過">訓練の経過</h4>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713181222.png" alt="/img/Pasted_image_20230713181222.png"></p>
<h4 id="推論結果">推論結果</h4>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713181619.png" alt="/img/Pasted_image_20230713181619.png">
このときBLEU = 17.92904579451379（０～１００）</p>
<h5 id="beam-search-を用いたときの結果">Beam Search を用いたときの結果</h5>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230713212813.png" alt="/img/Pasted_image_20230713212813.png"></p>
<p>あまりうまく行っていない用に思われる、慣用的な表現は難しかったりするのか、
命令文の構文理解もうまく行ってないことも考えると、コーパスが軽量という話もあったのでそちらの問題も考えられる。</p>
<h2 id="4_5_lecture_chap3_exercise_publicipynb">4_5_lecture_chap3_exercise_public.ipynb</h2>
<h3 id="tensorflowのバージョン違いの修正">tensorflowのバージョン違いの修正</h3>
<p>TPU実行周りの設定のバージョン違い修正にかかる時間が読めなかったため、途中で断念。</p>
<p>tensorflow.logging →　tensorflow.compat.v1.logging
tensorflow.train.Optimizer →　tensorflow.keras.optimizers.Optimizer
tensorflow.gfile →　tensorflow.compat.v1.io.gfile</p>
<h2 id="4_6_bertipynb">4_6_bert.ipynb</h2>
<h3 id="必要なライブラリをインストールする">必要なライブラリをインストールする</h3>
<p>日本語の形態素解析にもとづくTokenizeのためのパッケージ郡をインストール</p>
<pre tabindex="0"><code>!pip install mecab-python3
!pip install unidic
!python -m unidic download
!pip install fugashi
!pip install ipadic
</code></pre><p>Transformer関連のネットワークが使えるtransformersライブラリを使用</p>
<pre tabindex="0"><code class="language-code" data-lang="code">! pip install transformers
</code></pre><h3 id="データセットをダウンロード">データセットをダウンロード</h3>
<p>青空文庫から</p>
<pre tabindex="0"><code class="language-code" data-lang="code">!wget https://www.aozora.gr.jp/cards/000148/files/773_ruby_5968.zip
!unzip -O sjjs /content/773_ruby_5968.zip
!wget https://www.aozora.gr.jp/cards/000148/files/56143_ruby_50824.zip
!unzip -O sjjs  /content/56143_ruby_50824.zip
!wget https://www.aozora.gr.jp/cards/000148/files/799_ruby_6024.zip
!unzip -O sjjs 799_ruby_6024.zip
</code></pre><h3 id="文字化けを治す">文字化けを治す</h3>
<pre tabindex="0"><code class="language-code" data-lang="code"># nkfコマンドを実行環境にインストール
!apt install nkf
# コマンドを使って文字化けを上書き
!nkf -w --overwrite kokoro.txt sorekara.txt yume_juya.txt
# テキストファイルを一つにまとめる
!cat kokoro.txt sorekara.txt yume_juya.txt &gt; train.txt
</code></pre><h3 id="transformersから学習済みのtokenizerとモデルを取得する">transformersから学習済みのTokenizerとモデルを取得する</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> TFBertModel
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> BertJapaneseTokenizer
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> BertJapaneseTokenizer<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#39;cl-tohoku/bert-base-japanese-whole-word-masking&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>bert <span style="color:#f92672">=</span> TFBertModel<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#39;cl-tohoku/bert-base-japanese-whole-word-masking&#39;</span>)
</span></span></code></pre></div><h3 id="テキストから単語ベクトルを作成">テキストから単語ベクトルを作成</h3>
<ol>
<li>Mecabで使う辞書を指定</li>
<li>形態素解析を行う</li>
<li>set関数を使ってユニークな要素を得ることでこのデータセットのボキャブラリを取得</li>
<li>文字列からインデックス、インデックスから文字に治す辞書及び、配列を作成</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#39;train.txt&#39;</span>, <span style="color:#e6db74">&#39;r&#39;</span>, encoding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;utf-8&#39;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>  text <span style="color:#f92672">=</span> f<span style="color:#f92672">.</span>read()<span style="color:#f92672">.</span>replace(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>, <span style="color:#e6db74">&#39;&#39;</span>)
</span></span><span style="display:flex;"><span>mecab <span style="color:#f92672">=</span> MeCab<span style="color:#f92672">.</span>Tagger(<span style="color:#e6db74">&#34;-Owakati&#34;</span>)
</span></span><span style="display:flex;"><span>text <span style="color:#f92672">=</span> mecab<span style="color:#f92672">.</span>parse(text)<span style="color:#f92672">.</span>split()
</span></span><span style="display:flex;"><span>vocab <span style="color:#f92672">=</span> sorted(set(text))
</span></span><span style="display:flex;"><span>char2idx <span style="color:#f92672">=</span> {u: i <span style="color:#66d9ef">for</span> i, u <span style="color:#f92672">in</span> enumerate(vocab)}
</span></span><span style="display:flex;"><span>idx2char <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array(vocab)
</span></span><span style="display:flex;"><span>text_as_int <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([char2idx[c] <span style="color:#66d9ef">for</span> c <span style="color:#f92672">in</span> text])
</span></span></code></pre></div><h3 id="訓練用のデータセットを作成">訓練用のデータセットを作成</h3>
<ol>
<li>系列長を128に設定</li>
<li>numpy配列からデータセットを作成</li>
<li>データセットを系列長で分割
※ターゲットが次の時刻の単語になるため、系列長＋1のデータを利用する</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>seq_length <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 訓練用サンプルとターゲットを作る</span>
</span></span><span style="display:flex;"><span>char_dataset <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Dataset<span style="color:#f92672">.</span>from_tensor_slices(text_as_int)
</span></span><span style="display:flex;"><span>sequences <span style="color:#f92672">=</span> char_dataset<span style="color:#f92672">.</span>batch(seq_length<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>, drop_remainder<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">split_input_target</span>(chunk):
</span></span><span style="display:flex;"><span>    input_text <span style="color:#f92672">=</span> chunk[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>    target_text <span style="color:#f92672">=</span> chunk[<span style="color:#ae81ff">1</span>:]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> input_text, target_text
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dataset <span style="color:#f92672">=</span> sequences<span style="color:#f92672">.</span>map(split_input_target)
</span></span></code></pre></div><h3 id="文生成モデル">文生成モデル</h3>
<p>今回のデータセットのボキャブラリーにあった長さの出力になるようにbertにボキャブラリー数分のノードを持つ全結合層を付加したモデルを作成して転移学習を行う
※Inputの定義の際には、入力系列の長さが可変になるように引数shapeに(None,)を与える
※この訓練においてはBERTの重みは固定で全結合層部分のみを学習させるため、言語モデルの損失関数ではなく、単純にクラス分類のためのスパースクロスエントロピー誤差を用いる (one-hot表現でない整数ラベルに対して用いるクロスエントロピー誤差)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>BATCH_SIZE <span style="color:#f92672">=</span> <span style="color:#ae81ff">64</span>
</span></span><span style="display:flex;"><span>BUFFER_SIZE <span style="color:#f92672">=</span> <span style="color:#ae81ff">10000</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dataset <span style="color:#f92672">=</span> dataset<span style="color:#f92672">.</span>shuffle(BUFFER_SIZE)<span style="color:#f92672">.</span>batch(BATCH_SIZE, drop_remainder<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>input_ids <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Input(shape<span style="color:#f92672">=</span>(<span style="color:#66d9ef">None</span>, ), dtype<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;int32&#39;</span>, name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;input_ids&#39;</span>)
</span></span><span style="display:flex;"><span>inputs <span style="color:#f92672">=</span> [input_ids]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>bert<span style="color:#f92672">.</span>trainable <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> bert(inputs)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>out <span style="color:#f92672">=</span> x[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Y <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>layers<span style="color:#f92672">.</span>Dense(len(vocab))(out)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>checkpoint_dir <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;./training_checkpoints&#39;</span>
</span></span><span style="display:flex;"><span>checkpoint_prefix <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(checkpoint_dir, <span style="color:#e6db74">&#34;ckpt_</span><span style="color:#e6db74">{epoch}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>checkpoint_callback<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>callbacks<span style="color:#f92672">.</span>ModelCheckpoint(
</span></span><span style="display:flex;"><span>    filepath<span style="color:#f92672">=</span>checkpoint_prefix,
</span></span><span style="display:flex;"><span>    save_weights_only<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>Model(inputs<span style="color:#f92672">=</span>inputs, outputs<span style="color:#f92672">=</span>Y)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loss</span>(labels, logits):
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">return</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>losses<span style="color:#f92672">.</span>sparse_categorical_crossentropy(labels, logits, from_logits<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>compile(loss<span style="color:#f92672">=</span>loss,
</span></span><span style="display:flex;"><span>              optimizer<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>optimizers<span style="color:#f92672">.</span>Adam(<span style="color:#ae81ff">1e-7</span>))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>fit(dataset,epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>, callbacks<span style="color:#f92672">=</span>[checkpoint_callback])
</span></span></code></pre></div><p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230716125622.png" alt="/img/Pasted_image_20230716125622.png">
Loss関数の値は低下傾向のままなので、まだ学習が進む余地はあるが、GPU利用制限にかかってしまうので、途中で終了
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230716205010.png" alt="/img/Pasted_image_20230716205010.png"></p>
<h4 id="考察-1">考察</h4>
<p>上記のモデルで推論を実行した結果、学習が足りないのか、まだ自然な文章とはなっていない。
また、あくまでBERT部分は固定されていて、ボキャブラリが今回のデータセットに合うように射影しているだけなので、
元のデータセットのボキャブラリ（※32000）と今回のボキャブラリ(12750)の数が半分以下で違ったこと、元のデータセットがWikiから取ったデータで、今回のデータが夏目漱石の作品に限ったために、年代による文体の違いなどから、あまりうまく行かなかったのでは無いかと思われる。
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230716211754.png" alt="/img/Pasted_image_20230716211754.png">
※参考：<a href="https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking">cl-tohoku/bert-base-japanese-whole-word-masking · Hugging Face</a></p>
<h3 id="文章のクラス分類">文章のクラス分類</h3>
<ol>
<li>テキストファイルを行単位で分割</li>
<li>ターゲットを分類のone-hotベクトルとして定義</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">input_target</span>(chunk):
</span></span><span style="display:flex;"><span>    input_text <span style="color:#f92672">=</span> chunk
</span></span><span style="display:flex;"><span>    target <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>constant([<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>], dtype<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>float32)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> input_text, target
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>kokoro <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>TextLineDataset(<span style="color:#e6db74">&#39;kokoro.txt&#39;</span>)
</span></span><span style="display:flex;"><span>kokoro <span style="color:#f92672">=</span> kokoro<span style="color:#f92672">.</span>map(input_target)
</span></span></code></pre></div><p>学習結果
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230716131855.png" alt="/img/Pasted_image_20230716131855.png">
こちらもまだ精度は上がるが、GPU利用制限のため、50で中断</p>
<p>推論結果
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230716131806.png" alt="/img/Pasted_image_20230716131806.png">
デフォルトのインプットの一文がどこにも見つからなかったので、
「それから」から固有名詞を含む1行を抜き出して再度実行
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230716224443.png" alt="/img/Pasted_image_20230716224443.png">
期待としてはi=1の要素が一番高くなってほしかったが、こころに対応する要素が最も高くなってしまった。
学習が不十分な可能性がある。</p>
<h2 id="4_8_interpretabilityipynb">4_8_interpretability.ipynb</h2>
<p>CAM,Grad-CAM 共通で最後の畳み込みそうの出力が必要</p>
<ol>
<li>model.summary()などで、畳み込み層の名前を確認</li>
<li>model.get_layer(確認した畳み込み層の名前).outputで畳み込み層の出力を得る</li>
<li>畳み込み層の出力に掛ける係数を算出する
<ol>
<li>Grad-CAM の場合
<ol>
<li>勾配情報が必要なので、モデル定義時に勾配情報を保存するようにtf.GradientTape()のwithブロックで順伝播処理をくくる</li>
<li>Grad-CAMの場合はtape.gradient(勾配の基準（クラス判定結果）、たどるところまで（畳み込み層の出力）)で勾配を得る</li>
<li>Global Average Pooling　(tf.reduce_mean )</li>
<li>ReLUを使う</li>
</ol>
</li>
<li>CAMの場合
<ol>
<li>GAP→クラスの重みをmodel.get_layer (対象のレイヤー名).weightsで取得</li>
</ol>
</li>
</ol>
</li>
</ol>
<p>実行結果
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230719025720.png" alt="/img/Pasted_image_20230719025720.png">
トイレットペーパーの穴の部分でトイレットペーパー判定している。
一応円柱部分も寄与していると思われる。</p>
<h2 id="dcganipynb">dcgan.ipynb</h2>
<p>リソースの問題で実行できないが、認定テスト等で出された重要箇所についてまとめる。</p>
<h3 id="discriminatorの更新">Discriminatorの更新</h3>
<p>real_lossとfake_lossがあるが、
バイナリクロスエントロピーを取るときに比較するラベルが、ことなることに注意</p>
<ul>
<li>real_loss :　値が全て１のベクトルと比較</li>
<li>fake_loss :　ゼロベクトルと比較</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">update_discriminator</span>(self, noize, real_data):
</span></span><span style="display:flex;"><span>        fake_data <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>G(noize)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> d_tape:
</span></span><span style="display:flex;"><span>            real_pred <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>D(real_data)
</span></span><span style="display:flex;"><span>            fake_pred <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>D(fake_data)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            real_loss <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>losses<span style="color:#f92672">.</span>binary_crossentropy(
</span></span><span style="display:flex;"><span>                tf<span style="color:#f92672">.</span>ones_like(real_pred), real_pred
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>            fake_loss <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>keras<span style="color:#f92672">.</span>losses<span style="color:#f92672">.</span>binary_crossentropy(
</span></span><span style="display:flex;"><span>                tf<span style="color:#f92672">.</span>zeros_like(fake_pred), fake_pred
</span></span><span style="display:flex;"><span>            )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># batchの平均をとる</span>
</span></span><span style="display:flex;"><span>            real_loss <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>reduce_mean(real_loss)
</span></span><span style="display:flex;"><span>            fake_loss <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>reduce_mean(fake_loss)
</span></span><span style="display:flex;"><span>            adv_loss <span style="color:#f92672">=</span> real_loss <span style="color:#f92672">+</span> fake_loss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        d_grad <span style="color:#f92672">=</span> d_tape<span style="color:#f92672">.</span>gradient(adv_loss, sources<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>D<span style="color:#f92672">.</span>trainable_variables)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>d_optimizer<span style="color:#f92672">.</span>apply_gradients(zip(d_grad, self<span style="color:#f92672">.</span>D<span style="color:#f92672">.</span>trainable_variables))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>make_logs:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">with</span> self<span style="color:#f92672">.</span>summary_writer<span style="color:#f92672">.</span>as_default():
</span></span><span style="display:flex;"><span>                tf<span style="color:#f92672">.</span>summary<span style="color:#f92672">.</span>scalar(<span style="color:#e6db74">&#34;d_loss&#34;</span>, adv_loss)
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>summary_writer<span style="color:#f92672">.</span>flush()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> adv_loss
</span></span></code></pre></div>
              


            </div>
          </div>
          <div id="post-footer" class="post-footer main-content-wrap">
            
              
                
                
                  <div class="post-footer-tags">
                    <span class="text-color-light text-small">タグ</span><br/>
                    
  <a class="tag tag--primary tag--small" href="https://half-broken-engineer.github.io/tags/obsidian_note/">obsidian_note</a>

                  </div>
                
              
            
            
<div class="post-actions-wrap">
  <nav >
    <ul class="post-actions post-action-nav">
      
        <li class="post-action">
          
            <a class="post-action-btn btn btn--default tooltip--top" href="https://half-broken-engineer.github.io/%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB/" data-tooltip="はじめに" aria-label="次: はじめに">
          
              <i class="fa fa-angle-left"></i>
              <span class="hide-xs hide-sm text-small icon-ml">次</span>
            </a>
        </li>
        <li class="post-action">
          
            <a class="post-action-btn btn btn--default tooltip--top" href="https://half-broken-engineer.github.io/deeplearningday3/" data-tooltip="DeepLearningDay3" aria-label="前: DeepLearningDay3">
          
              <span class="hide-xs hide-sm text-small icon-mr">前</span>
              <i class="fa fa-angle-right"></i>
            </a>
        </li>
      
    </ul>
  </nav>
<ul class="post-actions post-action-share" >
  
    <li class="post-action hide-lg hide-md hide-sm">
      <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="この記事を共有する">
        <i class="fa fa-share-alt" aria-hidden="true"></i>
      </a>
    </li>
    
      <li class="post-action hide-xs">
        <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=https://half-broken-engineer.github.io/deeplearningday4/" title="Twitterで共有" aria-label="Twitterで共有">
          <i class="fab fa-twitter" aria-hidden="true"></i>
        </a>
      </li>
    
  
  
    <li class="post-action">
      <a class="post-action-btn btn btn--default" href="#disqus_thread" aria-label="コメントを残す">
        <i class="far fa-comment"></i>
      </a>
    </li>
  
  <li class="post-action">
    
      <a class="post-action-btn btn btn--default" href="#top" aria-label="トップに戻る">
      <i class="fa fa-arrow-up" aria-hidden="true"></i>
    
    </a>
  </li>
</ul>
</div>


            
  
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
    <script type="text/javascript">
      var disqus_config = function() {
        this.page.url = 'https:\/\/half-broken-engineer.github.io\/deeplearningday4\/';
        
          this.page.identifier = '\/deeplearningday4\/'
        
      };
      (function() {
        
        
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
          document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
          return;
        }
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        var disqus_shortname = 'hugo-tranquilpeak-theme';
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
  


          </div>
        </article>
        <footer>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</footer>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
    inlineMath: [['$','$']]
    }
});
</script>
      </div>
      <div id="bottom-bar" class="post-bottom-bar" data-behavior="5">
        
<div class="post-actions-wrap">
  <nav >
    <ul class="post-actions post-action-nav">
      
        <li class="post-action">
          
            <a class="post-action-btn btn btn--default tooltip--top" href="https://half-broken-engineer.github.io/%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB/" data-tooltip="はじめに" aria-label="次: はじめに">
          
              <i class="fa fa-angle-left"></i>
              <span class="hide-xs hide-sm text-small icon-ml">次</span>
            </a>
        </li>
        <li class="post-action">
          
            <a class="post-action-btn btn btn--default tooltip--top" href="https://half-broken-engineer.github.io/deeplearningday3/" data-tooltip="DeepLearningDay3" aria-label="前: DeepLearningDay3">
          
              <span class="hide-xs hide-sm text-small icon-mr">前</span>
              <i class="fa fa-angle-right"></i>
            </a>
        </li>
      
    </ul>
  </nav>
<ul class="post-actions post-action-share" >
  
    <li class="post-action hide-lg hide-md hide-sm">
      <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="この記事を共有する">
        <i class="fa fa-share-alt" aria-hidden="true"></i>
      </a>
    </li>
    
      <li class="post-action hide-xs">
        <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=https://half-broken-engineer.github.io/deeplearningday4/" title="Twitterで共有" aria-label="Twitterで共有">
          <i class="fab fa-twitter" aria-hidden="true"></i>
        </a>
      </li>
    
  
  
    <li class="post-action">
      <a class="post-action-btn btn btn--default" href="#disqus_thread" aria-label="コメントを残す">
        <i class="far fa-comment"></i>
      </a>
    </li>
  
  <li class="post-action">
    
      <a class="post-action-btn btn btn--default" href="#top" aria-label="トップに戻る">
      <i class="fa fa-arrow-up" aria-hidden="true"></i>
    
    </a>
  </li>
</ul>
</div>


      </div>
      
<div id="share-options-bar" class="share-options-bar" data-behavior="5">
  <i id="btn-close-shareoptions" class="fa fa-times"></i>
  <ul class="share-options">
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://twitter.com/intent/tweet?text=https%3A%2F%2Fhalf-broken-engineer.github.io%2Fdeeplearningday4%2F" aria-label="Twitterで共有">
          <i class="fab fa-twitter" aria-hidden="true"></i><span>Twitterで共有</span>
        </a>
      </li>
    
  </ul>
</div>
<div id="share-options-mask" class="share-options-mask"></div>


    </div>
    
    <div id="about">
  <div id="about-card">
    <div id="about-btn-close">
      <i class="fa fa-times"></i>
    </div>
    
      <img id="about-card-picture" src="https://half-broken-engineer.github.io/img/profile.png" alt="プロフィール画像" />
    
    <h4 id="about-card-name">Half-Broken Engineer</h4>
    
      <div id="about-card-bio">🤖　　　　壊れかけのエンジニア　　　　💻不安を解消したいから💰のお勉強もする</div>
    
    
      <div id="about-card-job">
        <i class="fa fa-briefcase"></i>
        <br/>
        Engineer
      </div>
    
    
      <div id="about-card-location">
        <i class="fa fa-map-marker-alt"></i>
        <br/>
        Japan
      </div>
    
  </div>
</div>

    

    
  
    
      <div id="cover" style="background-image:url('https://half-broken-engineer.github.io/images/cover.jpg');"></div>
    
  


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" integrity="sha512-894YE6QWD5I59HgZOGReFYm4dnWc1Qt5NtvYSaNcOP+u1T9qYdvdihz0PPSiiqn/+/3e7Jo4EaG7TubfWGUrMQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/highlight.min.js" integrity="sha512-z+/WWfyD5tccCukM4VvONpEtLmbAm5LDu7eKiyMQJ9m7OfPEDL7gENyDRL3Yfe8XAuGsS2fS4xSMnl6d30kqGQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha512-uURl+ZXMBrF4AwGaWmEetzrd+J5/8NRkWAvJx5sbPSSuOb0bZLqf+tOzniObO00BjHa/dD7gub9oCGMLPQHtQA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>


<script src="https://half-broken-engineer.github.io/js/script-yqzy9wdlzix4lbbwdnzvwx3egsne77earqmn73v9uno8aupuph8wfguccut.min.js"></script>


  
    <script async crossorigin="anonymous" defer integrity="sha512-gE8KAQyFIzV1C9+GZ8TKJHZS2s+n7EjNtC+IMRn1l5+WYJTHOODUM6JSjZhFhqXmc7bG8Av6XXpckA4tYhflnw==" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/apache.min.js"></script>
  

  
    <script async crossorigin="anonymous" defer integrity="sha512-EWROca+bote+7Oaaar1F6y74iZj1r1F9rm/ly7o+/FwJopbBaWtsFDmaKoZDd3QiGU2pGacBirHJNivmGLYrow==" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/go.min.js"></script>
  

  
    <script async crossorigin="anonymous" defer integrity="sha512-GDVzAn0wpx1yVtQsRWmFc6PhJiLBPdUic+h4GWgljBh904O3JU10fk9EKNpVyIoPqkFn54rgL2QBG4BmUTMpiQ==" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/http.min.js"></script>
  

  
    <script async crossorigin="anonymous" defer integrity="sha512-UgZlma8NzkrDb/NWgmLIcTrH7i/CSnLLDRFqCSNF5NGPpjKmzyM25qcoXGOup8+cDakKyaiTDd7N4dyH4YT+IA==" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/less.min.js"></script>
  

  
    <script async crossorigin="anonymous" defer integrity="sha512-lot9koe73sfXIrUvIPM/UEhuMciN56RPyBdOyZgfO53P2lkWyyXN7J+njcxIIBRV+nVDQeiWtiXg+bLAJZDTfg==" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/nginx.min.js"></script>
  

  
    <script async crossorigin="anonymous" defer integrity="sha512-Zd3e7XxHP00TD0Imr0PIfeM0fl0v95kMWuhyAS3Wn1UTSXTkz0OhtRgBAr4JlmADRgiXr4x7lpeUdqaGN8xIog==" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/puppet.min.js"></script>
  

  
    <script async crossorigin="anonymous" defer integrity="sha512-qtqDO052iXMSP+5d/aE/jMtL9vIIGvONgTJziC2K/ZIB1yEGa55WVxGE9/08rSQ62EoDifS9SWVGZ7ihSLhzMA==" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/scss.min.js"></script>
  

  
    <script async crossorigin="anonymous" defer integrity="sha512-1NmkjnEDnwwwcu28KoQF8vs3oaPFokQHbmbtwGhFfeDsQZtVFI8zW2aE9O8yMYdpdyKV/5blE4pSWw4Z/Sv97w==" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/stylus.min.js"></script>
  

  
    <script async crossorigin="anonymous" defer integrity="sha512-B2wSfruPjr8EJL6IIzQr1eAuDwrsfIfccNf/LCEdxELCgC/S/ZMt/Uvk80aD79m7IqOqW+Sw8nbkvha20yZpzg==" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/swift.min.js"></script>
  

  
    <script async crossorigin="anonymous" defer integrity="sha512-28oDiQZGKUVN6wQ7PSLPNipOcmkCALXKwOi7bnkyFf8QiMZQxG9EQoy/iiNx6Zxj2cG2SbVa4dXKigQhu7GiFw==" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/yaml.min.js"></script>
  


<script>
$(document).ready(function() {
  hljs.configure({ classPrefix: '', useBR: false });
  $('pre.code-highlight > code, pre > code').each(function(i, block) {
    if (!$(this).hasClass('codeblock')) {
      $(this).addClass('codeblock');
    }
    hljs.highlightBlock(block);
  });
});
</script>




    
  </body>
</html>

