<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RabbitChallenge on 壊れかけのエンジニアのログ</title>
    <link>https://half-broken-engineer.github.io/categories/rabbitchallenge/</link>
    <description>Recent content in RabbitChallenge on 壊れかけのエンジニアのログ</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Tue, 27 Jun 2023 00:00:00 +0900</lastBuildDate><atom:link href="https://half-broken-engineer.github.io/categories/rabbitchallenge/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DeepLearningDay4</title>
      <link>https://half-broken-engineer.github.io/2023/06/deeplearningday4/</link>
      <pubDate>Tue, 27 Jun 2023 00:00:00 +0900</pubDate>
      
      <guid>https://half-broken-engineer.github.io/2023/06/deeplearningday4/</guid>
      <description>実施内容と得点表 章タイトル 要点 実装演習 確認テストまたは考察 参考図書など関連記事レポート 強化学習 ◯ ◯ AlphaGo ◯ ◯ 軽量化・高速化技術 ◯ ◯ 応用技術 ◯ ◯ ◯ ResNet ◯ ◯ ◯ EfficientNet ◯ - - ◯ 物体検知と SS 解説 ◯ - - ◯ Mask R-CNN ◯ - - ◯ FCOS ◯ - - ◯ Transformer ◯ - - ◯ BERT ◯ ◯ ◯ ◯ GPT ◯ - - ◯ 音声認識 ◯ - - ◯ CTC ◯ - - DCGAN ◯ - - ◯ Conditinal GAN ◯ - - ◯ Pix2Pix ◯ - - ◯ A3C ◯ - - Metric-Learning ◯ - - MAML ◯ - - GCN ◯ - - ◯ CAM,Grad-CAM,LIME,SHAP ◯ ◯ - Docker ◯ - - 合計：４５ 基準：３６</description>
    </item>
    
    <item>
      <title>DeepLearningDay3</title>
      <link>https://half-broken-engineer.github.io/2023/03/deeplearningday3/</link>
      <pubDate>Wed, 29 Mar 2023 00:00:00 +0900</pubDate>
      
      <guid>https://half-broken-engineer.github.io/2023/03/deeplearningday3/</guid>
      <description>要点(最低100字) 再帰型NNの概念 要点 実装キャプチャ・演習 LSTM 要点 GRU 要点 実装キャプチャ ・演習 双方向RNN 要点 実装キャプチャ・演習 Seq2Seq 要点 Word2vec 要点 Attention Mechanism 要点 VQ-VAE 要点 フレームワーク演習：双方向RNN/勾配のクリッピング 要点 実装キャプチャ フレームワーク演習：Seq2Seq 要点 実装キャプチャ フレームワーク演習：Data-Augumentation 要点 実装キャプチャ フレームワーク演習：Activate-Functions 要点 実装キャプチャ 再帰型NNの概念 再帰型NN→RNN
時系列データとは 時間的順序を追って一定間隔ごとに観察され、 相互に統計的依存関係が認められるようなデータの系列
時系列データの例 音声データ テキストデータ RNNの全体像 $u^t = W_{(in)}x^t + W{z^{t-1}} + b$ $z^t = f(W_{(in)}x^t + Wz^{t-1} + b)$ $v^t = W_{(out)} z^t + c$ $y^t = g(W_{(out)} z^t + c)$</description>
    </item>
    
    <item>
      <title>DeepLearningDay2</title>
      <link>https://half-broken-engineer.github.io/2023/03/deeplearningday2/</link>
      <pubDate>Sat, 25 Mar 2023 00:00:00 +0900</pubDate>
      
      <guid>https://half-broken-engineer.github.io/2023/03/deeplearningday2/</guid>
      <description>要点(最低100字) 勾配消失問題 誤差逆伝播の復習 誤差から微分を逆算していくことで不要な再帰的計算を避けて微分を算出できる
確認問題１ 勾配消失問題の復習 誤差逆伝播が入力層に進んでいくに連れて勾配がどんどん減衰していくために パラメータ更新が入力層に近い側で進まなくなり、最適値に収束しなくなる現象
微分値の絶対値が１未満になると減衰していくことになる。
活性化関数の微分 $(1-sigmoid(x))\cdot sigmoid(x)$ シグモイド関数の微分は最大値が0.25であり、多層になると減衰が進んでいく。 （実装ノートより引用）
確認問題２ （２）
活性化関数による勾配消失対策 ReLU関数：勾配消失問題への対応とスパース化で貢献
（講義スライドより引用） 微分値は正の範囲で１，負の範囲で０
重みの初期化 Xavierの初期化 手法 正規分布での初期化値を一つ前の層のノード数で割る。（初期の方法でLeCunが提案したもの。） →各レイヤの出力の分散は「$n_{in} \times var_{in} \times var_{out}$」となるので、出力の分散を入力ノード数で割ることで分散を一定にできる。 逆伝播の方向も考慮した、入出力の平均値で割ってスケーリングするのが現在の方法
network[&amp;#39;W1&amp;#39;] = np.random.randn(input_layer_size, hidden_layer_size) / np.sqrt((input_layer_size+hidden_layer_size)/2) network[&amp;#39;W2&amp;#39;] = np.random.randn(hidden_layer_size, output_layer_size) / np.sqrt((hidden_layer_size+output_layer_size)/2) 対象の活性化関数 sigmoid 双曲線関数 ※Xavierの初期化は0近辺で線形近似できる前提をおいているので、ReLUには使えない。 効果 もともとSigmoidに対して、正規分布で初期化した際は活性化関数への入力が０から外れすぎることで出力が0か1に偏っていたのが、 分散を抑制することで0~1にうまくバラける様になった。
またネットワーク全体で見たときに各層の分散が一定に保たれる。
Heの初期化 手法 あるレイヤーを経た後の出力の分散は、ReLU を考慮すると、「1/2 × $n_{in}$ × 入力の分散 × 重みの分散」となります。そこで、Kaiming (He) 初期化では、重みを
として、標準偏差が $\sqrt{2/n_{in}}$ の正規分布によって初期化
network[&amp;#39;W1&amp;#39;] = np.random.randn(input_layer_size, hidden_layer_size) / np.</description>
    </item>
    
    <item>
      <title>DeepLearningDay1</title>
      <link>https://half-broken-engineer.github.io/2023/01/deeplearningday1/</link>
      <pubDate>Sat, 28 Jan 2023 00:00:00 +0900</pubDate>
      
      <guid>https://half-broken-engineer.github.io/2023/01/deeplearningday1/</guid>
      <description>要点(最低100字) 識別と生成 識別：データを目的別クラスに分類する $P(C_k|x)$：条件Xが与えられた条件の元でのクラスkの確率 高次元から低次元へ 必要な学習データは比較的少ない 応用例 画像認識 生成：特定クラスのデータを生成する $P(x|C_k)$：クラスｋという条件のものとでのデータCkの分布 低次元から高次元へ 必要な学習データが多い 応用例 画像の超解像 テキスト生成 主要なモデル 識別モデル [[決定木]] ロジスティック回帰 SVM [[08_project/G_Certification/ニューラルネットワーク]] 生成モデル 隠れマルコフモデル ベイジアンネットワーク VAE GAN DRAW 識別器の開発アプローチ 上から順に学習コストが大→小
生成モデル的アプローチ ベイズの定理を活用 モデル化の対象 各クラスの生起確率 データのクラス条件付き密度 データを人工的に生成できる 確率的な識別 識別モデル 決定理論に基づき識別結果を得る データがクラスに属する確率をモデル化 確率的な識別 識別関数 入力値ｘを直接クラスに写像（変換）する関数f(x)を推定 データの属するクラス情報のみ（確率は計算されない） 決定的な識別 生成モデルと識別モデルの比較 生成モデルのアプローチではより複雑なデータ分布を学習しようとするので、計算量が多い。
（講義ビデオより引用）
識別モデルと識別関数 モデルは推論結果の取り扱いを変更でき、間違いの程度も評価できるが 識別関数ではそれらが出来ない。結果を一足飛びに得る。
（講義ビデオより引用）
深層学習の強み：万能近似定理 万能近似定理：活性化関数をもつネットワークを使うことで、どんな関数でも近似できるという定理 これまでの機械学習では人間が関数を設計していた [[ノーフリーランチの定理]]
ニューラルネットの数学的表現 (ラビットチャレンジの深層学習day1講義資料より引用)
回帰と分類の違い 回帰：連続する実数値を取る関数の近似 分類：離散的な結果を予想するための分析
線形と非線形の違い 線形な関数は以下の性質を持つが、非線形関数は持たない。
加法性:$f(a+b) = f(a) + f(b)$ 斉次性$f(ka) = k\times f(a)$ [[活性化関数]] ステップ関数 def step_func(x): if x &amp;gt; 0: return 1 else: return 0 問題点：線形分離可能なものしか学習出来なかった</description>
    </item>
    
    <item>
      <title>MachineLearning</title>
      <link>https://half-broken-engineer.github.io/2023/01/machinelearning/</link>
      <pubDate>Thu, 26 Jan 2023 00:00:00 +0900</pubDate>
      
      <guid>https://half-broken-engineer.github.io/2023/01/machinelearning/</guid>
      <description>要点(最低100字) 機械学習モデリングプロセス 問題設定 データ選定 前処理 機械学習モデル剪定 モデルの学習 モデルの評価 ルールベースと機械学習の比較 タスクTと性能指標Pがあるときに、性能が経験Eによって改善されるとき、 タスクTおよび性能指標Pに関して経験E から学習すると言われる
人がプログラムするのは学習の仕方　（認識の仕方では無い） ルールベースは認識の仕方自体をプログラムする。
主なモデル 教師あり学習 回帰 線形回帰・非線形回帰：最小二乗法、尤度最適化 分類 ロジスティック回帰；尤度最大化 最近傍・K-近傍アルゴリズム SVM：マージン最大化 教師なし学習 クラスタリング K-Means 次元削減 主成分分析：分散最大化 回帰問題 入力（説明変数、特徴量）から出力（目的変数）を予測する問題 一般的に、外挿問題（学習データに含まれない範囲の値域）での予測の精度が下がる。 どの学習方法でも共通。
線形回帰 線形とは→ざっくりというと比例。　n次元の超平面の方程式
直線、平面→線形回帰 曲線、局面→非線形
線形回帰に関連する値 教師データに含まれるもの 入力$\vec{x} = (x_1,x_2,\dots,x_m)^\top \in R^m$ 出力$y\in R$ 学習によって最適化していくもの パラメータ$\vec{w} = (w_1,w_2,\dots,w_m)^\top \in R^m$ 切片$w_0$ 学習の際に使う値 予測値$\hat{y} = w_0 + \sum_{i=1}^n \vec{w}^\top \vec{x}$　（※データセットで考えるときは$\vec{\hat{y}} = X・\vec{w}$） 誤差$\epsilon = y - \hat{y}$ 線形回帰の最適化手法(最尤法) 最小２乗誤差　$MSE = \frac{1}{n_{train}}\sum(y-\hat{y})^2$ を用いて最適化する。 $\hat{\vec{w}} = arg \min_{w \in R^m} MSE$ MSEが最小値を取るとき $$\frac{\partial MSE}{\partial \vec{w}} = 0$$</description>
    </item>
    
    <item>
      <title>AppliedMathematics</title>
      <link>https://half-broken-engineer.github.io/2023/01/appliedmathematics/</link>
      <pubDate>Wed, 25 Jan 2023 00:00:00 +0900</pubDate>
      
      <guid>https://half-broken-engineer.github.io/2023/01/appliedmathematics/</guid>
      <description>要点(最低100字) 目次 線形代数 確率・統計 情報理論 線形代数 固有値・固有ベクトルの求め方 固有値分解とは 特異値・特位置ベクトルの概要 特異値分解について 固有値・固有ベクトル 固有ベクトルは対応する行列をかけた際に、ベクトルの方向が変わらないベクトル。 固有値は固有ベクトルに行列をかけたときに何倍になるかを示す。
求め方は[[固有方程式]]で固有値を求めたあと、固有値を（１）式に代入して連立方程式を解く。
固有ベクトルの大きさについては１にしておくのが一般的。（※後の特異値を求めるときにそちらのほうが都合が良い）
固有値分解 固有値ベクトルを列ベクトルに持つ行列Vと、対応する固有値を対角成分に持つ対角行列Λを用いて行列Aは以下の式に分解できる。
プログラミング的には、対角行列の演算負荷が低いことから、計算速度向上、リソース節約につながる。
import numpy as np values,vectors = np.linalg.eig(matrix) 特異値・特異ベクトル 固有値については正方行列のみしか扱えなかったが、これをm×nの行列に拡張したもの。
Uの列ベクトルを左特異ベクトル、、Vの列ベクトルを右特異ベクトルという。 Σの対角成分は特異値という。 ※このとき、ベクトルのノルムは１、σ&amp;gt;0,特異値の数はmin(m,n)となる。今回はｍ＜ｎとした。
特異値分解 $AA^\top$または$A^\top A$を求める　（計算量の問題で、$A^\top A$のほうが次数が少ないときはそちらを使う。式(4)に$A^\top$を掛けるか、式(5)にAを掛けるかの違い） $AA^\top$または$A^\top A$の固有方程式を解く 固有値の正の平方根を特異値とする 特異値を$(A^{\top} A - \sigma^2 E) \vec{v} = \vec{0}$または$(AA^{\top} - \sigma^2 E ) \vec{u} = \vec{0}$に代入して、どちらかの特異ベクトルを求める※重解のときはグラムシュミットの直交化方を用いて、同じ特異値に対する特異ベクトルを直交化すること。 式(4)または(5)を用いてもう一方の特異ベクトルを求める import numpy as np u,sigma,vt = np.linalg.svd(matrix) 確率・統計 条件付き確率について理解を深める ベイズ則の概要を知る 期待値・分散の求め方 様々な確率分布の概要を知る 客観確率と主観確率 客観確率：頻度確率とも言う。データに基づく事実 主観確率：ベイズ確率。条件の元で推測される信念の度合い。　診断など 条件付き確率 事象Y＝ｙが与えられた上での、事象X=xとなる確率 $P(X=x|Y=y) = \frac{P(X=x,Y=y)}{P(Y=y)}$</description>
    </item>
    
    <item>
      <title>RabbitChallenge</title>
      <link>https://half-broken-engineer.github.io/2023/01/rabbitchallenge/</link>
      <pubDate>Wed, 25 Jan 2023 00:00:00 +0900</pubDate>
      
      <guid>https://half-broken-engineer.github.io/2023/01/rabbitchallenge/</guid>
      <description> ラビット★チャレンジ : よくある質問（受講者用）
修了要件 以下3つの条件をみたすことで修了者ナンバーを受領できる。受験の際にはそのナンバーが必要になる。
シラバス変更後も2年間は有効。
ビデオ学習 レポート全提出 認定テスト合格 実行環境 [[GoogleColaboratory]]
レポート作成 各章につき100文字以上で要点をまとめ、実装演習結果、確認テストについての自身の考察を入れる 各科目の基準点が足りない場合や、実装演習が不足する場合は差し戻し レポートは、「機械学習」「深層学習day1」等 科目ごとに１つのレポート レポートの確認（受理完了）は、約3～7営業日程いただいております。 ※修了認定間際のレポート提出は、万が一何か不備があった場合でも事務局からの差し戻しができず、 軽微なミスでも不合格となったり修了認定が出来なくなることがございます。 余裕を持った学習計画を立てた上で取り組みをお願いいたします。 形式：PDF 提出方法：各種レポート提出フォーム［PDF版］ラビット受講生専用 実装演習 基本的に実装演習コードにそって行う。
デバッグ [[Python主要エラー]]を参考にpythonコードはデバッグする。
参考サイト Stack overflow回答は英語。私はほとんどこれを読んで解決します Stack overflow(日本語)最近出てきた。まだ検索で引っかかることは少ない 質問してみても良いかも QA@IT日本語Q&amp;amp;Aサイト。見たことがない teratail【テラテイル】｜ITエンジニア特化型Q&amp;amp;Aサイト 日本のPythonコミュニティ - python.jp GIthubのIssue </description>
    </item>
    
  </channel>
</rss>
