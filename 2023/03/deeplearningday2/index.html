<!DOCTYPE html>
<html lang="ja">
  <head>
    
    <script type="application/ld+json">

{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "DeepLearningDay2",
  
  "datePublished": "2023-03-25T00:00:00+09:00",
  "dateModified": "2023-03-25T00:00:00+09:00",
  "author": {
    "@type": "Person",
    "name": "Half-Broken Engineer",
    
    "image": "https://half-broken-engineer.github.io/img/profile.png"
    
  },
  "mainEntityOfPage": { 
    "@type": "WebPage",
    "@id": "https:\/\/half-broken-engineer.github.io\/2023\/03\/deeplearningday2\/" 
  },
  "publisher": {
    "@type": "Organization",
    "name": "壊れかけのエンジニアのログ",
    
    "logo": {
      "@type": "ImageObject",
      "url": "https://half-broken-engineer.github.io/img/profile.png"
    }
    
  },
  "description": "要点(最低100字) 勾配消失問題 誤差逆伝播の復習 誤差から微分を逆算していくことで不要な再帰的計算を避けて微分を算出できる\n確認問題１ 勾配消失問題の復習 誤差逆伝播が入力層に進んでいくに連れて勾配がどんどん減衰していくために パラメータ更新が入力層に近い側で進まなくなり、最適値に収束しなくなる現象\n微分値の絶対値が１未満になると減衰していくことになる。\n活性化関数の微分 $(1-sigmoid(x))\\cdot sigmoid(x)$ シグモイド関数の微分は最大値が0.25であり、多層になると減衰が進んでいく。 （実装ノートより引用）\n確認問題２ （２）\n活性化関数による勾配消失対策 ReLU関数：勾配消失問題への対応とスパース化で貢献\n（講義スライドより引用） 微分値は正の範囲で１，負の範囲で０\n重みの初期化 Xavierの初期化 手法 正規分布での初期化値を一つ前の層のノード数で割る。（初期の方法でLeCunが提案したもの。） →各レイヤの出力の分散は「$n_{in} \\times var_{in} \\times var_{out}$」となるので、出力の分散を入力ノード数で割ることで分散を一定にできる。 逆伝播の方向も考慮した、入出力の平均値で割ってスケーリングするのが現在の方法\nnetwork[\u0026#39;W1\u0026#39;] = np.random.randn(input_layer_size, hidden_layer_size) / np.sqrt((input_layer_size+hidden_layer_size)/2) network[\u0026#39;W2\u0026#39;] = np.random.randn(hidden_layer_size, output_layer_size) / np.sqrt((hidden_layer_size+output_layer_size)/2) 対象の活性化関数 sigmoid 双曲線関数 ※Xavierの初期化は0近辺で線形近似できる前提をおいているので、ReLUには使えない。 効果 もともとSigmoidに対して、正規分布で初期化した際は活性化関数への入力が０から外れすぎることで出力が0か1に偏っていたのが、 分散を抑制することで0~1にうまくバラける様になった。\nまたネットワーク全体で見たときに各層の分散が一定に保たれる。\nHeの初期化 手法 あるレイヤーを経た後の出力の分散は、ReLU を考慮すると、「1/2 × $n_{in}$ × 入力の分散 × 重みの分散」となります。そこで、Kaiming (He) 初期化では、重みを\nとして、標準偏差が $\\sqrt{2/n_{in}}$ の正規分布によって初期化\nnetwork[\u0026#39;W1\u0026#39;] = np.random.randn(input_layer_size, hidden_layer_size) / np.",
  "keywords": []
}

</script>
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="generator" content="Hugo 0.112.5 with theme Tranquilpeak 0.5.3-BETA">
<meta name="author" content="Half-Broken Engineer">
<meta name="keywords" content="">
<meta name="description" content="要点(最低100字) 勾配消失問題 誤差逆伝播の復習 誤差から微分を逆算していくことで不要な再帰的計算を避けて微分を算出できる
確認問題１ 勾配消失問題の復習 誤差逆伝播が入力層に進んでいくに連れて勾配がどんどん減衰していくために パラメータ更新が入力層に近い側で進まなくなり、最適値に収束しなくなる現象
微分値の絶対値が１未満になると減衰していくことになる。
活性化関数の微分 $(1-sigmoid(x))\cdot sigmoid(x)$ シグモイド関数の微分は最大値が0.25であり、多層になると減衰が進んでいく。 （実装ノートより引用）
確認問題２ （２）
活性化関数による勾配消失対策 ReLU関数：勾配消失問題への対応とスパース化で貢献
（講義スライドより引用） 微分値は正の範囲で１，負の範囲で０
重みの初期化 Xavierの初期化 手法 正規分布での初期化値を一つ前の層のノード数で割る。（初期の方法でLeCunが提案したもの。） →各レイヤの出力の分散は「$n_{in} \times var_{in} \times var_{out}$」となるので、出力の分散を入力ノード数で割ることで分散を一定にできる。 逆伝播の方向も考慮した、入出力の平均値で割ってスケーリングするのが現在の方法
network[&#39;W1&#39;] = np.random.randn(input_layer_size, hidden_layer_size) / np.sqrt((input_layer_size&#43;hidden_layer_size)/2) network[&#39;W2&#39;] = np.random.randn(hidden_layer_size, output_layer_size) / np.sqrt((hidden_layer_size&#43;output_layer_size)/2) 対象の活性化関数 sigmoid 双曲線関数 ※Xavierの初期化は0近辺で線形近似できる前提をおいているので、ReLUには使えない。 効果 もともとSigmoidに対して、正規分布で初期化した際は活性化関数への入力が０から外れすぎることで出力が0か1に偏っていたのが、 分散を抑制することで0~1にうまくバラける様になった。
またネットワーク全体で見たときに各層の分散が一定に保たれる。
Heの初期化 手法 あるレイヤーを経た後の出力の分散は、ReLU を考慮すると、「1/2 × $n_{in}$ × 入力の分散 × 重みの分散」となります。そこで、Kaiming (He) 初期化では、重みを
として、標準偏差が $\sqrt{2/n_{in}}$ の正規分布によって初期化
network[&#39;W1&#39;] = np.random.randn(input_layer_size, hidden_layer_size) / np.">


<meta property="og:description" content="要点(最低100字) 勾配消失問題 誤差逆伝播の復習 誤差から微分を逆算していくことで不要な再帰的計算を避けて微分を算出できる
確認問題１ 勾配消失問題の復習 誤差逆伝播が入力層に進んでいくに連れて勾配がどんどん減衰していくために パラメータ更新が入力層に近い側で進まなくなり、最適値に収束しなくなる現象
微分値の絶対値が１未満になると減衰していくことになる。
活性化関数の微分 $(1-sigmoid(x))\cdot sigmoid(x)$ シグモイド関数の微分は最大値が0.25であり、多層になると減衰が進んでいく。 （実装ノートより引用）
確認問題２ （２）
活性化関数による勾配消失対策 ReLU関数：勾配消失問題への対応とスパース化で貢献
（講義スライドより引用） 微分値は正の範囲で１，負の範囲で０
重みの初期化 Xavierの初期化 手法 正規分布での初期化値を一つ前の層のノード数で割る。（初期の方法でLeCunが提案したもの。） →各レイヤの出力の分散は「$n_{in} \times var_{in} \times var_{out}$」となるので、出力の分散を入力ノード数で割ることで分散を一定にできる。 逆伝播の方向も考慮した、入出力の平均値で割ってスケーリングするのが現在の方法
network[&#39;W1&#39;] = np.random.randn(input_layer_size, hidden_layer_size) / np.sqrt((input_layer_size&#43;hidden_layer_size)/2) network[&#39;W2&#39;] = np.random.randn(hidden_layer_size, output_layer_size) / np.sqrt((hidden_layer_size&#43;output_layer_size)/2) 対象の活性化関数 sigmoid 双曲線関数 ※Xavierの初期化は0近辺で線形近似できる前提をおいているので、ReLUには使えない。 効果 もともとSigmoidに対して、正規分布で初期化した際は活性化関数への入力が０から外れすぎることで出力が0か1に偏っていたのが、 分散を抑制することで0~1にうまくバラける様になった。
またネットワーク全体で見たときに各層の分散が一定に保たれる。
Heの初期化 手法 あるレイヤーを経た後の出力の分散は、ReLU を考慮すると、「1/2 × $n_{in}$ × 入力の分散 × 重みの分散」となります。そこで、Kaiming (He) 初期化では、重みを
として、標準偏差が $\sqrt{2/n_{in}}$ の正規分布によって初期化
network[&#39;W1&#39;] = np.random.randn(input_layer_size, hidden_layer_size) / np.">
<meta property="og:type" content="article">
<meta property="og:title" content="DeepLearningDay2">
<meta name="twitter:title" content="DeepLearningDay2">
<meta property="og:url" content="https://half-broken-engineer.github.io/2023/03/deeplearningday2/">
<meta property="twitter:url" content="https://half-broken-engineer.github.io/2023/03/deeplearningday2/">
<meta property="og:site_name" content="壊れかけのエンジニアのログ">
<meta property="og:description" content="要点(最低100字) 勾配消失問題 誤差逆伝播の復習 誤差から微分を逆算していくことで不要な再帰的計算を避けて微分を算出できる
確認問題１ 勾配消失問題の復習 誤差逆伝播が入力層に進んでいくに連れて勾配がどんどん減衰していくために パラメータ更新が入力層に近い側で進まなくなり、最適値に収束しなくなる現象
微分値の絶対値が１未満になると減衰していくことになる。
活性化関数の微分 $(1-sigmoid(x))\cdot sigmoid(x)$ シグモイド関数の微分は最大値が0.25であり、多層になると減衰が進んでいく。 （実装ノートより引用）
確認問題２ （２）
活性化関数による勾配消失対策 ReLU関数：勾配消失問題への対応とスパース化で貢献
（講義スライドより引用） 微分値は正の範囲で１，負の範囲で０
重みの初期化 Xavierの初期化 手法 正規分布での初期化値を一つ前の層のノード数で割る。（初期の方法でLeCunが提案したもの。） →各レイヤの出力の分散は「$n_{in} \times var_{in} \times var_{out}$」となるので、出力の分散を入力ノード数で割ることで分散を一定にできる。 逆伝播の方向も考慮した、入出力の平均値で割ってスケーリングするのが現在の方法
network[&#39;W1&#39;] = np.random.randn(input_layer_size, hidden_layer_size) / np.sqrt((input_layer_size&#43;hidden_layer_size)/2) network[&#39;W2&#39;] = np.random.randn(hidden_layer_size, output_layer_size) / np.sqrt((hidden_layer_size&#43;output_layer_size)/2) 対象の活性化関数 sigmoid 双曲線関数 ※Xavierの初期化は0近辺で線形近似できる前提をおいているので、ReLUには使えない。 効果 もともとSigmoidに対して、正規分布で初期化した際は活性化関数への入力が０から外れすぎることで出力が0か1に偏っていたのが、 分散を抑制することで0~1にうまくバラける様になった。
またネットワーク全体で見たときに各層の分散が一定に保たれる。
Heの初期化 手法 あるレイヤーを経た後の出力の分散は、ReLU を考慮すると、「1/2 × $n_{in}$ × 入力の分散 × 重みの分散」となります。そこで、Kaiming (He) 初期化では、重みを
として、標準偏差が $\sqrt{2/n_{in}}$ の正規分布によって初期化
network[&#39;W1&#39;] = np.random.randn(input_layer_size, hidden_layer_size) / np.">
<meta name="twitter:description" content="要点(最低100字) 勾配消失問題 誤差逆伝播の復習 誤差から微分を逆算していくことで不要な再帰的計算を避けて微分を算出できる
確認問題１ 勾配消失問題の復習 誤差逆伝播が入力層に進んでいくに連れて勾配がどんどん減衰していくために パラメータ更新が入力層に近い側で進まなくなり、最適値に収束しなくなる現象
微分値の絶対値が１未満になると減衰していくことになる。
活性化関数の微分 $(1-sigmoid(x))\cdot sigmoid(x)$ シグモイド関数の微分は最大値が0.25であり、多層になると減衰が進んでいく。 （実装ノートより引用）
確認問題２ （２）
活性化関数による勾配消失対策 ReLU関数：勾配消失問題への対応とスパース化で貢献
（講義スライドより引用） 微分値は正の範囲で１，負の範囲で０
重みの初期化 Xavierの初期化 手法 正規分布での初期化値を一つ前の層のノード数で割る。（初期の方法でLeCunが提案したもの。） →各レイヤの出力の分散は「$n_{in} \times var_{in} \times var_{out}$」となるので、出力の分散を入力ノード数で割ることで分散を一定にできる。 逆伝播の方向も考慮した、入出力の平均値で割ってスケーリングするのが現在の方法
network[&#39;W1&#39;] = np.random.randn(input_layer_size, hidden_layer_size) / np.sqrt((input_layer_size&#43;hidden_layer_size)/2) network[&#39;W2&#39;] = np.random.randn(hidden_layer_size, output_layer_size) / np.sqrt((hidden_layer_size&#43;output_layer_size)/2) 対象の活性化関数 sigmoid 双曲線関数 ※Xavierの初期化は0近辺で線形近似できる前提をおいているので、ReLUには使えない。 効果 もともとSigmoidに対して、正規分布で初期化した際は活性化関数への入力が０から外れすぎることで出力が0か1に偏っていたのが、 分散を抑制することで0~1にうまくバラける様になった。
またネットワーク全体で見たときに各層の分散が一定に保たれる。
Heの初期化 手法 あるレイヤーを経た後の出力の分散は、ReLU を考慮すると、「1/2 × $n_{in}$ × 入力の分散 × 重みの分散」となります。そこで、Kaiming (He) 初期化では、重みを
として、標準偏差が $\sqrt{2/n_{in}}$ の正規分布によって初期化
network[&#39;W1&#39;] = np.random.randn(input_layer_size, hidden_layer_size) / np.">
<meta property="og:locale" content="ja">

  
    <meta property="article:published_time" content="2023-03-25T00:00:00">
  
  
    <meta property="article:modified_time" content="2023-03-25T00:00:00">
  
  
  
    
      <meta property="article:section" content="RabbitChallenge">
    
  
  
    
      <meta property="article:tag" content="obsidian_note">
    
  


<meta name="twitter:card" content="summary">

  <meta name="twitter:site" content="@overcome_kidney">


  <meta name="twitter:creator" content="@overcome_kidney">






  <meta property="og:image" content="https://half-broken-engineer.github.io/img/profile.png">
  <meta property="twitter:image" content="https://half-broken-engineer.github.io/img/profile.png">






    <title>DeepLearningDay2</title>

    <link rel="icon" href="https://half-broken-engineer.github.io/favicon.png">
    

    

    <link rel="canonical" href="https://half-broken-engineer.github.io/2023/03/deeplearningday2/">

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" integrity="sha512-iBBXm8fW90+nuLcSKlbmrPcLa0OT92xO1BIsZ+ywDWZCvqsWgccV3gFoRBv0z+8dLJgyAHIhR35VZc2oM/gI1w==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha512-H9jrZiiopUdsLpg94A333EfumgUBpO9MdbxStdeITo+KEIMaNfHNvwyjjDJb+ERPaRS6DpyRlKbvPUasNItRyw==" crossorigin="anonymous" referrerpolicy="no-referrer" />
    
    
    
    <link rel="stylesheet" href="https://half-broken-engineer.github.io/css/style-h6ccsoet3mzkbb0wngshlfbaweimexgqcxj0h5hu4h82olsdzz6wmqdkajm.min.css" />
    
    
      
        <link rel="stylesheet"  href="https://half-broken-engineer.github.io/css/mystyle.css">
      
    

    
      
    
    
  </head>

  <body>
    <div id="blog">
      <header id="header" data-behavior="5">
  <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
  <div class="header-title">
    <a class="header-title-link" href="https://half-broken-engineer.github.io/" aria-label="ホームページへ">壊れかけのエンジニアのログ</a>
  </div>
  
    
      <a class="header-right-picture "
         href="https://half-broken-engineer.github.io/#about" aria-label="リンクを開く: /#about">
    
    
    
      
        <img class="header-picture" src="https://half-broken-engineer.github.io/img/profile.png" alt="プロフィール画像" />
      
    
    </a>
  
</header>

      <nav id="sidebar" data-behavior="5">
  <div class="sidebar-container">
    
      <div class="sidebar-profile">
        <a href="https://half-broken-engineer.github.io/#about" aria-label="著者についてもっと読む">
          <img class="sidebar-profile-picture" src="https://half-broken-engineer.github.io/img/profile.png" alt="プロフィール画像" />
        </a>
        <h4 class="sidebar-profile-name">Half-Broken Engineer</h4>
        
          <h5 class="sidebar-profile-bio">🤖　　　　壊れかけのエンジニア　　　　💻不安を解消したいから💰のお勉強もする</h5>
        
      </div>
    
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://half-broken-engineer.github.io/" title="Home">
    
      <i class="sidebar-button-icon fas fa-lg fa-home" aria-hidden="true"></i>
      
      <span class="sidebar-button-desc">ホーム</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://half-broken-engineer.github.io/categories" title="Categories">
    
      <i class="sidebar-button-icon fas fa-lg fa-bookmark" aria-hidden="true"></i>
      
      <span class="sidebar-button-desc">カテゴリー</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://half-broken-engineer.github.io/tags" title="Tags">
    
      <i class="sidebar-button-icon fas fa-lg fa-tags" aria-hidden="true"></i>
      
      <span class="sidebar-button-desc">タグ</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://half-broken-engineer.github.io/archives" title="Archives">
    
      <i class="sidebar-button-icon fas fa-lg fa-archive" aria-hidden="true"></i>
      
      <span class="sidebar-button-desc">アーカイブ</span>
    </a>
  </li>

  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://half-broken-engineer.github.io/#about" title="About">
    
      <i class="sidebar-button-icon fas fa-lg fa-question" aria-hidden="true"></i>
      
      <span class="sidebar-button-desc">プロフィール</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://github.com/Half-Broken-Engineer" target="_blank" rel="noopener" title="GitHub">
    
      <i class="sidebar-button-icon fab fa-lg fa-github" aria-hidden="true"></i>
      
      <span class="sidebar-button-desc">GitHub</span>
    </a>
  </li>


    </ul>
    <ul class="sidebar-buttons">
      
  <li class="sidebar-button">
    
      <a class="sidebar-button-link " href="https://half-broken-engineer.github.io/index.xml" title="RSS">
    
      <i class="sidebar-button-icon fas fa-lg fa-rss" aria-hidden="true"></i>
      
      <span class="sidebar-button-desc">RSS</span>
    </a>
  </li>


    </ul>
  </div>
</nav>

      

      <div id="main" data-behavior="5"
        class="
               hasCoverMetaIn
               ">
        <article class="post" id="top">
          
          
            <div class="post-header main-content-wrap text-left">
  
    <h1 class="post-title">
      DeepLearningDay2
    </h1>
  
  
  <div class="postShorten-meta post-meta">
    
      <time datetime="2023-03-25T00:00:00&#43;09:00">
        
  
  
  
  
    2023-03-25
  

      </time>
    
    
  
  
    <span>カテゴリー</span>
    
      <a class="category-link" href="https://half-broken-engineer.github.io/categories/rabbitchallenge">RabbitChallenge</a>
    
  

  </div>

</div>
          
          <div class="post-content markdown">
            <div class="main-content-wrap">
              <ul>
<li></li>
</ul>
<h1 id="要点最低100字">要点(最低100字)</h1>
<h2 id="勾配消失問題">勾配消失問題</h2>
<h3 id="誤差逆伝播の復習">誤差逆伝播の復習</h3>
<p>誤差から微分を逆算していくことで不要な再帰的計算を避けて微分を算出できる</p>
<h4 id="確認問題１">確認問題１</h4>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230325225109.png" alt="/img/Pasted_image_20230325225109.png">
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230325225654.png" alt="/img/Pasted_image_20230325225654.png"></p>
<h3 id="勾配消失問題の復習">勾配消失問題の復習</h3>
<p>誤差逆伝播が入力層に進んでいくに連れて勾配がどんどん減衰していくために
パラメータ更新が入力層に近い側で進まなくなり、最適値に収束しなくなる現象</p>
<p>微分値の絶対値が１未満になると減衰していくことになる。</p>
<h3 id="活性化関数の微分">活性化関数の微分</h3>
<p>$(1-sigmoid(x))\cdot sigmoid(x)$
シグモイド関数の微分は最大値が0.25であり、多層になると減衰が進んでいく。
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230325230416.png" alt="/img/Pasted_image_20230325230416.png">
（実装ノートより引用）</p>
<h4 id="確認問題２">確認問題２</h4>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230325230555.png" alt="/img/Pasted_image_20230325230555.png">
（２）</p>
<h3 id="活性化関数による勾配消失対策">活性化関数による勾配消失対策</h3>
<p>ReLU関数：勾配消失問題への対応とスパース化で貢献</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230325230753.png" alt="/img/Pasted_image_20230325230753.png">
（講義スライドより引用）
微分値は正の範囲で１，負の範囲で０</p>
<h3 id="重みの初期化">重みの初期化</h3>
<h4 id="xavierの初期化">Xavierの初期化</h4>
<h5 id="手法">手法</h5>
<p>正規分布での初期化値を一つ前の層のノード数で割る。（初期の方法でLeCunが提案したもの。）
→各レイヤの出力の分散は「$n_{in} \times var_{in} \times var_{out}$」となるので、出力の分散を入力ノード数で割ることで分散を一定にできる。
逆伝播の方向も考慮した、入出力の平均値で割ってスケーリングするのが現在の方法</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>network[<span style="color:#e6db74">&#39;W1&#39;</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(input_layer_size, hidden_layer_size) <span style="color:#f92672">/</span> np<span style="color:#f92672">.</span>sqrt((input_layer_size<span style="color:#f92672">+</span>hidden_layer_size)<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>network[<span style="color:#e6db74">&#39;W2&#39;</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(hidden_layer_size, output_layer_size) <span style="color:#f92672">/</span> np<span style="color:#f92672">.</span>sqrt((hidden_layer_size<span style="color:#f92672">+</span>output_layer_size)<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span>)
</span></span></code></pre></div><h5 id="対象の活性化関数">対象の活性化関数</h5>
<ul>
<li>sigmoid</li>
<li>双曲線関数
※Xavierの初期化は0近辺で線形近似できる前提をおいているので、ReLUには使えない。</li>
</ul>
<h5 id="効果">効果</h5>
<p>もともとSigmoidに対して、正規分布で初期化した際は活性化関数への入力が０から外れすぎることで出力が0か1に偏っていたのが、
分散を抑制することで0~1にうまくバラける様になった。</p>
<p>またネットワーク全体で見たときに各層の分散が一定に保たれる。</p>
<h4 id="heの初期化">Heの初期化</h4>
<h5 id="手法-1">手法</h5>
<p>あるレイヤーを経た後の出力の分散は、ReLU を考慮すると、「1/2 × $n_{in}$ × 入力の分散 × 重みの分散」となります。そこで、Kaiming (He) 初期化では、重みを</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230123232706.png" alt="/img/Pasted_image_20230123232706.png"></p>
<p>として、標準偏差が $\sqrt{2/n_{in}}$ の正規分布によって初期化</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Python" data-lang="Python"><span style="display:flex;"><span>network[<span style="color:#e6db74">&#39;W1&#39;</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(input_layer_size, hidden_layer_size) <span style="color:#f92672">/</span> np<span style="color:#f92672">.</span>sqrt(input_layer_size) <span style="color:#f92672">*</span> sqrt(<span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>network[<span style="color:#e6db74">&#39;W2&#39;</span>] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randn(hidden_layer_size, output_layer_size) <span style="color:#f92672">/</span> np<span style="color:#f92672">.</span>sqrt(hidden_layer_size) <span style="color:#f92672">*</span> sqrt(<span style="color:#ae81ff">2</span>)
</span></span></code></pre></div><h5 id="対象の活性化関数-1">対象の活性化関数</h5>
<ul>
<li>ReLU</li>
</ul>
<h5 id="効果-1">効果</h5>
<p>ReLUに対して正規分布で初期化すると出力がほぼ０になるため表現力が失われていた。（活性化関数の式を見ればわかる）
Heの初期化をすることによって出力分布が0~1にうまくバラける。</p>
<h4 id="確認問題３">確認問題３</h4>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326074624.png" alt="/img/Pasted_image_20230326074624.png">
各層の出力がバイアスのみの関数になるといえるため、
入力に関係の無い、出力値の平均を学習するだけになる。</p>
<h3 id="バッチ正規化">[[バッチ正規化]]</h3>
<p>ミニバッチ単位で入力値の偏りをなくす手法。
活性化関数の前にバッチ正規化の処理を加える。
(各層の入力 OR 線形変換後の値）</p>
<h4 id="処理">処理</h4>
<ol>
<li>ミニバッチ単位での平均を求める</li>
<li>ミニバッチ単位での分散を求める</li>
<li>求めた平均と分散によって標準化</li>
<li>標準化した値をスケーリング及びシフトさせる
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230327111112.png" alt="/img/Pasted_image_20230327111112.png">
（<a href="https://yogeshluthra.github.io/ConvNets_OptimizationAlgorithms/">Neural Nets and Optimization Algorithms (Fully Connected and Convolutional Neural Nets) – Yogesh Luthra – Machine Learning, Data Science, Artificial Intelligence, Statistics</a>より引用）
※θはゼロ除算などを避けるための運用上の微小値</li>
</ol>
<h4 id="効果-2">効果</h4>
<ul>
<li>学習の高速化：極端に小さい値をクリッピングすることによる勾配消失問題への対応</li>
<li>学習の安定化：アクティベーション前後の分布が変化してしまう内部共変量シフトへの対応</li>
<li>過学習への対応：各層で正規化処理を行うことになるので外れ値が減り、過学習が抑制される。</li>
</ul>
<h4 id="確認問題４">確認問題４</h4>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326080600.png" alt="/img/Pasted_image_20230326080600.png">
内部共変量と勾配消失問題への対応</p>
<h4 id="例題チャレンジ">例題チャレンジ</h4>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326081527.png" alt="/img/Pasted_image_20230326081527.png">
（講義スライドより引用）
回答：（１）</p>
<h2 id="学習率最適化手法">学習率最適化手法</h2>
<h3 id="はじめに">はじめに</h3>
<p>勾配降下法によってパラメータを最適化していくが
その際に学習率の取り扱いが重要になる。
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326081807.png" alt="/img/Pasted_image_20230326081807.png">
（講義スライドより引用）</p>
<h3 id="学習率のトレードオフ">学習率のトレードオフ</h3>
<ul>
<li>大きいと最適値にたどり着かずに発散</li>
<li>小さいと
<ul>
<li>収束までに時間がかかる</li>
<li>局所最適解に陥る</li>
</ul>
</li>
</ul>
<h3 id="学習率の最適化">学習率の最適化</h3>
<ul>
<li>学習の段階に応じて学習率の大きさを小さくしていく</li>
<li>パラメータごとに可変にしていく</li>
</ul>
<h4 id="主な学習率最適化手法">主な学習率最適化手法</h4>
<ul>
<li>モメンタム：前回の更新量に係数をかけたものを更新に使う</li>
<li>AdaGrad：パラメータの要素ごとに学習率を調整する。勾配の絶対値が大きい要素には更新量を小さくする</li>
<li>RMSProp：</li>
<li>Adam</li>
</ul>
<p>※勾配に対する挙動については<a href="https://github.com/Jaewan-Yun/optimizer-visualization">GitHub - Jaewan-Yun/optimizer-visualization: Visualize Tensorflow&rsquo;s optimizers.</a>を参考にすると良い</p>
<h5 id="モメンタム">モメンタム</h5>
<p>※学習率自体の調整ではない
$V_t = \mu V_{t-1} - \epsilon \nabla E$
$W^{(t+1)} = W^{(t)} + V_t$</p>
<h6 id="メリット">メリット</h6>
<ul>
<li>局所最適解に陥りにくい</li>
<li>最適値への収束が早い</li>
</ul>
<h5 id="adagrad">AdaGrad</h5>
<p>$h_0 = \kappa$　　　　　　　　　　　 ：任意の値で初期化
$h_t = h_{t-1} + (\nabla E)^2$　　　　　　 ：勾配の二乗の値を累積していく
$W^{(t+1)} = W^{(t)} - \epsilon \frac{1}{\sqrt{h_t}+\theta} \nabla E$ 　　：ｈを用いて学習率を減衰させていく
※θは調整項、学習率の減衰が常に入っていくので初期値は他の方法の１０倍程度にしておく</p>
<h6 id="メリット-1">メリット</h6>
<ul>
<li>誤差への影響が稀な特徴量に対する感度があがる</li>
</ul>
<h6 id="デメリット">デメリット</h6>
<ul>
<li>誤差が減るにつれ学習率が小さくなっていくので鞍点問題を起こしやすい</li>
<li>勾配が急なところで最適解にたどり着けないことがある</li>
</ul>
<h5 id="rmsprop">RMSProp</h5>
<p>AdaGradの改良版
$h_0 = \kappa$　　　　　　　　　　　 ：任意の値で初期化
$h_t = \alpha h_{t-1} + (1-\alpha)(\nabla E)^2$　　 ：勾配の二乗の値について過去の値と今回の値で移動平均をとる
$W^{(t+1)} = W^{(t)} - \epsilon \frac{1}{\sqrt{h_t}+\theta} \nabla E$ 　　：ｈを用いて学習率を減衰させていく
※θは調整項</p>
<h6 id="メリット-2">メリット</h6>
<ul>
<li>局所的最適解にはならず、大域的最適解となる</li>
<li>ハイパーパラメータの調整が必要な場合が少ない</li>
</ul>
<h5 id="adam">Adam</h5>
<p>モメンタムとRMSPropの組み合わせ。
$V_t = \alpha V_{t-1} + (1-\alpha)(\nabla E)$ 　　：1次モーメント（勾配の移動平均）
$h_t = \beta h_{t-1} + (1-\beta)(\nabla E)^2$  　  ：2次モーメント（勾配の二乗の移動平均）
$\hat{V_t}=\frac{V_t}{1-\alpha^t}$ 　　　　　　　　　 ：バイアス修正
$\hat{h_t}=\frac{h_t}{1-\beta^t}$　　　　　　　　　  ：バイアス修正
$W^{(t)} = W^{(t-1)} - \epsilon \frac{\hat{V_t}}{\sqrt{\hat{h_t}}+\theta}$</p>
<h6 id="メリット-3">メリット</h6>
<ul>
<li>局所最適解に陥りにくい</li>
<li>最適値への収束が早い</li>
</ul>
<h4 id="確認問題５">確認問題５</h4>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326092439.png" alt="/img/Pasted_image_20230326092439.png">
モメンタムは過去の更新量を反映することで勾配が弱い領域でも学習が進むようになっているため収束が早く、局所最適解にも陥りにくい
AdaGradは勾配の２乗の累積値で学習率を減衰させていくことで出現が稀な特徴量への感度を上げることができる
RMSPropはAdaGradが学習段階初期の特徴量分布に大きく依存することを移動平均を取ることによって対応したもの。</p>
<h2 id="過学習">過学習</h2>
<p>特定の訓練データに特化して学習してしまうこと
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326093354.png" alt="/img/Pasted_image_20230326093354.png">
（講義スライドより引用）</p>
<h3 id="原因">原因</h3>
<p>ネットワークの自由度が高すぎること</p>
<ul>
<li>パラメータの数が多い</li>
<li>パラメータの値域が適切でない</li>
<li>ネットワークが深すぎる</li>
</ul>
<h3 id="対策">対策</h3>
<p>正則化を用いて自由度を制約する</p>
<h4 id="確認テスト">確認テスト</h4>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326093835.png" alt="/img/Pasted_image_20230326093835.png">
（a）</p>
<h3 id="正則化">正則化</h3>
<p>過学習が起こらない範囲で重みのばらつきを出す必要がある。
重みに制約を与えることを荷重減衰という。WeightDecay</p>
<p>pノルムに係数をかけて誤差関数に加える。
$$
\begin{array}{ll}
E_n(w) +  \frac{1}{p} \overbrace{\lambda}^{hyper \space parameter} || w ||_p^p &amp; : 誤差関数に、pノルムのp乗を加える \<br>
|| w ||_p =
\Bigl(
|w_1|^p  + \cdots + |w_n|^p
\Bigr)^{\frac{1}{p}} &amp; : pノルムの計算
\end{array}
$$</p>
<p>※λの前の1/pは微分したときに無駄な係数が出ないための調整項。
P＝１のときマンハッタン距離
P＝２のときユークリッド距離
という</p>
<h4 id="l1正則化">L1正則化</h4>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326104817.png" alt="/img/Pasted_image_20230326104817.png">
（講義スライドより引用）</p>
<h4 id="l2正則化">L2正則化</h4>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326104631.png" alt="/img/Pasted_image_20230326104631.png">
（講義スライドより引用）</p>
<h4 id="確認問題６">確認問題６</h4>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326104300.png" alt="/img/Pasted_image_20230326104300.png">
右のLasso推定量、スパース推定のグラフ
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326105311.png" alt="/img/Pasted_image_20230326105311.png">
（講義スライドより引用）
答え：（４）
もとめたいのは微分結果で、損失関数に追加されている項は$\frac{1}{p}<em>rate</em>(||x||_p)^p$なので、paramのみがのこる
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326112101.png" alt="/img/Pasted_image_20230326112101.png">
答え：（３）
求めたいのは微分結果で、正則化項は1/1×rate×|w|なので、符号とrateが残れば良い。
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326112400.png" alt="/img/Pasted_image_20230326112400.png">
答え：（４）
一般に画像データは左上が原点で今回のデータは関数の説明から縦→横→チャンネルの順だから。</p>
<h4 id="ドロップアウト">ドロップアウト</h4>
<p>学習時にランダムにノードを削除してパラメータを更新する。
推論時には落とさない。
ノード数が多すぎることによる過学習の対策。</p>
<p>複数の異なるモデルを学習するアンサンブル学習のような効果が得られる。</p>
<h3 id="マルチタスク学習">マルチタスク学習</h3>
<p>複数のタスクを単一のモデルでまとめて解く手法</p>
<h4 id="マルチタスク学習の仕組み">マルチタスク学習の仕組み</h4>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326112848.png" alt="/img/Pasted_image_20230326112848.png">
（講義スライドより引用）</p>
<h4 id="マルチタスク学習のメリット">マルチタスク学習のメリット</h4>
<ul>
<li>モデル数が少なくて済む</li>
<li>タスクの相乗効果で個別のタスクの精度向上が期待できる</li>
<li>学習時間や総パラメータ数の削減</li>
</ul>
<h4 id="マルチタスク学習をする際の注意点">マルチタスク学習をする際の注意点</h4>
<ul>
<li>個別タスクで見たときには必要なメモリや演算量が増える</li>
<li>学習の難化</li>
</ul>
<h4 id="マルチタスク学習の例">マルチタスク学習の例</h4>
<p>画像から分類結果と物体検出を行う
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326113146.png" alt="/img/Pasted_image_20230326113146.png">
（講義スライドより引用）
個別の文入力から穴埋めと文章推測を行う
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326113215.png" alt="/img/Pasted_image_20230326113215.png">
（講義スライドより引用）</p>
<h4 id="アンサンブル学習">アンサンブル学習</h4>
<p>バイアス：実測値と推測値の差
バリアンス：推測値のばらつき</p>
<p>過学習はバリアンスが高くてバイアスが低い状態といえる。</p>
<p>バイアスとバリアンスはトレードオフの関係にあり、うまくバランスを取ることが必要。</p>
<h5 id="バギング">バギング</h5>
<p>![[バギング]]</p>
<h5 id="ブースティング">ブースティング</h5>
<p>![[ブースティング]]</p>
<h5 id="スタッキング">スタッキング</h5>
<p>![[スタッキング]]</p>
<h2 id="畳み込みニューラルネットワークの概念">畳み込みニューラルネットワークの概念</h2>
<p>CNNの特徴として次元感で繋がりのあるデータを扱えるということがあげられる</p>
<h3 id="lenet">LeNet</h3>
<p>4層の畳み込みニューラルネットワークで画像特徴量を抽出し、
２層の全結合層で抽出した特徴量によって文字の分類を学習する
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326155053.png" alt="/img/Pasted_image_20230326155053.png">
(Yann LeCun et al. Gradient-Based Learning Applied to Document Recognitionより引用)</p>
<h2 id="cnnの各処理の説明">CNNの各処理の説明</h2>
<p>![[深層学習モデル#基本構造]]</p>
<h4 id="確認問題６-1">確認問題６</h4>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326160302.png" alt="/img/Pasted_image_20230326160302.png">
回答：７×７
(6+1×2 - 2)/1 +1 = 7</p>
<h2 id="初期のcnn">初期のCNN</h2>
<p>![[08_project/G_Certification/AlexNet]]</p>
<h1 id="実装演習結果">実装演習結果</h1>
<h2 id="2_1_network_modifiedipynb">2_1_network_modified.ipynb</h2>
<p>コードを一部変更して重みの初期化ごとの結果を確認した。
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326172635.png" alt="/img/Pasted_image_20230326172635.png"></p>
<table>
<thead>
<tr>
<th>標準偏差0.01の正規分布</th>
<th>Xavierの初期化</th>
<th>Heの初期化</th>
<th>層ごとに変更</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326170417.png" alt="/img/Pasted_image_20230326170417.png"></td>
<td><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326170758.png" alt="/img/Pasted_image_20230326170758.png"></td>
<td><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326170856.png" alt="/img/Pasted_image_20230326170856.png"></td>
<td><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326172703.png" alt="/img/Pasted_image_20230326172703.png"></td>
</tr>
<tr>
<td>実際に確認するには何度か実行して平均を取る必要があると思うが、重みの初期化を適切にしたほうが確かに学習初期から精度が上がる傾向があるように見受けられる。</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="2_2_2_vanishing_gradient_modifiedipynb">2_2_2_vanishing_gradient_modified.ipynb</h2>
<table>
<thead>
<tr>
<th></th>
<th>sigmoid - gauss</th>
<th>ReLU - gauss</th>
<th>sigmoid - Xavier</th>
<th>ReLU - He</th>
</tr>
</thead>
<tbody>
<tr>
<td>（40,20）</td>
<td><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326172929.png" alt="/img/Pasted_image_20230326172929.png"></td>
<td><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326172913.png" alt="/img/Pasted_image_20230326172913.png"></td>
<td><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326172851.png" alt="/img/Pasted_image_20230326172851.png"></td>
<td><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326172827.png" alt="/img/Pasted_image_20230326172827.png"></td>
</tr>
<tr>
<td>（400,200）</td>
<td><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326175225.png" alt="/img/Pasted_image_20230326175225.png"></td>
<td><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326175953.png" alt="/img/Pasted_image_20230326175953.png"></td>
<td><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326175928.png" alt="/img/Pasted_image_20230326175928.png"></td>
<td><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326175900.png" alt="/img/Pasted_image_20230326175900.png"></td>
</tr>
<tr>
<td>Sigmoid-gaussの組み合わせは想定どおり、初期分布によって微分値が０に近い領域に入る傾向が出てしまいやはり学習がすすまなかった。</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>ReLU-GaussはSigmoidよりはマシなものの、初期値によってReLUへの入力が負になったときに誤差逆伝播の値が０になってしまうため適切なスケーリングで深い負の値への分布を抑えないと学習が進みづらいように見える。</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Sigmoid-Xavierの組み合わせはスケーリングによって学習が進む様になっているが、Sigmoid自体の勾配が入力層に近づくにつれて減衰する傾向は変わらないので、学習に時間がかかっている</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>ReLU-Heについては初期値で学習初期から学習が進みやすくなっており、勾配が減衰する傾向もないので、スムーズに学習が進んでいる様に思われる。</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="try-hidden_size_listの数字を変更してみよう">try： hidden_size_listの数字を変更してみよう</h3>
<p>Gaussのケースで設定しているハイパーパラメータが各種初期化方法と近くなれば学習の進み方は改善され、隠れ層のノード数を増やしているので表現力が上がって精度は若干良くなっている。
XavierやHeはそもそも入出力の分散が変わらないようにしているので、ノード数変更による表現力の違いの影響だと思われる。
モデルの表現力自体はデフォルトの隠れ層でReLU-Heの組み合わせを見る限り十分に見受けられるので、改善効果は初期値の分布のスケーリングが改善されたかどうかが大きいかと思われる。</p>
<h3 id="try-sigmoid---he-と-relu---xavier-についても試してみよう">try :sigmoid - He と relu - Xavier についても試してみよう</h3>
<table>
<thead>
<tr>
<th>sigmoid - He</th>
<th>ReLU - Xavier</th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326180620.png" alt="/img/Pasted_image_20230326180620.png"></td>
<td><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326180631.png" alt="/img/Pasted_image_20230326180631.png"></td>
</tr>
</tbody>
</table>
<p>Sigmoid-Heの組み合わせは学習が早くなり、ReLU-Xavierについてはあまり変化が見られないか？
初期化のスケーリングの違いが√2倍がはいるかどうかなので、そこまで大きな違いが出ず、最初のランダムサンプリングの影響が強く出た可能性がある。
何度か試して平均的な傾向を確認する必要があると思われる。</p>
<h3 id="2_3_batch_normalizationipynb">2_3_batch_normalization.ipynb</h3>
<table>
<thead>
<tr>
<th></th>
<th>バッチ正規化あり</th>
<th>バッチ正規化無し</th>
</tr>
</thead>
<tbody>
<tr>
<td>sigmoid-Xavier</td>
<td><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326182116.png" alt="/img/Pasted_image_20230326182116.png"></td>
<td><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326182139.png" alt="/img/Pasted_image_20230326182139.png"></td>
</tr>
<tr>
<td>ReLU-He</td>
<td><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326183804.png" alt="/img/Pasted_image_20230326183804.png"></td>
<td><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230326183928.png" alt="/img/Pasted_image_20230326183928.png"></td>
</tr>
<tr>
<td>Sigmoid-Xavier のケースではバッチ正規化無しで学習が進んでいないため、内部共変量シフトか勾配消失が起こってしまっていると考えられる。</td>
<td></td>
<td></td>
</tr>
<tr>
<td>ReLU関数の利用で改善されていること、そもそも層がそこまで深くなく内部共変量シフトがおこるほどではないことから勾配消失が起こっていたのではないかと考えられる。</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="2_4_optimizeripynb">2_4_optimizer.ipynb</h2>
<p>Optimizerを変更してMNISTデータを予測するモデルの学習を行った結果を以下に示す。
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230327054144.png" alt="/img/Pasted_image_20230327054144.png"></p>
<ul>
<li>SGDではそもそも学習がうまく行っていない。</li>
<li>Momentumでは学習は進むが、かなり精度が変動していることがわかる</li>
<li>AdaGradは学習率の減衰パラメータが誤差関数の傾きの二乗の累積であるためにRMSPropに比べて学習が遅くなる傾向が見られた</li>
<li>AdaGradおよびRMSPropは訓練精度＞テスト精度の傾向があり、局所最適解に陥った可能性がある</li>
</ul>
<h3 id="try活性化関数と重みの初期化方法の変更とバッチ正規化">try:活性化関数と重みの初期化方法の変更とバッチ正規化</h3>
<p>バッチ正規化をすることで学習の速度の向上が見られた。
ReLUとSigmoidではReLUのほうが学習速度がはやく、最終的な精度も良かった。</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230327064242.png" alt="/img/Pasted_image_20230327064242.png"></p>
<h3 id="momentumのコードをもとにadagradを作る">MomentumのコードをもとにAdaGradを作る</h3>
<p>以下の様に変更した。
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230327070414.png" alt="/img/Pasted_image_20230327070414.png"></p>
<h2 id="2_5_overfitingipynb">2_5_overfiting.ipynb</h2>
<h3 id="正則化と荷重減衰の強さ">正則化と荷重減衰の強さ</h3>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230327081803.png" alt="/img/Pasted_image_20230327081803.png">
L1正則化はスパース化で特徴量への制約がかなり強く働くので、より正則化の強さの設定に敏感に効いて来ると考えられる。
また、特徴量空間に描かれる誤差関数の曲面がなめらかでなくなるためか、精度の振動が激しい。</p>
<h3 id="ドロップアウトの利用">ドロップアウトの利用</h3>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230327082736.png" alt="/img/Pasted_image_20230327082736.png">
同じドロップアウト比率を使っていても最適化器の違いでかなりの精度差がうまれた。
SGDで学習をしたときはまだ右肩上がりの状態で学習が終了していたため、Adamに切り替えたことで学習が収束するまで進んだことも一員として効果があるのでは無いかと考える。
また、ドロップアウトで学習中の誤差関数をもとに描かれる曲面の形状がいくらか変わるはずなので、局所最適解や鞍点などの問題に陥りやすいSGDはより学習が進みにくかったのでは無いかと考えられる。</p>
<h3 id="dropoutl1">dropout+L1</h3>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230327083852.png" alt="/img/Pasted_image_20230327083852.png"></p>
<h2 id="2_6_simple_convolution_network_afteripynb">2_6_simple_convolution_network_after.ipynb</h2>
<p>np.padの第二引数は各次元の前後にどれだけの長さのパディングをするか。</p>
<h3 id="tryim2colの処理を確認しよう">try：im2colの処理を確認しよう</h3>
<p>・関数内でtransposeの処理をしている行をコメントアウトして下のコードを実行してみよう
・input_dataの各次元のサイズやフィルターサイズ・ストライド・パディングを変えてみよう
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230327103232.png" alt="/img/Pasted_image_20230327103232.png"></p>
<h3 id="convolution-class">convolution class</h3>
<p>forward処理ではフィルタリングが要素積の和になるので、和を取りたい範囲の次元を1次元配列化して内積を取るような処理になるようにインプットとフィルタの形状やデータ順序を変更している。
backward処理も基本的に同様でforwardでした内容の逆をして計算をし易い状態にしてから通常のNNのBackward処理をして、形状を戻すことをしている。。
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230327105200.png" alt="/img/Pasted_image_20230327105200.png"></p>
<h2 id="2_7_double_convolution_network_afteripynb">2_7_double_convolution_network_after.ipynb</h2>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230327110332.png" alt="/img/Pasted_image_20230327110332.png"></p>
<h2 id="2_8_deep_convolution_netipynb">2_8_deep_convolution_net.ipynb</h2>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230327142338.png" alt="/img/Pasted_image_20230327142338.png"></p>
<h2 id="2_9_regularizationipynb">2_9_regularization.ipynb</h2>
<p>CIFAR10 の画像分類タスクに対して各種正規化を適用。</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230329152559.png" alt="/img/Pasted_image_20230329152559.png">
正規化を適用していないモデルでは、学習が進むに連れテスト誤差が上がってきてしまっているが、
L1,L2及びErasticNetではそれが抑制できていることがわかる。</p>
<h2 id="2_10_layer-normalizationipynb">2_10_layer-normalization.ipynb</h2>
<p>レイヤ正規化自体の説明は[[#プラスアルファ]]のところに記載。</p>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230329154629.png" alt="/img/Pasted_image_20230329154629.png">
レイヤ正規化では過学習が発生しているようだが、特徴量次元方向で正規化した結果、確信度が低い特徴量の切り捨てが多く発生したのでは無いかと考えられる。インスタンス正規化の方はあまりその傾向が見られないため、その可能性は高いのでは無いかとおもう。</p>
<h2 id="2_11_dropoutipynb">2_11_dropout.ipynb</h2>
<p>ドロップアウト比率0.3での比較。
無しでテスト誤差1.25→ありで1.0で確かに過学習が抑制されていることがわかる。
実装箇所は最終の全結合層の直前。
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230329160335.png" alt="/img/Pasted_image_20230329160335.png"></p>
<h2 id="2_app_1_tensorflow_codes_afteripynb">2_app_1_tensorflow_codes_after.ipynb</h2>
<h3 id="tensorflowでの実装">tensorflowでの実装</h3>
<h4 id="定数定義">定数定義</h4>
<p>tensorflow.constant(value)</p>
<h4 id="変数定義">変数定義</h4>
<p>tensorflow.Variable(value)</p>
<h4 id="計算グラフの定義">計算グラフの定義</h4>
<p>tensorflow.functionデコレータを使って関数定義する</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@tf.function</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">add</span>(a,b):
</span></span><span style="display:flex;"><span>	<span style="color:#66d9ef">return</span> a<span style="color:#f92672">+</span>b
</span></span></code></pre></div><h4 id="optimizer">optimizer</h4>
<p>tensorflow.keras.optimizersのなかから選択</p>
<h4 id="訓練ステップ">訓練ステップ</h4>
<p>こちらもtensorflow.funcitonデコレータを用いる。
GradientTape()というクラスのgradient(損失関数,訓練パラメータのリスト)メソッドで勾配を得る
oplimizers内のクラスが持つapply_gradientsメソッドに（勾配、勾配に対応するパラメータ）のタプルを返すイテレータを与えることでパラメータの更新を行う。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#a6e22e">@tf.function</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train_step</span>(inputs, labels):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
</span></span><span style="display:flex;"><span>        y <span style="color:#f92672">=</span> linear_regression(inputs, W, b)
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> calc_loss(y, labels)
</span></span><span style="display:flex;"><span>    gradients <span style="color:#f92672">=</span> tape<span style="color:#f92672">.</span>gradient(loss, trainable_variables) <span style="color:#75715e"># 勾配 (loss を trainable_variables の各変数で微分) を求める</span>
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">.</span>apply_gradients(zip(gradients, trainable_variables)) <span style="color:#75715e"># 勾配を用いて変数の更新</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> loss
</span></span></code></pre></div><h2 id="2_app_2_save_loadipynb">2_app_2_save_load.ipynb</h2>
<h3 id="tensorflowでの実装の続き">tensorflowでの実装の続き</h3>
<h4 id="モデルの保存">モデルの保存</h4>
<p>tensorflow.train.Checkpointクラスの利用
インスタンス生成時に与えるのはモデルでも、変数の辞書でも可。
.save(パス)メソッドで保存
.restore(パス)メソッドで復元</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>ckpt <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>train<span style="color:#f92672">.</span>Checkpoint(step<span style="color:#f92672">=</span>tf<span style="color:#f92672">.</span>Variable(<span style="color:#ae81ff">1</span>), optimizer<span style="color:#f92672">=</span>opt, net<span style="color:#f92672">=</span>net, iterator<span style="color:#f92672">=</span>iterator)
</span></span><span style="display:flex;"><span>manager <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>train<span style="color:#f92672">.</span>CheckpointManager(ckpt, <span style="color:#e6db74">&#39;./tf_ckpts&#39;</span>, max_to_keep<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>)
</span></span></code></pre></div><p>（<a href="https://www.tensorflow.org/guide/checkpoint?hl=ja">トレーニングのチェックポイント  |  TensorFlow Core</a> より引用）</p>
<h2 id="2_app_3_keras_codes_afteripynb">2_app_3_keras_codes_after.ipynb</h2>
<p>tensorflow.keras.models.Sequentialクラスを用いた
深層学習モデルの設計方法を学んだ。</p>
<ol>
<li>Sequentialクラスでモデルインスタンスを生成</li>
<li>.addメソッドでtensorflow.keras.layersモジュールにあるレイヤクラスを追加していく</li>
<li>.compileメソッドで損失関数、オプティマイザ、評価指標の設定を行う</li>
<li>.fitメソッドでモデルの学習</li>
</ol>
<h1 id="プラスアルファ">プラスアルファ</h1>
<h2 id="レイヤ正規化">レイヤ正規化</h2>
<p>実装演習内で
レイヤ正規化という言葉が新しく出てきたので、
<a href="https://data-analytics.fun/2020/07/16/understanding-layer-normalization/">Layer Normalizationを理解する | 楽しみながら理解するAI・機械学習入門</a>を参考にまとめる。</p>
<p>[[バッチ正規化]] で残った以下の問題に対応するための手法</p>
<ul>
<li>ミニバッチごとの平均分散を計算するので、バッチサイズが小さいときに平均・分散が不安定になる</li>
<li>RNNに適応するのが難しい（サンプル長が可変）</li>
</ul>
<p>以上に対応するために
バッチ正規化ではサンプル次元の方向で系列をとって平均分散を計算していたのを、
特徴量次元の方向で系列を取って平均分散を計算するようにしている。
<img src="https://half-broken-engineer.github.io/img/Pasted_image_20230327111611.png" alt="/img/Pasted_image_20230327111611.png">
(<a href="https://data-analytics.fun/2020/07/16/understanding-layer-normalization/">Layer Normalizationを理解する | 楽しみながら理解するAI・機械学習入門</a>より引用）</p>
<h3 id="各種正規化の図解">各種正規化の図解</h3>
<p><img src="https://half-broken-engineer.github.io/img/Pasted_image_20230329153357.png" alt="/img/Pasted_image_20230329153357.png">
(Group Normalization(<a href="https://arxiv.org/abs/1803.08494">https://arxiv.org/abs/1803.08494</a>)より引用)</p>
<h3 id="各種正規化の使われ方">各種正規化の使われ方</h3>
<ul>
<li>バッチ正規化：幅広いタスクで使用、ただしバッチサイズが小さいときに安定しないことから、近年の大規模モデルでバッチサイズが大きくできないときに他の手法に置き換えられる。</li>
<li>レイヤ正規化：時系列モデル</li>
<li>インスタンス正規化：Style Transferで利用。各データサンプルが同じようなコントラストになるように変換される</li>
</ul>
<h2 id="erastic-net">Erastic Net</h2>
<p>L1正則化とL2正則化を混ぜ合わせた正則化手法、</p>

              


            </div>
          </div>
          <div id="post-footer" class="post-footer main-content-wrap">
            
              
                
                
                  <div class="post-footer-tags">
                    <span class="text-color-light text-small">タグ</span><br/>
                    
  <a class="tag tag--primary tag--small" href="https://half-broken-engineer.github.io/tags/obsidian_note/">obsidian_note</a>

                  </div>
                
              
            
            
<div class="post-actions-wrap">
  <nav >
    <ul class="post-actions post-action-nav">
      
        <li class="post-action">
          
            <a class="post-action-btn btn btn--default tooltip--top" href="https://half-broken-engineer.github.io/2023/03/deeplearningday3/" data-tooltip="DeepLearningDay3" aria-label="次: DeepLearningDay3">
          
              <i class="fa fa-angle-left"></i>
              <span class="hide-xs hide-sm text-small icon-ml">次</span>
            </a>
        </li>
        <li class="post-action">
          
            <a class="post-action-btn btn btn--default tooltip--top" href="https://half-broken-engineer.github.io/2023/03/google%E3%81%AE%E7%8C%AB/" data-tooltip="Googleの猫" aria-label="前: Googleの猫">
          
              <span class="hide-xs hide-sm text-small icon-mr">前</span>
              <i class="fa fa-angle-right"></i>
            </a>
        </li>
      
    </ul>
  </nav>
<ul class="post-actions post-action-share" >
  
    <li class="post-action hide-lg hide-md hide-sm">
      <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="この記事を共有する">
        <i class="fa fa-share-alt" aria-hidden="true"></i>
      </a>
    </li>
    
      <li class="post-action hide-xs">
        <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=https://half-broken-engineer.github.io/2023/03/deeplearningday2/" title="Twitterで共有" aria-label="Twitterで共有">
          <i class="fab fa-twitter" aria-hidden="true"></i>
        </a>
      </li>
    
  
  
    <li class="post-action">
      <a class="post-action-btn btn btn--default" href="#disqus_thread" aria-label="コメントを残す">
        <i class="far fa-comment"></i>
      </a>
    </li>
  
  <li class="post-action">
    
      <a class="post-action-btn btn btn--default" href="#top" aria-label="トップに戻る">
      <i class="fa fa-arrow-up" aria-hidden="true"></i>
    
    </a>
  </li>
</ul>
</div>


            
  
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
    <script type="text/javascript">
      var disqus_config = function() {
        this.page.url = 'https:\/\/half-broken-engineer.github.io\/2023\/03\/deeplearningday2\/';
        
          this.page.identifier = '\/2023\/03\/deeplearningday2\/'
        
      };
      (function() {
        
        
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
          document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
          return;
        }
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        var disqus_shortname = 'hugo-tranquilpeak-theme';
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
  


          </div>
        </article>
        <footer>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</footer>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
    inlineMath: [['$','$']]
    }
});
</script>
      </div>
      <div id="bottom-bar" class="post-bottom-bar" data-behavior="5">
        
<div class="post-actions-wrap">
  <nav >
    <ul class="post-actions post-action-nav">
      
        <li class="post-action">
          
            <a class="post-action-btn btn btn--default tooltip--top" href="https://half-broken-engineer.github.io/2023/03/deeplearningday3/" data-tooltip="DeepLearningDay3" aria-label="次: DeepLearningDay3">
          
              <i class="fa fa-angle-left"></i>
              <span class="hide-xs hide-sm text-small icon-ml">次</span>
            </a>
        </li>
        <li class="post-action">
          
            <a class="post-action-btn btn btn--default tooltip--top" href="https://half-broken-engineer.github.io/2023/03/google%E3%81%AE%E7%8C%AB/" data-tooltip="Googleの猫" aria-label="前: Googleの猫">
          
              <span class="hide-xs hide-sm text-small icon-mr">前</span>
              <i class="fa fa-angle-right"></i>
            </a>
        </li>
      
    </ul>
  </nav>
<ul class="post-actions post-action-share" >
  
    <li class="post-action hide-lg hide-md hide-sm">
      <a class="post-action-btn btn btn--default btn-open-shareoptions" href="#btn-open-shareoptions" aria-label="この記事を共有する">
        <i class="fa fa-share-alt" aria-hidden="true"></i>
      </a>
    </li>
    
      <li class="post-action hide-xs">
        <a class="post-action-btn btn btn--default" target="new" href="https://twitter.com/intent/tweet?text=https://half-broken-engineer.github.io/2023/03/deeplearningday2/" title="Twitterで共有" aria-label="Twitterで共有">
          <i class="fab fa-twitter" aria-hidden="true"></i>
        </a>
      </li>
    
  
  
    <li class="post-action">
      <a class="post-action-btn btn btn--default" href="#disqus_thread" aria-label="コメントを残す">
        <i class="far fa-comment"></i>
      </a>
    </li>
  
  <li class="post-action">
    
      <a class="post-action-btn btn btn--default" href="#top" aria-label="トップに戻る">
      <i class="fa fa-arrow-up" aria-hidden="true"></i>
    
    </a>
  </li>
</ul>
</div>


      </div>
      
<div id="share-options-bar" class="share-options-bar" data-behavior="5">
  <i id="btn-close-shareoptions" class="fa fa-times"></i>
  <ul class="share-options">
    
      <li class="share-option">
        <a class="share-option-btn" target="new" href="https://twitter.com/intent/tweet?text=https%3A%2F%2Fhalf-broken-engineer.github.io%2F2023%2F03%2Fdeeplearningday2%2F" aria-label="Twitterで共有">
          <i class="fab fa-twitter" aria-hidden="true"></i><span>Twitterで共有</span>
        </a>
      </li>
    
  </ul>
</div>
<div id="share-options-mask" class="share-options-mask"></div>


    </div>
    
    <div id="about">
  <div id="about-card">
    <div id="about-btn-close">
      <i class="fa fa-times"></i>
    </div>
    
      <img id="about-card-picture" src="https://half-broken-engineer.github.io/img/profile.png" alt="プロフィール画像" />
    
    <h4 id="about-card-name">Half-Broken Engineer</h4>
    
      <div id="about-card-bio">🤖　　　　壊れかけのエンジニア　　　　💻不安を解消したいから💰のお勉強もする</div>
    
    
      <div id="about-card-job">
        <i class="fa fa-briefcase"></i>
        <br/>
        Engineer
      </div>
    
    
      <div id="about-card-location">
        <i class="fa fa-map-marker-alt"></i>
        <br/>
        Japan
      </div>
    
  </div>
</div>

    

    
  
    
      <div id="cover" style="background-image:url('https://half-broken-engineer.github.io/images/cover.jpg');"></div>
    
  


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" integrity="sha512-894YE6QWD5I59HgZOGReFYm4dnWc1Qt5NtvYSaNcOP+u1T9qYdvdihz0PPSiiqn/+/3e7Jo4EaG7TubfWGUrMQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/highlight.min.js" integrity="sha512-z+/WWfyD5tccCukM4VvONpEtLmbAm5LDu7eKiyMQJ9m7OfPEDL7gENyDRL3Yfe8XAuGsS2fS4xSMnl6d30kqGQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha512-uURl+ZXMBrF4AwGaWmEetzrd+J5/8NRkWAvJx5sbPSSuOb0bZLqf+tOzniObO00BjHa/dD7gub9oCGMLPQHtQA==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>


<script src="https://half-broken-engineer.github.io/js/script-yqzy9wdlzix4lbbwdnzvwx3egsne77earqmn73v9uno8aupuph8wfguccut.min.js"></script>


  
    <script async crossorigin="anonymous" defer integrity="sha512-gE8KAQyFIzV1C9+GZ8TKJHZS2s+n7EjNtC+IMRn1l5+WYJTHOODUM6JSjZhFhqXmc7bG8Av6XXpckA4tYhflnw==" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/apache.min.js"></script>
  

  
    <script async crossorigin="anonymous" defer integrity="sha512-EWROca+bote+7Oaaar1F6y74iZj1r1F9rm/ly7o+/FwJopbBaWtsFDmaKoZDd3QiGU2pGacBirHJNivmGLYrow==" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/go.min.js"></script>
  

  
    <script async crossorigin="anonymous" defer integrity="sha512-GDVzAn0wpx1yVtQsRWmFc6PhJiLBPdUic+h4GWgljBh904O3JU10fk9EKNpVyIoPqkFn54rgL2QBG4BmUTMpiQ==" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/http.min.js"></script>
  

  
    <script async crossorigin="anonymous" defer integrity="sha512-UgZlma8NzkrDb/NWgmLIcTrH7i/CSnLLDRFqCSNF5NGPpjKmzyM25qcoXGOup8+cDakKyaiTDd7N4dyH4YT+IA==" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/less.min.js"></script>
  

  
    <script async crossorigin="anonymous" defer integrity="sha512-lot9koe73sfXIrUvIPM/UEhuMciN56RPyBdOyZgfO53P2lkWyyXN7J+njcxIIBRV+nVDQeiWtiXg+bLAJZDTfg==" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/nginx.min.js"></script>
  

  
    <script async crossorigin="anonymous" defer integrity="sha512-Zd3e7XxHP00TD0Imr0PIfeM0fl0v95kMWuhyAS3Wn1UTSXTkz0OhtRgBAr4JlmADRgiXr4x7lpeUdqaGN8xIog==" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/puppet.min.js"></script>
  

  
    <script async crossorigin="anonymous" defer integrity="sha512-qtqDO052iXMSP+5d/aE/jMtL9vIIGvONgTJziC2K/ZIB1yEGa55WVxGE9/08rSQ62EoDifS9SWVGZ7ihSLhzMA==" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/scss.min.js"></script>
  

  
    <script async crossorigin="anonymous" defer integrity="sha512-1NmkjnEDnwwwcu28KoQF8vs3oaPFokQHbmbtwGhFfeDsQZtVFI8zW2aE9O8yMYdpdyKV/5blE4pSWw4Z/Sv97w==" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/stylus.min.js"></script>
  

  
    <script async crossorigin="anonymous" defer integrity="sha512-B2wSfruPjr8EJL6IIzQr1eAuDwrsfIfccNf/LCEdxELCgC/S/ZMt/Uvk80aD79m7IqOqW+Sw8nbkvha20yZpzg==" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/swift.min.js"></script>
  

  
    <script async crossorigin="anonymous" defer integrity="sha512-28oDiQZGKUVN6wQ7PSLPNipOcmkCALXKwOi7bnkyFf8QiMZQxG9EQoy/iiNx6Zxj2cG2SbVa4dXKigQhu7GiFw==" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.1.0/languages/yaml.min.js"></script>
  


<script>
$(document).ready(function() {
  hljs.configure({ classPrefix: '', useBR: false });
  $('pre.code-highlight > code, pre > code').each(function(i, block) {
    if (!$(this).hasClass('codeblock')) {
      $(this).addClass('codeblock');
    }
    hljs.highlightBlock(block);
  });
});
</script>




    
  </body>
</html>

